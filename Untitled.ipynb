{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99250fab-ec3d-4ea5-8e44-e40e66f03a18",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3828675870.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    data:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# configs/default.yaml\n",
    "# 默认配置文件\n",
    "data:\n",
    "  root_dir: \"data/chest_xray\"\n",
    "  image_dir: \"image\"\n",
    "  mask_dir: \"mask\"\n",
    "  metadata_file: \"metadata.csv\"\n",
    "  split_file: \"split.csv\"\n",
    "  \n",
    "  # 数据预处理\n",
    "  image_size: 512\n",
    "  num_workers: 4\n",
    "  pin_memory: true\n",
    "  \n",
    "  # 数据分割比例\n",
    "  train_ratio: 0.7\n",
    "  val_ratio: 0.15\n",
    "  test_ratio: 0.15\n",
    "\n",
    "training:\n",
    "  # 基础训练参数\n",
    "  batch_size: 8\n",
    "  num_epochs: 100\n",
    "  learning_rate: 1e-4\n",
    "  weight_decay: 1e-5\n",
    "  \n",
    "  # 优化器设置\n",
    "  optimizer: \"adam\"\n",
    "  scheduler: \"reduce_on_plateau\"\n",
    "  scheduler_patience: 5\n",
    "  scheduler_factor: 0.5\n",
    "  \n",
    "  # 早停设置\n",
    "  early_stopping:\n",
    "    patience: 15\n",
    "    min_delta: 0.001\n",
    "    restore_best_weights: true\n",
    "  \n",
    "  # 损失函数权重\n",
    "  loss_weights:\n",
    "    bce: 0.5\n",
    "    dice: 0.5\n",
    "\n",
    "model:\n",
    "  # 模型基础参数\n",
    "  in_channels: 1\n",
    "  out_channels: 1\n",
    "  dropout_rate: 0.3\n",
    "  bilinear: false\n",
    "\n",
    "augmentation:\n",
    "  # 数据增强参数\n",
    "  horizontal_flip: 0.5\n",
    "  shift_scale_rotate:\n",
    "    shift_limit: 0.1\n",
    "    scale_limit: 0.1\n",
    "    rotate_limit: 15\n",
    "    p: 0.5\n",
    "  brightness_contrast:\n",
    "    brightness_limit: 0.2\n",
    "    contrast_limit: 0.2\n",
    "    p: 0.5\n",
    "  elastic_transform:\n",
    "    alpha: 1\n",
    "    sigma: 50\n",
    "    alpha_affine: 50\n",
    "    p: 0.3\n",
    "  noise_blur:\n",
    "    gauss_noise_var: [10, 50]\n",
    "    gauss_blur_limit: 3\n",
    "    p: 0.3\n",
    "\n",
    "logging:\n",
    "  # 日志设置\n",
    "  log_dir: \"logs\"\n",
    "  checkpoint_dir: \"checkpoints\"\n",
    "  output_dir: \"outputs\"\n",
    "  vis_dir: \"outputs/vis\"\n",
    "  results_dir: \"results\"\n",
    "  \n",
    "  # 保存设置\n",
    "  save_every_n_epochs: 10\n",
    "  save_best_only: true\n",
    "  monitor_metric: \"val_loss\"\n",
    "  \n",
    "experiment:\n",
    "  # 实验设置\n",
    "  name: \"lung_segmentation\"\n",
    "  tags: [\"unet\", \"chest_xray\", \"segmentation\"]\n",
    "  seed: 42\n",
    "  deterministic: true\n",
    "  \n",
    "  # Weights & Biases 设置\n",
    "  wandb:\n",
    "    enabled: false\n",
    "    project: \"lung_segmentation\"\n",
    "    entity: null\n",
    "\n",
    "# ==============================================================================\n",
    "# configs/unet.yaml\n",
    "# U-Net 专用配置文件\n",
    "\n",
    "defaults:\n",
    "  - default\n",
    "\n",
    "# 覆盖默认配置\n",
    "model:\n",
    "  name: \"unet\"\n",
    "  in_channels: 1\n",
    "  out_channels: 1\n",
    "  features: [64, 128, 256, 512]  # 每层特征数\n",
    "  dropout_rate: 0.3\n",
    "  bilinear: false\n",
    "\n",
    "training:\n",
    "  batch_size: 8\n",
    "  num_epochs: 100\n",
    "  learning_rate: 1e-4\n",
    "  weight_decay: 1e-5\n",
    "  \n",
    "  # U-Net 特定的损失权重\n",
    "  loss_weights:\n",
    "    bce: 0.4\n",
    "    dice: 0.6\n",
    "\n",
    "experiment:\n",
    "  name: \"unet_baseline\"\n",
    "  tags: [\"unet\", \"baseline\"]\n",
    "\n",
    "# ==============================================================================\n",
    "# configs/config_loader.py\n",
    "# 配置加载器\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "from typing import Dict, Any\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "class ConfigLoader:\n",
    "    \"\"\"统一配置加载器，支持 YAML 配置文件的加载和合并\"\"\"\n",
    "    \n",
    "    def __init__(self, config_dir: str = \"configs\"):\n",
    "        self.config_dir = Path(config_dir)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def load_config(self, config_name: str = \"default\") -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        加载配置文件\n",
    "        \n",
    "        Args:\n",
    "            config_name: 配置文件名（不包含.yaml后缀）\n",
    "            \n",
    "        Returns:\n",
    "            配置字典\n",
    "        \"\"\"\n",
    "        config_path = self.config_dir / f\"{config_name}.yaml\"\n",
    "        \n",
    "        if not config_path.exists():\n",
    "            raise FileNotFoundError(f\"配置文件不存在: {config_path}\")\n",
    "            \n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            \n",
    "        # 处理默认配置继承\n",
    "        if 'defaults' in config:\n",
    "            base_configs = config.pop('defaults')\n",
    "            merged_config = {}\n",
    "            \n",
    "            # 递归加载默认配置\n",
    "            for base_config in base_configs:\n",
    "                if isinstance(base_config, str):\n",
    "                    base_cfg = self.load_config(base_config)\n",
    "                    merged_config = self._merge_configs(merged_config, base_cfg)\n",
    "                    \n",
    "            # 当前配置覆盖默认配置\n",
    "            config = self._merge_configs(merged_config, config)\n",
    "            \n",
    "        # 创建必要的目录\n",
    "        self._create_directories(config)\n",
    "        \n",
    "        self.logger.info(f\"成功加载配置: {config_name}\")\n",
    "        return config\n",
    "        \n",
    "    def _merge_configs(self, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"递归合并配置字典\"\"\"\n",
    "        result = base.copy()\n",
    "        \n",
    "        for key, value in override.items():\n",
    "            if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n",
    "                result[key] = self._merge_configs(result[key], value)\n",
    "            else:\n",
    "                result[key] = value\n",
    "                \n",
    "        return result\n",
    "        \n",
    "    def _create_directories(self, config: Dict[str, Any]):\n",
    "        \"\"\"根据配置创建必要的目录\"\"\"\n",
    "        directories_to_create = []\n",
    "        \n",
    "        if 'logging' in config:\n",
    "            logging_config = config['logging']\n",
    "            directories_to_create.extend([\n",
    "                logging_config.get('log_dir', 'logs'),\n",
    "                logging_config.get('checkpoint_dir', 'checkpoints'),\n",
    "                logging_config.get('output_dir', 'outputs'),\n",
    "                logging_config.get('vis_dir', 'outputs/vis'),\n",
    "                logging_config.get('results_dir', 'results'),\n",
    "            ])\n",
    "            \n",
    "        for directory in directories_to_create:\n",
    "            Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# configs/__init__.py\n",
    "\n",
    "from .config_loader import ConfigLoader\n",
    "\n",
    "__all__ = ['ConfigLoader']\n",
    "\n",
    "# ==============================================================================\n",
    "# configs/schema.py\n",
    "# 配置验证模式\n",
    "\n",
    "from typing import Dict, Any, List, Optional, Union\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"数据相关配置\"\"\"\n",
    "    root_dir: str\n",
    "    image_dir: str\n",
    "    mask_dir: str\n",
    "    metadata_file: str\n",
    "    split_file: str\n",
    "    image_size: int\n",
    "    num_workers: int\n",
    "    pin_memory: bool\n",
    "    train_ratio: float\n",
    "    val_ratio: float\n",
    "    test_ratio: float\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # 验证比例总和\n",
    "        total_ratio = self.train_ratio + self.val_ratio + self.test_ratio\n",
    "        if abs(total_ratio - 1.0) > 1e-6:\n",
    "            raise ValueError(f\"数据分割比例总和必须为1.0，当前为: {total_ratio}\")\n",
    "\n",
    "@dataclass\n",
    "class EarlyStoppingConfig:\n",
    "    \"\"\"早停配置\"\"\"\n",
    "    patience: int\n",
    "    min_delta: float\n",
    "    restore_best_weights: bool\n",
    "\n",
    "@dataclass\n",
    "class LossWeights:\n",
    "    \"\"\"损失函数权重\"\"\"\n",
    "    bce: float\n",
    "    dice: float\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"训练相关配置\"\"\"\n",
    "    batch_size: int\n",
    "    num_epochs: int\n",
    "    learning_rate: float\n",
    "    weight_decay: float\n",
    "    optimizer: str\n",
    "    scheduler: str\n",
    "    scheduler_patience: int\n",
    "    scheduler_factor: float\n",
    "    early_stopping: EarlyStoppingConfig\n",
    "    loss_weights: LossWeights\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"模型相关配置\"\"\"\n",
    "    name: str\n",
    "    in_channels: int\n",
    "    out_channels: int\n",
    "    dropout_rate: float\n",
    "    bilinear: bool\n",
    "    features: Optional[List[int]] = None\n",
    "\n",
    "@dataclass\n",
    "class AugmentationConfig:\n",
    "    \"\"\"数据增强配置\"\"\"\n",
    "    horizontal_flip: float\n",
    "    shift_scale_rotate: Dict[str, Union[float, int]]\n",
    "    brightness_contrast: Dict[str, Union[float, int]]\n",
    "    elastic_transform: Dict[str, Union[float, int]]\n",
    "    noise_blur: Dict[str, Union[List[int], int, float]]\n",
    "\n",
    "@dataclass\n",
    "class LoggingConfig:\n",
    "    \"\"\"日志相关配置\"\"\"\n",
    "    log_dir: str\n",
    "    checkpoint_dir: str\n",
    "    output_dir: str\n",
    "    vis_dir: str\n",
    "    results_dir: str\n",
    "    save_every_n_epochs: int\n",
    "    save_best_only: bool\n",
    "    monitor_metric: str\n",
    "\n",
    "@dataclass\n",
    "class WandbConfig:\n",
    "    \"\"\"Weights & Biases配置\"\"\"\n",
    "    enabled: bool\n",
    "    project: str\n",
    "    entity: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"实验配置\"\"\"\n",
    "    name: str\n",
    "    tags: List[str]\n",
    "    seed: int\n",
    "    deterministic: bool\n",
    "    wandb: WandbConfig\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"完整配置\"\"\"\n",
    "    data: DataConfig\n",
    "    training: TrainingConfig\n",
    "    model: ModelConfig\n",
    "    augmentation: AugmentationConfig\n",
    "    logging: LoggingConfig\n",
    "    experiment: ExperimentConfig\n",
    "\n",
    "def validate_config(config_dict: Dict[str, Any]) -> Config:\n",
    "    \"\"\"验证并转换配置字典为结构化配置对象\"\"\"\n",
    "    try:\n",
    "        # 递归创建配置对象\n",
    "        data_config = DataConfig(**config_dict['data'])\n",
    "        \n",
    "        early_stopping = EarlyStoppingConfig(**config_dict['training']['early_stopping'])\n",
    "        loss_weights = LossWeights(**config_dict['training']['loss_weights'])\n",
    "        training_config = TrainingConfig(\n",
    "            **{k: v for k, v in config_dict['training'].items() \n",
    "               if k not in ['early_stopping', 'loss_weights']},\n",
    "            early_stopping=early_stopping,\n",
    "            loss_weights=loss_weights\n",
    "        )\n",
    "        \n",
    "        model_config = ModelConfig(**config_dict['model'])\n",
    "        augmentation_config = AugmentationConfig(**config_dict['augmentation'])\n",
    "        logging_config = LoggingConfig(**config_dict['logging'])\n",
    "        \n",
    "        wandb_config = WandbConfig(**config_dict['experiment']['wandb'])\n",
    "        experiment_config = ExperimentConfig(\n",
    "            **{k: v for k, v in config_dict['experiment'].items() if k != 'wandb'},\n",
    "            wandb=wandb_config\n",
    "        )\n",
    "        \n",
    "        return Config(\n",
    "            data=data_config,\n",
    "            training=training_config,\n",
    "            model=model_config,\n",
    "            augmentation=augmentation_config,\n",
    "            logging=logging_config,\n",
    "            experiment=experiment_config\n",
    "        )\n",
    "        \n",
    "    except KeyError as e:\n",
    "        raise ValueError(f\"配置文件缺少必要字段: {e}\")\n",
    "    except TypeError as e:\n",
    "        raise ValueError(f\"配置文件字段类型错误: {e}\")\n",
    "\n",
    "# 使用示例：\n",
    "if __name__ == \"__main__\":\n",
    "    # 测试配置加载\n",
    "    loader = ConfigLoader()\n",
    "    config_dict = loader.load_config(\"default\")\n",
    "    config = validate_config(config_dict)\n",
    "    print(f\"成功加载配置: {config.experiment.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb23e6-718a-4553-8994-8f3ed2d49440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975148e4-d7bc-432d-8862-66c16f9c1336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beb4410-807c-40be-9192-d091ff99ab2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf5bb826-f4c4-4462-8838-3b5d56d90d85",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 417)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<string>:417\u001b[0;36m\u001b[0m\n\u001b[0;31m    def prepare_lung_masks_only(self) -> bool:\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# data/prepare_data.py\n",
    "# 数据下载、解压、预处理和分割模块\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "class ChestXrayDataPreparer:\n",
    "    \"\"\"胸片数据准备器：下载、解压、预处理、分割\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.data_config = config['data']\n",
    "        self.root_dir = Path(self.data_config['root_dir'])\n",
    "        self.image_dir = self.root_dir / self.data_config['image_dir']\n",
    "        self.mask_dir = self.root_dir / self.data_config['mask_dir']\n",
    "        \n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # 创建目录\n",
    "        self.root_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.image_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.mask_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def download_data(self, url: Optional[str] = None, force_download: bool = False) -> bool:\n",
    "        \"\"\"\n",
    "        下载胸片数据集\n",
    "        \n",
    "        Args:\n",
    "            url: 数据集下载链接\n",
    "            force_download: 是否强制重新下载\n",
    "            \n",
    "        Returns:\n",
    "            下载是否成功\n",
    "        \"\"\"\n",
    "        if url is None:\n",
    "            self.logger.warning(\"未提供下载链接，跳过数据下载步骤\")\n",
    "            return False\n",
    "            \n",
    "        zip_path = self.root_dir / \"chest_xray_data.zip\"\n",
    "        \n",
    "        # 检查是否已存在\n",
    "        if zip_path.exists() and not force_download:\n",
    "            self.logger.info(\"数据文件已存在，跳过下载\")\n",
    "            return True\n",
    "            \n",
    "        try:\n",
    "            self.logger.info(f\"开始下载数据: {url}\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            with open(zip_path, 'wb') as f, tqdm(\n",
    "                desc=\"下载中\",\n",
    "                total=total_size,\n",
    "                unit='B',\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "            ) as pbar:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        pbar.update(len(chunk))\n",
    "                        \n",
    "            self.logger.info(f\"数据下载完成: {zip_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"数据下载失败: {e}\")\n",
    "            if zip_path.exists():\n",
    "                zip_path.unlink()\n",
    "            return False\n",
    "            \n",
    "    def extract_data(self, zip_path: Optional[str] = None) -> bool:\n",
    "        \"\"\"\n",
    "        解压数据文件\n",
    "        \n",
    "        Args:\n",
    "            zip_path: zip文件路径，如果为None则使用默认路径\n",
    "            \n",
    "        Returns:\n",
    "            解压是否成功\n",
    "        \"\"\"\n",
    "        if zip_path is None:\n",
    "            zip_path = self.root_dir / \"chest_xray_data.zip\"\n",
    "        else:\n",
    "            zip_path = Path(zip_path)\n",
    "            \n",
    "        if not zip_path.exists():\n",
    "            self.logger.error(f\"数据文件不存在: {zip_path}\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            self.logger.info(f\"开始解压数据: {zip_path}\")\n",
    "            \n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                # 获取文件列表\n",
    "                file_list = zip_ref.namelist()\n",
    "                \n",
    "                # 过滤图像和掩码文件\n",
    "                image_files = [f for f in file_list if self._is_image_file(f)]\n",
    "                mask_files = [f for f in file_list if self._is_mask_file(f)]\n",
    "                \n",
    "                # 解压图像文件\n",
    "                for file_path in tqdm(image_files, desc=\"解压图像文件\"):\n",
    "                    target_path = self.image_dir / Path(file_path).name\n",
    "                    with zip_ref.open(file_path) as source, open(target_path, 'wb') as target:\n",
    "                        target.write(source.read())\n",
    "                        \n",
    "                # 解压掩码文件\n",
    "                for file_path in tqdm(mask_files, desc=\"解压掩码文件\"):\n",
    "                    target_path = self.mask_dir / Path(file_path).name\n",
    "                    with zip_ref.open(file_path) as source, open(target_path, 'wb') as target:\n",
    "                        target.write(source.read())\n",
    "                        \n",
    "            self.logger.info(\"数据解压完成\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"数据解压失败: {e}\")\n",
    "            return False\n",
    "            \n",
    "    def _is_image_file(self, filename: str) -> bool:\n",
    "        \"\"\"判断是否为图像文件\"\"\"\n",
    "        image_extensions = {'.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.tif'}\n",
    "        return (\n",
    "            Path(filename).suffix.lower() in image_extensions and\n",
    "            'mask' not in filename.lower() and\n",
    "            'label' not in filename.lower()\n",
    "        )\n",
    "        \n",
    "    def _is_mask_file(self, filename: str) -> bool:\n",
    "        \"\"\"判断是否为掩码文件\"\"\"\n",
    "        mask_extensions = {'.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.tif'}\n",
    "        return (\n",
    "            Path(filename).suffix.lower() in mask_extensions and\n",
    "            ('mask' in filename.lower() or 'label' in filename.lower())\n",
    "        )\n",
    "        \n",
    "    def validate_data_integrity(self) -> Tuple[bool, Dict]:\n",
    "        \"\"\"\n",
    "        验证数据完整性\n",
    "        \n",
    "        Returns:\n",
    "            (是否通过验证, 验证报告)\n",
    "        \"\"\"\n",
    "        self.logger.info(\"开始验证数据完整性...\")\n",
    "        \n",
    "        report = {\n",
    "            'total_images': 0,\n",
    "            'total_masks': 0,\n",
    "            'matched_pairs': 0,\n",
    "            'corrupted_images': [],\n",
    "            'corrupted_masks': [],\n",
    "            'missing_pairs': [],\n",
    "            'size_mismatches': []\n",
    "        }\n",
    "        \n",
    "        # 获取所有文件\n",
    "        image_files = list(self.image_dir.glob('*'))\n",
    "        mask_files = list(self.mask_dir.glob('*'))\n",
    "        \n",
    "        # 过滤有效的图像文件\n",
    "        valid_image_files = [f for f in image_files if self._is_valid_image_file(f)]\n",
    "        valid_mask_files = [f for f in mask_files if self._is_valid_image_file(f)]\n",
    "        \n",
    "        report['total_images'] = len(valid_image_files)\n",
    "        report['total_masks'] = len(valid_mask_files)\n",
    "        \n",
    "        # 检查图像-掩码配对\n",
    "        for image_file in tqdm(valid_image_files, desc=\"验证数据完整性\"):\n",
    "            image_name = image_file.stem\n",
    "            mask_file = self.mask_dir / f\"{image_name}{image_file.suffix}\"\n",
    "            \n",
    "            if not mask_file.exists():\n",
    "                report['missing_pairs'].append(image_name)\n",
    "                continue\n",
    "                \n",
    "            # 检查图像是否损坏\n",
    "            try:\n",
    "                img = cv2.imread(str(image_file), cv2.IMREAD_GRAYSCALE)\n",
    "                if img is None:\n",
    "                    report['corrupted_images'].append(str(image_file))\n",
    "                    continue\n",
    "                    \n",
    "                mask = cv2.imread(str(mask_file), cv2.IMREAD_GRAYSCALE)\n",
    "                if mask is None:\n",
    "                    report['corrupted_masks'].append(str(mask_file))\n",
    "                    continue\n",
    "                    \n",
    "                # 检查尺寸是否匹配\n",
    "                if img.shape != mask.shape:\n",
    "                    report['size_mismatches'].append({\n",
    "                        'image': str(image_file),\n",
    "                        'image_shape': img.shape,\n",
    "                        'mask_shape': mask.shape\n",
    "                    })\n",
    "                    continue\n",
    "                    \n",
    "                report['matched_pairs'] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"验证文件时出错 {image_file}: {e}\")\n",
    "                report['corrupted_images'].append(str(image_file))\n",
    "                \n",
    "        # 判断是否通过验证\n",
    "        is_valid = (\n",
    "            report['matched_pairs'] > 0 and\n",
    "            len(report['corrupted_images']) == 0 and\n",
    "            len(report['corrupted_masks']) == 0 and\n",
    "            len(report['missing_pairs']) == 0\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"数据验证完成: {report['matched_pairs']} 对有效数据\")\n",
    "        \n",
    "        if not is_valid:\n",
    "            self.logger.warning(\"发现数据问题，请检查验证报告\")\n",
    "            \n",
    "        return is_valid, report\n",
    "        \n",
    "    def _is_valid_image_file(self, file_path: Path) -> bool:\n",
    "        \"\"\"检查是否为有效的图像文件\"\"\"\n",
    "        if not file_path.is_file():\n",
    "            return False\n",
    "        return file_path.suffix.lower() in {'.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.tif'}\n",
    "        \n",
    "    def generate_metadata(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        生成元数据CSV文件\n",
    "        \n",
    "        Returns:\n",
    "            元数据DataFrame\n",
    "        \"\"\"\n",
    "        self.logger.info(\"生成元数据...\")\n",
    "        \n",
    "        metadata_list = []\n",
    "        \n",
    "        # 获取所有有效的图像文件\n",
    "        image_files = [f for f in self.image_dir.glob('*') if self._is_valid_image_file(f)]\n",
    "        \n",
    "        for image_file in tqdm(image_files, desc=\"处理元数据\"):\n",
    "            image_name = image_file.stem\n",
    "            mask_file = self.mask_dir / f\"{image_name}{image_file.suffix}\"\n",
    "            \n",
    "            # 跳过没有对应掩码的图像\n",
    "            if not mask_file.exists():\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # 读取图像信息\n",
    "                img = cv2.imread(str(image_file), cv2.IMREAD_GRAYSCALE)\n",
    "                mask = cv2.imread(str(mask_file), cv2.IMREAD_GRAYSCALE)\n",
    "                \n",
    "                if img is None or mask is None:\n",
    "                    continue\n",
    "                    \n",
    "                # 计算图像统计信息\n",
    "                img_stats = {\n",
    "                    'mean': float(np.mean(img)),\n",
    "                    'std': float(np.std(img)),\n",
    "                    'min': int(np.min(img)),\n",
    "                    'max': int(np.max(img))\n",
    "                }\n",
    "                \n",
    "                # 计算掩码统计信息\n",
    "                mask_binary = (mask > 127).astype(np.uint8)\n",
    "                mask_stats = {\n",
    "                    'lung_pixels': int(np.sum(mask_binary)),\n",
    "                    'total_pixels': int(mask_binary.size),\n",
    "                    'lung_ratio': float(np.sum(mask_binary) / mask_binary.size)\n",
    "                }\n",
    "                \n",
    "                # 计算文件哈希（用于去重）\n",
    "                image_hash = self._calculate_file_hash(image_file)\n",
    "                mask_hash = self._calculate_file_hash(mask_file)\n",
    "                \n",
    "                metadata_list.append({\n",
    "                    'image_id': image_name,\n",
    "                    'image_path': str(image_file.relative_to(self.root_dir)),\n",
    "                    'mask_path': str(mask_file.relative_to(self.root_dir)),\n",
    "                    'image_height': img.shape[0],\n",
    "                    'image_width': img.shape[1],\n",
    "                    'image_mean': img_stats['mean'],\n",
    "                    'image_std': img_stats['std'],\n",
    "                    'image_min': img_stats['min'],\n",
    "                    'image_max': img_stats['max'],\n",
    "                    'lung_pixels': mask_stats['lung_pixels'],\n",
    "                    'total_pixels': mask_stats['total_pixels'],\n",
    "                    'lung_ratio': mask_stats['lung_ratio'],\n",
    "                    'image_hash': image_hash,\n",
    "                    'mask_hash': mask_hash,\n",
    "                    'file_size_bytes': image_file.stat().st_size\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"处理文件时出错 {image_file}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        # 创建DataFrame\n",
    "        metadata_df = pd.DataFrame(metadata_list)\n",
    "        \n",
    "        # 去重（基于图像哈希）\n",
    "        initial_count = len(metadata_df)\n",
    "        metadata_df = metadata_df.drop_duplicates(subset=['image_hash'], keep='first')\n",
    "        final_count = len(metadata_df)\n",
    "        \n",
    "        if initial_count != final_count:\n",
    "            self.logger.info(f\"检测到重复数据: {initial_count - final_count} 个重复样本已移除\")\n",
    "            \n",
    "        # 保存元数据\n",
    "        metadata_path = self.root_dir / self.data_config['metadata_file']\n",
    "        metadata_df.to_csv(metadata_path, index=False)\n",
    "        \n",
    "        self.logger.info(f\"元数据生成完成: {len(metadata_df)} 个样本，保存至 {metadata_path}\")\n",
    "        \n",
    "        return metadata_df\n",
    "        \n",
    "    def _calculate_file_hash(self, file_path: Path) -> str:\n",
    "        \"\"\"计算文件的MD5哈希值\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "        \n",
    "    def create_data_splits(self, metadata_df: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        创建训练/验证/测试数据分割\n",
    "        \n",
    "        Args:\n",
    "            metadata_df: 元数据DataFrame，如果为None则从文件加载\n",
    "            \n",
    "        Returns:\n",
    "            包含分割信息的DataFrame\n",
    "        \"\"\"\n",
    "        if metadata_df is None:\n",
    "            metadata_path = self.root_dir / self.data_config['metadata_file']\n",
    "            if not metadata_path.exists():\n",
    "                raise FileNotFoundError(f\"元数据文件不存在: {metadata_path}\")\n",
    "            metadata_df = pd.read_csv(metadata_path)\n",
    "            \n",
    "        self.logger.info(\"创建数据分割...\")\n",
    "        \n",
    "        # 获取分割比例\n",
    "        train_ratio = self.data_config['train_ratio']\n",
    "        val_ratio = self.data_config['val_ratio']\n",
    "        test_ratio = self.data_config['test_ratio']\n",
    "        \n",
    "        # 分层分割（基于肺部像素比例）\n",
    "        # 将肺部比例分为几个层级以确保各集合的分布均衡\n",
    "        metadata_df['lung_ratio_bin'] = pd.cut(\n",
    "            metadata_df['lung_ratio'], \n",
    "            bins=5, \n",
    "            labels=['very_low', 'low', 'medium', 'high', 'very_high']\n",
    "        )\n",
    "        \n",
    "        # 第一次分割：训练集 vs (验证集+测试集)\n",
    "        train_df, temp_df = train_test_split(\n",
    "            metadata_df,\n",
    "            test_size=(val_ratio + test_ratio),\n",
    "            random_state=self.config['experiment']['seed'],\n",
    "            stratify=metadata_df['lung_ratio_bin']\n",
    "        )\n",
    "        \n",
    "        # 第二次分割：验证集 vs 测试集\n",
    "        val_df, test_df = train_test_split(\n",
    "            temp_df,\n",
    "            test_size=(test_ratio / (val_ratio + test_ratio)),\n",
    "            random_state=self.config['experiment']['seed'],\n",
    "            stratify=temp_df['lung_ratio_bin']\n",
    "        )\n",
    "        \n",
    "        # 为每个样本添加分割标签\n",
    "        split_dict = {}\n",
    "        for idx in train_df.index:\n",
    "            split_dict[idx] = 'train'\n",
    "        for idx in val_df.index:\n",
    "            split_dict[idx] = 'val'\n",
    "        for idx in test_df.index:\n",
    "            split_dict[idx] = 'test'\n",
    "            \n",
    "        metadata_df['split'] = metadata_df.index.map(split_dict)\n",
    "        \n",
    "        # 移除辅助列\n",
    "        metadata_df = metadata_df.drop('lung_ratio_bin', axis=1)\n",
    "        \n",
    "        # 保存分割信息\n",
    "        split_path = self.root_dir / self.data_config['split_file']\n",
    "        metadata_df.to_csv(split_path, index=False)\n",
    "        \n",
    "        # 打印分割统计\n",
    "        split_counts = metadata_df['split'].value_counts()\n",
    "        split_ratios = split_counts / len(metadata_df)\n",
    "        \n",
    "        self.logger.info(\"数据分割完成:\")\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            count = split_counts.get(split, 0)\n",
    "            ratio = split_ratios.get(split, 0)\n",
    "            self.logger.info(f\"  {split}: {count} 样本 ({ratio:.1%})\")\n",
    "            \n",
    "        split_path = self.root_dir / self.data_config['split_file']\n",
    "        self.logger.info(f\"分割信息保存至: {split_path}\")\n",
    "        \n",
    "        return metadata_df\n",
    "        \n",
    "     def prepare_lung_masks_only(self) -> bool:\n",
    "        \"\"\"\n",
    "        确保掩码只包含肺部区域（二值化处理）\n",
    "        \n",
    "        Returns:\n",
    "            处理是否成功\n",
    "        \"\"\"\n",
    "        self.logger.info(\"处理肺部掩码...\")\n",
    "        \n",
    "        mask_files = list(self.mask_dir.glob('*'))\n",
    "        valid_mask_files = [f for f in mask_files if self._is_valid_image_file(f)]\n",
    "        \n",
    "        processed_count = 0\n",
    "        error_count = 0\n",
    "        \n",
    "        for mask_file in tqdm(valid_mask_files, desc=\"处理掩码文件\"):\n",
    "            try:\n",
    "                # 读取掩码\n",
    "                mask = cv2.imread(str(mask_file), cv2.IMREAD_GRAYSCALE)\n",
    "                if mask is None:\n",
    "                    self.logger.warning(f\"无法读取掩码文件: {mask_file}\")\n",
    "                    error_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # 二值化处理：假设肺部区域为白色（高像素值）\n",
    "                # 使用Otsu阈值或固定阈值\n",
    "                _, binary_mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "                \n",
    "                # 形态学操作：去除噪声，填充孔洞\n",
    "                kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "                \n",
    "                # 开运算：去除小噪声\n",
    "                binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_OPEN, kernel)\n",
    "                \n",
    "                # 闭运算：填充小孔洞\n",
    "                binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n",
    "                \n",
    "                # 保存处理后的掩码\n",
    "                cv2.imwrite(str(mask_file), binary_mask)\n",
    "                processed_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"处理掩码文件时出错 {mask_file}: {e}\")\n",
    "                error_count += 1\n",
    "                \n",
    "        self.logger.info(f\"掩码处理完成: {processed_count} 个文件处理成功，{error_count} 个文件出错\")\n",
    "        \n",
    "        return error_count == 0\n",
    "        \n",
    "    def run_full_preparation(self, \n",
    "                           download_url: Optional[str] = None,\n",
    "                           zip_path: Optional[str] = None,\n",
    "                           force_download: bool = False) -> bool:\n",
    "        \"\"\"\n",
    "        运行完整的数据准备流程\n",
    "        \n",
    "        Args:\n",
    "            download_url: 数据下载链接\n",
    "            zip_path: 本地zip文件路径\n",
    "            force_download: 是否强制重新下载\n",
    "            \n",
    "        Returns:\n",
    "            准备是否成功\n",
    "        \"\"\"\n",
    "        self.logger.info(\"开始完整数据准备流程...\")\n",
    "        \n",
    "        try:\n",
    "            # 步骤1: 下载数据（如果提供了URL）\n",
    "            if download_url:\n",
    "                if not self.download_data(download_url, force_download):\n",
    "                    self.logger.error(\"数据下载失败\")\n",
    "                    return False\n",
    "                    \n",
    "            # 步骤2: 解压数据\n",
    "            if zip_path or download_url:\n",
    "                if not self.extract_data(zip_path):\n",
    "                    self.logger.error(\"数据解压失败\")\n",
    "                    return False\n",
    "                    \n",
    "            # 步骤3: 验证数据完整性\n",
    "            is_valid, report = self.validate_data_integrity()\n",
    "            if not is_valid:\n",
    "                self.logger.error(\"数据验证失败\")\n",
    "                self.logger.error(f\"验证报告: {json.dumps(report, indent=2)}\")\n",
    "                return False\n",
    "                \n",
    "            # 步骤4: 处理肺部掩码\n",
    "            if not self.prepare_lung_masks_only():\n",
    "                self.logger.warning(\"掩码处理过程中出现错误，但继续执行\")\n",
    "                \n",
    "            # 步骤5: 生成元数据\n",
    "            metadata_df = self.generate_metadata()\n",
    "            if metadata_df.empty:\n",
    "                self.logger.error(\"元数据生成失败\")\n",
    "                return False\n",
    "                \n",
    "            # 步骤6: 创建数据分割\n",
    "            split_df = self.create_data_splits(metadata_df)\n",
    "            \n",
    "            self.logger.info(\"数据准备流程完成！\")\n",
    "            self.logger.info(f\"总样本数: {len(split_df)}\")\n",
    "            \n",
    "            # 保存最终报告\n",
    "            final_report = {\n",
    "                'total_samples': len(split_df),\n",
    "                'data_splits': split_df['split'].value_counts().to_dict(),\n",
    "                'integrity_report': report,\n",
    "                'config_used': self.config\n",
    "            }\n",
    "            \n",
    "            report_path = self.root_dir / \"preparation_report.json\"\n",
    "            with open(report_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(final_report, f, indent=2, ensure_ascii=False)\n",
    "                \n",
    "            self.logger.info(f\"准备报告保存至: {report_path}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"数据准备过程中出现未预期错误: {e}\")\n",
    "            return False\n",
    "\n",
    "# ==============================================================================\n",
    "# data/__init__.py\n",
    "\n",
    "from .prepare_data import ChestXrayDataPreparer\n",
    "\n",
    "__all__ = ['ChestXrayDataPreparer']\n",
    "\n",
    "# ==============================================================================\n",
    "# data/data_utils.py\n",
    "# 数据处理工具函数\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional\n",
    "import logging\n",
    "\n",
    "def load_image_and_mask(image_path: str, mask_path: str) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    加载图像和掩码\n",
    "    \n",
    "    Args:\n",
    "        image_path: 图像文件路径\n",
    "        mask_path: 掩码文件路径\n",
    "        \n",
    "    Returns:\n",
    "        (图像数组, 掩码数组)，失败时返回None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if image is None or mask is None:\n",
    "            return None, None\n",
    "            \n",
    "        # 确保掩码是二值的\n",
    "        mask = (mask > 127).astype(np.uint8)\n",
    "        \n",
    "        return image, mask\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"加载图像或掩码失败: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def resize_image_and_mask(image: np.ndarray, \n",
    "                         mask: np.ndarray, \n",
    "                         target_size: Tuple[int, int]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    调整图像和掩码大小\n",
    "    \n",
    "    Args:\n",
    "        image: 输入图像\n",
    "        mask: 输入掩码\n",
    "        target_size: 目标尺寸 (height, width)\n",
    "        \n",
    "    Returns:\n",
    "        调整大小后的图像和掩码\n",
    "    \"\"\"\n",
    "    resized_image = cv2.resize(image, (target_size[1], target_size[0]), interpolation=cv2.INTER_LINEAR)\n",
    "    resized_mask = cv2.resize(mask, (target_size[1], target_size[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    # 确保掩码仍然是二值的\n",
    "    resized_mask = (resized_mask > 0.5).astype(np.uint8)\n",
    "    \n",
    "    return resized_image, resized_mask\n",
    "\n",
    "def normalize_image(image: np.ndarray, method: str = 'min_max') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    图像归一化\n",
    "    \n",
    "    Args:\n",
    "        image: 输入图像\n",
    "        method: 归一化方法 ('min_max', 'z_score', 'histogram_eq')\n",
    "        \n",
    "    Returns:\n",
    "        归一化后的图像\n",
    "    \"\"\"\n",
    "    if method == 'min_max':\n",
    "        return (image - image.min()) / (image.max() - image.min() + 1e-8)\n",
    "    elif method == 'z_score':\n",
    "        return (image - image.mean()) / (image.std() + 1e-8)\n",
    "    elif method == 'histogram_eq':\n",
    "        return cv2.equalizeHist(image.astype(np.uint8)).astype(np.float32) / 255.0\n",
    "    else:\n",
    "        raise ValueError(f\"未知的归一化方法: {method}\")\n",
    "\n",
    "def calculate_lung_statistics(mask: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    计算肺部区域统计信息\n",
    "    \n",
    "    Args:\n",
    "        mask: 二值掩码\n",
    "        \n",
    "    Returns:\n",
    "        统计信息字典\n",
    "    \"\"\"\n",
    "    lung_pixels = np.sum(mask > 0)\n",
    "    total_pixels = mask.size\n",
    "    lung_ratio = lung_pixels / total_pixels\n",
    "    \n",
    "    # 查找肺部连通组件\n",
    "    num_labels, labels = cv2.connectedComponents(mask.astype(np.uint8))\n",
    "    \n",
    "    # 计算每个连通组件的面积\n",
    "    component_areas = []\n",
    "    for i in range(1, num_labels):  # 跳过背景（标签0）\n",
    "        area = np.sum(labels == i)\n",
    "        component_areas.append(area)\n",
    "        \n",
    "    return {\n",
    "        'lung_pixels': lung_pixels,\n",
    "        'total_pixels': total_pixels,\n",
    "        'lung_ratio': lung_ratio,\n",
    "        'num_components': num_labels - 1,  # 减去背景\n",
    "        'largest_component': max(component_areas) if component_areas else 0,\n",
    "        'component_areas': component_areas\n",
    "    }\n",
    "\n",
    "# ==============================================================================\n",
    "# CLI脚本示例\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    import sys\n",
    "    sys.path.append('..')\n",
    "    from configs import ConfigLoader\n",
    "    \n",
    "    # 设置日志\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    \n",
    "    # 命令行参数\n",
    "    parser = argparse.ArgumentParser(description=\"胸片数据准备\")\n",
    "    parser.add_argument('--config', type=str, default='default', \n",
    "                       help='配置文件名称（不含.yaml后缀）')\n",
    "    parser.add_argument('--download-url', type=str, default=None,\n",
    "                       help='数据下载链接')\n",
    "    parser.add_argument('--zip-path', type=str, default=None,\n",
    "                       help='本地zip文件路径')\n",
    "    parser.add_argument('--force-download', action='store_true',\n",
    "                       help='强制重新下载')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    try:\n",
    "        # 加载配置\n",
    "        config_loader = ConfigLoader()\n",
    "        config = config_loader.load_config(args.config)\n",
    "        \n",
    "        # 创建数据准备器\n",
    "        preparer = ChestXrayDataPreparer(config)\n",
    "        \n",
    "        # 运行完整准备流程\n",
    "        success = preparer.run_full_preparation(\n",
    "            download_url=args.download_url,\n",
    "            zip_path=args.zip_path,\n",
    "            force_download=args.force_download\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            print(\"✅ 数据准备完成！\")\n",
    "            sys.exit(0)\n",
    "        else:\n",
    "            print(\"❌ 数据准备失败！\")\n",
    "            sys.exit(1)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 运行出错: {e}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f4b3ff-3380-4f67-b153-5220eaf6ebfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5ef9fe-3e7a-4671-ae96-320520e4fcda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4183b5-f50e-4294-ac75-5d883bf9f16b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41cd1fc-af87-4dd6-bff7-adeb16d68a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets/chest_xray_dataset.py\n",
    "# 胸片数据集封装，支持train/val/test三种模式\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, Tuple, List, Callable\n",
    "import logging\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class ChestXrayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    胸片分割数据集\n",
    "    \n",
    "    支持train/val/test三种模式，集成Albumentations数据增强\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 config: Dict[str, Any],\n",
    "                 split: str = 'train',\n",
    "                 transform: Optional[Callable] = None,\n",
    "                 cache_images: bool = False):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        \n",
    "        Args:\n",
    "            config: 配置字典\n",
    "            split: 数据分割 ('train', 'val', 'test')\n",
    "            transform: 数据变换/增强管道\n",
    "            cache_images: 是否缓存图像到内存\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.data_config = config['data']\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.cache_images = cache_images\n",
    "        \n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # 数据路径\n",
    "        self.root_dir = Path(self.data_config['root_dir'])\n",
    "        self.image_dir = self.root_dir / self.data_config['image_dir']\n",
    "        self.mask_dir = self.root_dir / self.data_config['mask_dir']\n",
    "        \n",
    "        # 加载数据分割信息\n",
    "        self._load_split_data()\n",
    "        \n",
    "        # 图像缓存\n",
    "        self.image_cache = {} if cache_images else None\n",
    "        self.mask_cache = {} if cache_images else None\n",
    "        \n",
    "        self.logger.info(f\"初始化 {split} 数据集: {len(self.data)} 个样本\")\n",
    "        \n",
    "    def _load_split_data(self):\n",
    "        \"\"\"加载数据分割信息\"\"\"\n",
    "        split_file = self.root_dir / self.data_config['split_file']\n",
    "        \n",
    "        if not split_file.exists():\n",
    "            raise FileNotFoundError(f\"数据分割文件不存在: {split_file}\")\n",
    "            \n",
    "        # 读取完整数据\n",
    "        full_data = pd.read_csv(split_file)\n",
    "        \n",
    "        # 过滤当前分割的数据\n",
    "        self.data = full_data[full_data['split'] == self.split].reset_index(drop=True)\n",
    "        \n",
    "        if len(self.data) == 0:\n",
    "            raise ValueError(f\"在 {self.split} 分割中未找到数据\")\n",
    "            \n",
    "        # 验证文件存在性\n",
    "        self._validate_file_existence()\n",
    "        \n",
    "    def _validate_file_existence(self):\n",
    "        \"\"\"验证文件存在性\"\"\"\n",
    "        valid_indices = []\n",
    "        \n",
    "        for idx, row in self.data.iterrows():\n",
    "            image_path = self.root_dir / row['image_path']\n",
    "            mask_path = self.root_dir / row['mask_path']\n",
    "            \n",
    "            if image_path.exists() and mask_path.exists():\n",
    "                valid_indices.append(idx)\n",
    "            else:\n",
    "                self.logger.warning(f\"文件不存在，跳过样本: {row['image_id']}\")\n",
    "                \n",
    "        # 只保留有效样本\n",
    "        self.data = self.data.iloc[valid_indices].reset_index(drop=True)\n",
    "        \n",
    "        if len(self.data) == 0:\n",
    "            raise ValueError(f\"在 {self.split} 分割中未找到有效文件\")\n",
    "            \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"返回数据集大小\"\"\"\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        获取单个样本\n",
    "        \n",
    "        Args:\n",
    "            idx: 样本索引\n",
    "            \n",
    "        Returns:\n",
    "            包含image、mask等信息的字典\n",
    "        \"\"\"\n",
    "        if idx >= len(self.data):\n",
    "            raise IndexError(f\"索引 {idx} 超出数据集大小 {len(self.data)}\")\n",
    "            \n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # 获取图像和掩码\n",
    "        image, mask = self._load_image_and_mask(row)\n",
    "        \n",
    "        # 应用变换\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "            \n",
    "        # 确保是tensor格式\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = torch.from_numpy(image).float()\n",
    "        if not isinstance(mask, torch.Tensor):\n",
    "            mask = torch.from_numpy(mask).float()\n",
    "            \n",
    "        # 添加通道维度（如果需要）\n",
    "        if len(image.shape) == 2:  # H, W -> 1, H, W\n",
    "            image = image.unsqueeze(0)\n",
    "        if len(mask.shape) == 2:  # H, W -> 1, H, W\n",
    "            mask = mask.unsqueeze(0)\n",
    "            \n",
    "        # 归一化图像到[0,1]\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "            \n",
    "        # 确保掩码是[0,1]\n",
    "        mask = mask.float()\n",
    "        if mask.max() > 1.0:\n",
    "            mask = mask / 255.0\n",
    "            \n",
    "        return {\n",
    "            'image': image,\n",
    "            'mask': mask,\n",
    "            'image_id': row['image_id'],\n",
    "            'image_path': row['image_path'],\n",
    "            'mask_path': row['mask_path'],\n",
    "            'metadata': {\n",
    "                'lung_ratio': row.get('lung_ratio', 0.0),\n",
    "                'image_height': row.get('image_height', 0),\n",
    "                'image_width': row.get('image_width', 0)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def _load_image_and_mask(self, row: pd.Series) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        加载图像和掩码\n",
    "        \n",
    "        Args:\n",
    "            row: 数据行\n",
    "            \n",
    "        Returns:\n",
    "            (图像数组, 掩码数组)\n",
    "        \"\"\"\n",
    "        image_id = row['image_id']\n",
    "        \n",
    "        # 检查缓存\n",
    "        if self.cache_images and image_id in self.image_cache:\n",
    "            return self.image_cache[image_id], self.mask_cache[image_id]\n",
    "            \n",
    "        # 构建文件路径\n",
    "        image_path = self.root_dir / row['image_path']\n",
    "        mask_path = self.root_dir / row['mask_path']\n",
    "        \n",
    "        try:\n",
    "            # 读取图像和掩码\n",
    "            image = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "            mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            if image is None:\n",
    "                raise ValueError(f\"无法读取图像: {image_path}\")\n",
    "            if mask is None:\n",
    "                raise ValueError(f\"无法读取掩码: {mask_path}\")\n",
    "                \n",
    "            # 确保掩码是二值的\n",
    "            mask = (mask > 127).astype(np.uint8)\n",
    "            \n",
    "            # 缓存图像（如果启用）\n",
    "            if self.cache_images:\n",
    "                self.image_cache[image_id] = image.copy()\n",
    "                self.mask_cache[image_id] = mask.copy()\n",
    "                \n",
    "            return image, mask\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"加载样本 {image_id} 时出错: {e}\")\n",
    "            \n",
    "            # 返回默认图像\n",
    "            default_size = (self.data_config['image_size'], self.data_config['image_size'])\n",
    "            image = np.zeros(default_size, dtype=np.uint8)\n",
    "            mask = np.zeros(default_size, dtype=np.uint8)\n",
    "            \n",
    "            return image, mask\n",
    "            \n",
    "    def get_sample_by_id(self, image_id: str) -> Optional[Dict[str, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        根据image_id获取样本\n",
    "        \n",
    "        Args:\n",
    "            image_id: 图像ID\n",
    "            \n",
    "        Returns:\n",
    "            样本字典，如果未找到返回None\n",
    "        \"\"\"\n",
    "        matching_rows = self.data[self.data['image_id'] == image_id]\n",
    "        \n",
    "        if len(matching_rows) == 0:\n",
    "            return None\n",
    "            \n",
    "        idx = matching_rows.index[0]\n",
    "        return self.__getitem__(idx)\n",
    "        \n",
    "    def get_class_distribution(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        获取类别分布（基于肺部像素比例）\n",
    "        \n",
    "        Returns:\n",
    "            类别分布字典\n",
    "        \"\"\"\n",
    "        if 'lung_ratio' not in self.data.columns:\n",
    "            self.logger.warning(\"元数据中缺少lung_ratio信息\")\n",
    "            return {}\n",
    "            \n",
    "        lung_ratios = self.data['lung_ratio']\n",
    "        \n",
    "        return {\n",
    "            'mean_lung_ratio': float(lung_ratios.mean()),\n",
    "            'std_lung_ratio': float(lung_ratios.std()),\n",
    "            'min_lung_ratio': float(lung_ratios.min()),\n",
    "            'max_lung_ratio': float(lung_ratios.max()),\n",
    "            'median_lung_ratio': float(lung_ratios.median())\n",
    "        }\n",
    "        \n",
    "    def get_data_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        获取数据集统计信息\n",
    "        \n",
    "        Returns:\n",
    "            统计信息字典\n",
    "        \"\"\"\n",
    "        stats = {\n",
    "            'num_samples': len(self.data),\n",
    "            'split': self.split,\n",
    "            'cache_enabled': self.cache_images,\n",
    "            'transform_enabled': self.transform is not None\n",
    "        }\n",
    "        \n",
    "        # 添加类别分布\n",
    "        stats.update(self.get_class_distribution())\n",
    "        \n",
    "        # 添加图像尺寸分布\n",
    "        if 'image_height' in self.data.columns and 'image_width' in self.data.columns:\n",
    "            stats.update({\n",
    "                'height_range': (int(self.data['image_height'].min()), \n",
    "                               int(self.data['image_height'].max())),\n",
    "                'width_range': (int(self.data['image_width'].min()), \n",
    "                              int(self.data['image_width'].max())),\n",
    "                'most_common_size': tuple(self.data.groupby(['image_height', 'image_width']).size().idxmax())\n",
    "            })\n",
    "            \n",
    "        return stats\n",
    "        \n",
    "    def clear_cache(self):\n",
    "        \"\"\"清空图像缓存\"\"\"\n",
    "        if self.cache_images:\n",
    "            self.image_cache.clear()\n",
    "            self.mask_cache.clear()\n",
    "            self.logger.info(\"图像缓存已清空\")\n",
    "\n",
    "# ==============================================================================\n",
    "# datasets/data_loader.py\n",
    "# 数据加载器工厂\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from typing import Dict, Any, Optional\n",
    "import numpy as np\n",
    "\n",
    "class DataLoaderFactory:\n",
    "    \"\"\"数据加载器工厂类\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.data_config = config['data']\n",
    "        self.training_config = config['training']\n",
    "        \n",
    "    def create_dataloader(self, \n",
    "                         dataset: ChestXrayDataset,\n",
    "                         shuffle: Optional[bool] = None,\n",
    "                         batch_size: Optional[int] = None,\n",
    "                         num_workers: Optional[int] = None,\n",
    "                         use_weighted_sampling: bool = False) -> DataLoader:\n",
    "        \"\"\"\n",
    "        创建数据加载器\n",
    "        \n",
    "        Args:\n",
    "            dataset: 数据集实例\n",
    "            shuffle: 是否打乱数据，None时根据split自动判断\n",
    "            batch_size: 批次大小\n",
    "            num_workers: 工作线程数\n",
    "            use_weighted_sampling: 是否使用加权采样\n",
    "            \n",
    "        Returns:\n",
    "            数据加载器\n",
    "        \"\"\"\n",
    "        # 默认参数\n",
    "        if shuffle is None:\n",
    "            shuffle = dataset.split == 'train'\n",
    "        if batch_size is None:\n",
    "            batch_size = self.training_config['batch_size']\n",
    "        if num_workers is None:\n",
    "            num_workers = self.data_config['num_workers']\n",
    "            \n",
    "        # 采样器\n",
    "        sampler = None\n",
    "        if use_weighted_sampling and dataset.split == 'train':\n",
    "            sampler = self._create_weighted_sampler(dataset)\n",
    "            shuffle = False  # 使用采样器时不能同时shuffle\n",
    "            \n",
    "        return DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=self.data_config['pin_memory'],\n",
    "            drop_last=dataset.split == 'train',  # 训练时丢弃最后不完整的batch\n",
    "            persistent_workers=num_workers > 0\n",
    "        )\n",
    "        \n",
    "    def _create_weighted_sampler(self, dataset: ChestXrayDataset) -> WeightedRandomSampler:\n",
    "        \"\"\"\n",
    "        创建加权随机采样器（基于肺部像素比例）\n",
    "        \n",
    "        Args:\n",
    "            dataset: 数据集实例\n",
    "            \n",
    "        Returns:\n",
    "            加权随机采样器\n",
    "        \"\"\"\n",
    "        if 'lung_ratio' not in dataset.data.columns:\n",
    "            raise ValueError(\"数据集元数据中缺少lung_ratio信息，无法创建加权采样器\")\n",
    "            \n",
    "        lung_ratios = dataset.data['lung_ratio'].values\n",
    "        \n",
    "        # 将肺部比例分为几个区间\n",
    "        bins = np.linspace(0, lung_ratios.max(), 6)  # 5个区间\n",
    "        bin_indices = np.digitize(lung_ratios, bins) - 1\n",
    "        bin_indices = np.clip(bin_indices, 0, len(bins) - 2)\n",
    "        \n",
    "        # 计算每个区间的权重（反比于样本数量）\n",
    "        unique_bins, bin_counts = np.unique(bin_indices, return_counts=True)\n",
    "        bin_weights = 1.0 / bin_counts\n",
    "        \n",
    "        # 为每个样本分配权重\n",
    "        sample_weights = np.zeros(len(dataset))\n",
    "        for bin_idx, weight in zip(unique_bins, bin_weights):\n",
    "            mask = bin_indices == bin_idx\n",
    "            sample_weights[mask] = weight\n",
    "            \n",
    "        return WeightedRandomSampler(\n",
    "            weights=sample_weights,\n",
    "            num_samples=len(dataset),\n",
    "            replacement=True\n",
    "        )\n",
    "        \n",
    "    def create_all_dataloaders(self, \n",
    "                              transforms: Dict[str, Any],\n",
    "                              cache_images: bool = False,\n",
    "                              use_weighted_sampling: bool = False) -> Dict[str, DataLoader]:\n",
    "        \"\"\"\n",
    "        创建所有分割的数据加载器\n",
    "        \n",
    "        Args:\n",
    "            transforms: 变换字典 {'train': transform, 'val': transform, 'test': transform}\n",
    "            cache_images: 是否缓存图像\n",
    "            use_weighted_sampling: 是否对训练集使用加权采样\n",
    "            \n",
    "        Returns:\n",
    "            数据加载器字典\n",
    "        \"\"\"\n",
    "        dataloaders = {}\n",
    "        \n",
    "        for split in ['train', 'val', 'test']:\n",
    "            # 创建数据集\n",
    "            dataset = ChestXrayDataset(\n",
    "                config=self.config,\n",
    "                split=split,\n",
    "                transform=transforms.get(split),\n",
    "                cache_images=cache_images\n",
    "            )\n",
    "            \n",
    "            # 创建数据加载器\n",
    "            use_sampling = use_weighted_sampling and split == 'train'\n",
    "            dataloader = self.create_dataloader(\n",
    "                dataset=dataset,\n",
    "                use_weighted_sampling=use_sampling\n",
    "            )\n",
    "            \n",
    "            dataloaders[split] = dataloader\n",
    "            \n",
    "        return dataloaders\n",
    "\n",
    "# ==============================================================================\n",
    "# datasets/__init__.py\n",
    "\n",
    "from .chest_xray_dataset import ChestXrayDataset\n",
    "from .data_loader import DataLoaderFactory\n",
    "\n",
    "__all__ = ['ChestXrayDataset', 'DataLoaderFactory']\n",
    "\n",
    "# ==============================================================================\n",
    "# datasets/utils.py\n",
    "# 数据集相关工具函数\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import cv2\n",
    "\n",
    "def visualize_dataset_samples(dataset: ChestXrayDataset, \n",
    "                            num_samples: int = 4,\n",
    "                            figsize: Tuple[int, int] = (15, 10)) -> None:\n",
    "    \"\"\"\n",
    "    可视化数据集样本\n",
    "    \n",
    "    Args:\n",
    "        dataset: 数据集实例\n",
    "        num_samples: 显示样本数量\n",
    "        figsize: 图像大小\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=figsize)\n",
    "    \n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "        \n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        sample = dataset[i]\n",
    "        \n",
    "        image = sample['image'].squeeze().numpy()\n",
    "        mask = sample['mask'].squeeze().numpy()\n",
    "        image_id = sample['image_id']\n",
    "        \n",
    "        # 原图\n",
    "        axes[i, 0].imshow(image, cmap='gray')\n",
    "        axes[i, 0].set_title(f'Image: {image_id}')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # 掩码\n",
    "        axes[i, 1].imshow(mask, cmap='gray')\n",
    "        axes[i, 1].set_title('Mask')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # 叠加显示\n",
    "        overlay = np.stack([image, image, image], axis=-1)\n",
    "        overlay[:, :, 1] = np.maximum(overlay[:, :, 1], mask * 0.5)  # 绿色叠加\n",
    "        axes[i, 2].imshow(overlay)\n",
    "        axes[i, 2].set_title('Overlay')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_dataset_statistics(datasets: Dict[str, ChestXrayDataset]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    分析数据集统计信息\n",
    "    \n",
    "    Args:\n",
    "        datasets: 数据集字典 {'train': dataset, 'val': dataset, 'test': dataset}\n",
    "        \n",
    "    Returns:\n",
    "        统计信息字典\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    for split, dataset in datasets.items():\n",
    "        split_stats = dataset.get_data_statistics()\n",
    "        stats[split] = split_stats\n",
    "        \n",
    "    # 计算总体统计\n",
    "    total_samples = sum(stats[split]['num_samples'] for split in stats)\n",
    "    stats['total'] = {\n",
    "        'num_samples': total_samples,\n",
    "        'split_distribution': {\n",
    "            split: stats[split]['num_samples'] / total_samples \n",
    "            for split in stats if split != 'total'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def plot_data_distribution(datasets: Dict[str, ChestXrayDataset],\n",
    "                          figsize: Tuple[int, int] = (15, 5)) -> None:\n",
    "    \"\"\"\n",
    "    绘制数据分布图\n",
    "    \n",
    "    Args:\n",
    "        datasets: 数据集字典\n",
    "        figsize: 图像大小\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    \n",
    "    # 样本数量分布\n",
    "    splits = list(datasets.keys())\n",
    "    counts = [len(dataset) for dataset in datasets.values()]\n",
    "    \n",
    "    axes[0].bar(splits, counts, color=['blue', 'orange', 'green'])\n",
    "    axes[0].set_title('Sample Count by Split')\n",
    "    axes[0].set_ylabel('Number of Samples')\n",
    "    \n",
    "    # 肺部像素比例分布\n",
    "    for i, (split, dataset) in enumerate(datasets.items()):\n",
    "        if 'lung_ratio' in dataset.data.columns:\n",
    "            lung_ratios = dataset.data['lung_ratio'].values\n",
    "            axes[1].hist(lung_ratios, bins=30, alpha=0.7, label=split)\n",
    "            \n",
    "    axes[1].set_title('Lung Ratio Distribution')\n",
    "    axes[1].set_xlabel('Lung Pixel Ratio')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # 图像尺寸分布\n",
    "    all_heights = []\n",
    "    all_widths = []\n",
    "    \n",
    "    for dataset in datasets.values():\n",
    "        if 'image_height' in dataset.data.columns:\n",
    "            all_heights.extend(dataset.data['image_height'].values)\n",
    "            all_widths.extend(dataset.data['image_width'].values)\n",
    "            \n",
    "    if all_heights and all_widths:\n",
    "        axes[2].scatter(all_widths, all_heights, alpha=0.6)\n",
    "        axes[2].set_title('Image Size Distribution')\n",
    "        axes[2].set_xlabel('Width')\n",
    "        axes[2].set_ylabel('Height')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def validate_batch_data(batch: Dict[str, torch.Tensor]) -> bool:\n",
    "    \"\"\"\n",
    "    验证批次数据的有效性\n",
    "    \n",
    "    Args:\n",
    "        batch: 批次数据\n",
    "        \n",
    "    Returns:\n",
    "        是否有效\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = batch['image']\n",
    "        mask = batch['mask']\n",
    "        \n",
    "        # 检查tensor类型\n",
    "        if not isinstance(image, torch.Tensor) or not isinstance(mask, torch.Tensor):\n",
    "            return False\n",
    "            \n",
    "        # 检查形状\n",
    "        if len(image.shape) != 4 or len(mask.shape) != 4:  # [B, C, H, W]\n",
    "            return False\n",
    "            \n",
    "        # 检查批次大小匹配\n",
    "        if image.shape[0] != mask.shape[0]:\n",
    "            return False\n",
    "            \n",
    "        # 检查数值范围\n",
    "        if torch.any(image < 0) or torch.any(image > 1):\n",
    "            return False\n",
    "            \n",
    "        if torch.any(mask < 0) or torch.any(mask > 1):\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def calculate_dataset_mean_std(dataset: ChestXrayDataset, \n",
    "                             num_samples: int = 1000) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    计算数据集的均值和标准差（用于归一化）\n",
    "    \n",
    "    Args:\n",
    "        dataset: 数据集实例\n",
    "        num_samples: 采样样本数量\n",
    "        \n",
    "    Returns:\n",
    "        (均值, 标准差)\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    # 随机采样\n",
    "    indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
    "    \n",
    "    pixel_values = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        sample = dataset[idx]\n",
    "        image = sample['image'].numpy()\n",
    "        pixel_values.extend(image.flatten())\n",
    "        \n",
    "    pixel_values = np.array(pixel_values)\n",
    "    \n",
    "    return float(np.mean(pixel_values)), float(np.std(pixel_values))\n",
    "\n",
    "# ==============================================================================\n",
    "# 测试脚本示例\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    sys.path.append('..')\n",
    "    from configs import ConfigLoader\n",
    "    from transforms import get_transforms\n",
    "    \n",
    "    # 加载配置\n",
    "    config_loader = ConfigLoader()\n",
    "    config = config_loader.load_config('default')\n",
    "    \n",
    "    # 创建数据集\n",
    "    train_transform = get_transforms('train', config)\n",
    "    val_transform = get_transforms('val', config)\n",
    "    \n",
    "    datasets = {}\n",
    "    for split, transform in [('train', train_transform), ('val', val_transform), ('test', val_transform)]:\n",
    "        try:\n",
    "            dataset = ChestXrayDataset(\n",
    "                config=config,\n",
    "                split=split,\n",
    "                transform=transform,\n",
    "                cache_images=False\n",
    "            )\n",
    "            datasets[split] = dataset\n",
    "            print(f\"✅ {split} 数据集创建成功: {len(dataset)} 个样本\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {split} 数据集创建失败: {e}\")\n",
    "            \n",
    "    if datasets:\n",
    "        # 可视化样本\n",
    "        print(\"\\n📊 可视化数据集样本...\")\n",
    "        visualize_dataset_samples(datasets['train'], num_samples=3)\n",
    "        \n",
    "        # 分析统计信息\n",
    "        print(\"\\n📈 分析数据集统计信息...\")\n",
    "        stats = analyze_dataset_statistics(datasets)\n",
    "        for split, split_stats in stats.items():\n",
    "            print(f\"{split}: {split_stats}\")\n",
    "            \n",
    "        # 绘制分布图\n",
    "        print(\"\\n📉 绘制数据分布...\")\n",
    "        plot_data_distribution(datasets)\n",
    "        \n",
    "        # 测试数据加载器\n",
    "        print(\"\\n🔄 测试数据加载器...\")\n",
    "        factory = DataLoaderFactory(config)\n",
    "        \n",
    "        try:\n",
    "            train_loader = factory.create_dataloader(datasets['train'])\n",
    "            batch = next(iter(train_loader))\n",
    "            \n",
    "            print(f\"批次形状: image={batch['image'].shape}, mask={batch['mask'].shape}\")\n",
    "            print(f\"数据有效性: {validate_batch_data(batch)}\")\n",
    "            \n",
    "            # 计算数据集均值和标准差\n",
    "            mean, std = calculate_dataset_mean_std(datasets['train'], num_samples=100)\n",
    "            print(f\"数据集统计: mean={mean:.4f}, std={std:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 数据加载器测试失败: {e}\")\n",
    "            \n",
    "    print(\"\\n✅ 数据集模块测试完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c6650-b539-438b-80a6-b3fd14a2bd2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b510c1-a995-42d8-8832-638827e12234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17635504-7e7e-46ce-9703-f1c4b3da4c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a4878-37be-4dcc-b7f6-53d03b6c6a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms/pipeline.py\n",
    "# 数据预处理和增强流水线模块\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import numpy as np\n",
    "from typing import Dict, Any, List, Optional, Callable\n",
    "import logging\n",
    "\n",
    "class TransformPipeline:\n",
    "    \"\"\"数据变换流水线管理器\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.data_config = config['data']\n",
    "        self.aug_config = config['augmentation']\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def get_base_transforms(self, image_size: int) -> List[A.BasicTransform]:\n",
    "        \"\"\"\n",
    "        获取基础变换（resize等）\n",
    "        \n",
    "        Args:\n",
    "            image_size: 目标图像大小\n",
    "            \n",
    "        Returns:\n",
    "            基础变换列表\n",
    "        \"\"\"\n",
    "        return [\n",
    "            A.Resize(height=image_size, width=image_size, interpolation=cv2.INTER_LINEAR),\n",
    "        ]\n",
    "        \n",
    "    def get_training_augmentations(self) -> List[A.BasicTransform]:\n",
    "        \"\"\"\n",
    "        获取训练时的数据增强\n",
    "        \n",
    "        Returns:\n",
    "            训练增强变换列表\n",
    "        \"\"\"\n",
    "        augmentations = []\n",
    "        \n",
    "        # 水平翻转\n",
    "        if self.aug_config['horizontal_flip'] > 0:\n",
    "            augmentations.append(\n",
    "                A.HorizontalFlip(p=self.aug_config['horizontal_flip'])\n",
    "            )\n",
    "            \n",
    "        # 几何变换：平移、缩放、旋转\n",
    "        if self.aug_config['shift_scale_rotate']['p'] > 0:\n",
    "            ssr_config = self.aug_config['shift_scale_rotate']\n",
    "            augmentations.append(\n",
    "                A.ShiftScaleRotate(\n",
    "                    shift_limit=ssr_config['shift_limit'],\n",
    "                    scale_limit=ssr_config['scale_limit'],\n",
    "                    rotate_limit=ssr_config['rotate_limit'],\n",
    "                    interpolation=cv2.INTER_LINEAR,\n",
    "                    border_mode=cv2.BORDER_CONSTANT,\n",
    "                    value=0,\n",
    "                    mask_value=0,\n",
    "                    p=ssr_config['p']\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        # 亮度和对比度调整\n",
    "        if self.aug_config['brightness_contrast']['p'] > 0:\n",
    "            bc_config = self.aug_config['brightness_contrast']\n",
    "            augmentations.append(\n",
    "                A.RandomBrightnessContrast(\n",
    "                    brightness_limit=bc_config['brightness_limit'],\n",
    "                    contrast_limit=bc_config['contrast_limit'],\n",
    "                    p=bc_config['p']\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        # 弹性变换\n",
    "        if self.aug_config['elastic_transform']['p'] > 0:\n",
    "            et_config = self.aug_config['elastic_transform']\n",
    "            augmentations.append(\n",
    "                A.ElasticTransform(\n",
    "                    alpha=et_config['alpha'],\n",
    "                    sigma=et_config['sigma'],\n",
    "                    alpha_affine=et_config['alpha_affine'],\n",
    "                    interpolation=cv2.INTER_LINEAR,\n",
    "                    border_mode=cv2.BORDER_CONSTANT,\n",
    "                    value=0,\n",
    "                    mask_value=0,\n",
    "                    p=et_config['p']\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        # 噪声和模糊\n",
    "        if self.aug_config['noise_blur']['p'] > 0:\n",
    "            nb_config = self.aug_config['noise_blur']\n",
    "            noise_blur_augmentations = A.OneOf([\n",
    "                A.GaussNoise(\n",
    "                    var_limit=tuple(nb_config['gauss_noise_var']),\n",
    "                    mean=0,\n",
    "                    p=1.0\n",
    "                ),\n",
    "                A.GaussianBlur(\n",
    "                    blur_limit=nb_config['gauss_blur_limit'],\n",
    "                    p=1.0\n",
    "                ),\n",
    "            ], p=nb_config['p'])\n",
    "            augmentations.append(noise_blur_augmentations)\n",
    "            \n",
    "        return augmentations\n",
    "        \n",
    "    def get_validation_augmentations(self) -> List[A.BasicTransform]:\n",
    "        \"\"\"\n",
    "        获取验证/测试时的变换（通常只有基础变换）\n",
    "        \n",
    "        Returns:\n",
    "            验证变换列表\n",
    "        \"\"\"\n",
    "        return []  # 验证时不使用数据增强\n",
    "        \n",
    "    def get_normalization_transforms(self) -> List[A.BasicTransform]:\n",
    "        \"\"\"\n",
    "        获取归一化变换\n",
    "        \n",
    "        Returns:\n",
    "            归一化变换列表\n",
    "        \"\"\"\n",
    "        return [\n",
    "            A.Normalize(\n",
    "                mean=(0.0,),  # 灰度图像\n",
    "                std=(1.0,),\n",
    "                max_pixel_value=255.0\n",
    "            ),\n",
    "        ]\n",
    "        \n",
    "    def get_tensor_transforms(self) -> List[A.BasicTransform]:\n",
    "        \"\"\"\n",
    "        获取张量转换\n",
    "        \n",
    "        Returns:\n",
    "            张量转换列表\n",
    "        \"\"\"\n",
    "        return [ToTensorV2()]\n",
    "        \n",
    "    def build_transform_pipeline(self, \n",
    "                               phase: str = 'train',\n",
    "                               image_size: Optional[int] = None,\n",
    "                               additional_transforms: Optional[List[A.BasicTransform]] = None) -> A.Compose:\n",
    "        \"\"\"\n",
    "        构建完整的变换流水线\n",
    "        \n",
    "        Args:\n",
    "            phase: 阶段 ('train', 'val', 'test')\n",
    "            image_size: 图像大小，None时使用配置中的默认值\n",
    "            additional_transforms: 额外的变换\n",
    "            \n",
    "        Returns:\n",
    "            完整的变换流水线\n",
    "        \"\"\"\n",
    "        if image_size is None:\n",
    "            image_size = self.data_config['image_size']\n",
    "            \n",
    "        transforms = []\n",
    "        \n",
    "        # 1. 基础变换\n",
    "        transforms.extend(self.get_base_transforms(image_size))\n",
    "        \n",
    "        # 2. 数据增强（仅训练时）\n",
    "        if phase == 'train':\n",
    "            transforms.extend(self.get_training_augmentations())\n",
    "        else:\n",
    "            transforms.extend(self.get_validation_augmentations())\n",
    "            \n",
    "        # 3. 额外变换\n",
    "        if additional_transforms:\n",
    "            transforms.extend(additional_transforms)\n",
    "            \n",
    "        # 4. 归一化（可选）\n",
    "        # transforms.extend(self.get_normalization_transforms())\n",
    "        \n",
    "        # 5. 张量转换（可选，取决于是否在Dataset中处理）\n",
    "        # transforms.extend(self.get_tensor_transforms())\n",
    "        \n",
    "        # 构建流水线\n",
    "        pipeline = A.Compose(\n",
    "            transforms,\n",
    "            additional_targets={},  # 可以添加额外的目标，如关键点等\n",
    "            p=1.0\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"构建 {phase} 变换流水线: {len(transforms)} 个变换\")\n",
    "        \n",
    "        return pipeline\n",
    "\n",
    "# ==============================================================================\n",
    "# transforms/medical_specific.py\n",
    "# 医学图像特定的变换\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "\n",
    "class MedicalImageTransforms:\n",
    "    \"\"\"医学图像特定的变换\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def clahe_enhancement(clip_limit: float = 2.0, tile_grid_size: tuple = (8, 8)) -> A.CLAHE:\n",
    "        \"\"\"\n",
    "        对比度受限的自适应直方图均衡化\n",
    "        \n",
    "        Args:\n",
    "            clip_limit: 对比度限制\n",
    "            tile_grid_size: 网格大小\n",
    "            \n",
    "        Returns:\n",
    "            CLAHE变换\n",
    "        \"\"\"\n",
    "        return A.CLAHE(\n",
    "            clip_limit=clip_limit,\n",
    "            tile_grid_size=tile_grid_size,\n",
    "            p=1.0\n",
    "        )\n",
    "        \n",
    "    @staticmethod\n",
    "    def histogram_matching(reference_image: np.ndarray) -> Callable:\n",
    "        \"\"\"\n",
    "        直方图匹配变换\n",
    "        \n",
    "        Args:\n",
    "            reference_image: 参考图像\n",
    "            \n",
    "        Returns:\n",
    "            直方图匹配函数\n",
    "        \"\"\"\n",
    "        def histogram_match_transform(image, **kwargs):\n",
    "            \"\"\"直方图匹配实现\"\"\"\n",
    "            # 计算累积分布函数\n",
    "            def get_histogram_cdf(img):\n",
    "                hist, bins = np.histogram(img.flatten(), 256, [0, 256])\n",
    "                cdf = hist.cumsum()\n",
    "                cdf_normalized = cdf * hist.max() / cdf.max()\n",
    "                return cdf_normalized\n",
    "                \n",
    "            # 获取参考图像和当前图像的CDF\n",
    "            ref_cdf = get_histogram_cdf(reference_image)\n",
    "            img_cdf = get_histogram_cdf(image)\n",
    "            \n",
    "            # 创建查找表\n",
    "            lut = np.zeros(256, dtype=np.uint8)\n",
    "            for i in range(256):\n",
    "                diff = np.abs(ref_cdf - img_cdf[i])\n",
    "                lut[i] = np.argmin(diff)\n",
    "                \n",
    "            # 应用查找表\n",
    "            matched_image = cv2.LUT(image, lut)\n",
    "            \n",
    "            return matched_image\n",
    "            \n",
    "        return A.Lambda(image=histogram_match_transform, p=1.0)\n",
    "        \n",
    "    @staticmethod\n",
    "    def lung_window_adjustment(window_center: int = 40, window_width: int = 350) -> Callable:\n",
    "        \"\"\"\n",
    "        肺窗调整（模拟CT的窗口调整）\n",
    "        \n",
    "        Args:\n",
    "            window_center: 窗口中心\n",
    "            window_width: 窗口宽度\n",
    "            \n",
    "        Returns:\n",
    "            窗口调整函数\n",
    "        \"\"\"\n",
    "        def window_adjust(image, **kwargs):\n",
    "            # 计算窗口范围\n",
    "            window_min = window_center - window_width // 2\n",
    "            window_max = window_center + window_width // 2\n",
    "            \n",
    "            # 应用窗口\n",
    "            windowed = np.clip(image, window_min, window_max)\n",
    "            \n",
    "            # 归一化到0-255\n",
    "            windowed = ((windowed - window_min) / (window_max - window_min) * 255).astype(np.uint8)\n",
    "            \n",
    "            return windowed\n",
    "            \n",
    "        return A.Lambda(image=window_adjust, p=1.0)\n",
    "        \n",
    "    @staticmethod\n",
    "    def random_gamma_correction(gamma_range: tuple = (0.8, 1.2)) -> A.RandomGamma:\n",
    "        \"\"\"\n",
    "        随机伽马校正\n",
    "        \n",
    "        Args:\n",
    "            gamma_range: 伽马值范围\n",
    "            \n",
    "        Returns:\n",
    "            伽马校正变换\n",
    "        \"\"\"\n",
    "        return A.RandomGamma(\n",
    "            gamma_limit=gamma_range,\n",
    "            p=1.0\n",
    "        )\n",
    "\n",
    "# ==============================================================================\n",
    "# transforms/custom_transforms.py\n",
    "# 自定义变换\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "class CustomLungTransforms:\n",
    "    \"\"\"肺部分割专用的自定义变换\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def lung_crop_transform(crop_margin: float = 0.1) -> Callable:\n",
    "        \"\"\"\n",
    "        基于肺部掩码的智能裁剪\n",
    "        \n",
    "        Args:\n",
    "            crop_margin: 裁剪边距比例\n",
    "            \n",
    "        Returns:\n",
    "            智能裁剪函数\n",
    "        \"\"\"\n",
    "        def smart_lung_crop(image, mask, **kwargs):\n",
    "            # 找到肺部区域的边界框\n",
    "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            if not contours:\n",
    "                return image, mask\n",
    "                \n",
    "            # 获取所有轮廓的边界框\n",
    "            x_coords = []\n",
    "            y_coords = []\n",
    "            \n",
    "            for contour in contours:\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                x_coords.extend([x, x + w])\n",
    "                y_coords.extend([y, y + h])\n",
    "                \n",
    "            if not x_coords or not y_coords:\n",
    "                return image, mask\n",
    "                \n",
    "            # 计算总体边界框\n",
    "            x_min, x_max = min(x_coords), max(x_coords)\n",
    "            y_min, y_max = min(y_coords), max(y_coords)\n",
    "            \n",
    "            # 添加边距\n",
    "            h, w = image.shape[:2]\n",
    "            margin_x = int((x_max - x_min) * crop_margin)\n",
    "            margin_y = int((y_max - y_min) * crop_margin)\n",
    "            \n",
    "            x_min = max(0, x_min - margin_x)\n",
    "            x_max = min(w, x_max + margin_x)\n",
    "            y_min = max(0, y_min - margin_y)\n",
    "            y_max = min(h, y_max + margin_y)\n",
    "            \n",
    "            # 裁剪图像和掩码\n",
    "            cropped_image = image[y_min:y_max, x_min:x_max]\n",
    "            cropped_mask = mask[y_min:y_max, x_min:x_max]\n",
    "            \n",
    "            return cropped_image, cropped_mask\n",
    "            \n",
    "        return A.Lambda(image=smart_lung_crop, mask=smart_lung_crop, p=1.0)\n",
    "        \n",
    "    @staticmethod\n",
    "    def random_lung_simulation() -> Callable:\n",
    "        \"\"\"\n",
    "        随机肺部状态模拟（模拟不同的肺部充气状态等）\n",
    "        \"\"\"\n",
    "        def lung_simulation(image, mask, **kwargs):\n",
    "            # 随机选择模拟类型\n",
    "            simulation_type = np.random.choice(['normal', 'inflate', 'deflate'])\n",
    "            \n",
    "            if simulation_type == 'normal':\n",
    "                return image, mask\n",
    "            elif simulation_type == 'inflate':\n",
    "                # 轻微膨胀肺部区域\n",
    "                kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "                inflated_mask = cv2.dilate(mask, kernel, iterations=1)\n",
    "                return image, inflated_mask\n",
    "            else:  # deflate\n",
    "                # 轻微收缩肺部区域\n",
    "                kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "                deflated_mask = cv2.erode(mask, kernel, iterations=1)\n",
    "                return image, deflated_mask\n",
    "                \n",
    "        return A.Lambda(image=lung_simulation, mask=lung_simulation, p=0.3)\n",
    "\n",
    "# ==============================================================================\n",
    "# transforms/factory.py\n",
    "# 变换工厂函数\n",
    "\n",
    "def get_transforms(phase: str, \n",
    "                  config: Dict[str, Any],\n",
    "                  enable_medical_transforms: bool = True,\n",
    "                  enable_custom_transforms: bool = False) -> A.Compose:\n",
    "    \"\"\"\n",
    "    获取指定阶段的变换流水线\n",
    "    \n",
    "    Args:\n",
    "        phase: 阶段 ('train', 'val', 'test')\n",
    "        config: 配置字典\n",
    "        enable_medical_transforms: 是否启用医学图像专用变换\n",
    "        enable_custom_transforms: 是否启用自定义变换\n",
    "        \n",
    "    Returns:\n",
    "        变换流水线\n",
    "    \"\"\"\n",
    "    # 创建流水线管理器\n",
    "    pipeline_manager = TransformPipeline(config)\n",
    "    \n",
    "    # 构建额外变换\n",
    "    additional_transforms = []\n",
    "    \n",
    "    # 添加医学图像专用变换\n",
    "    if enable_medical_transforms and phase == 'train':\n",
    "        # CLAHE增强（适度使用）\n",
    "        additional_transforms.append(\n",
    "            A.OneOf([\n",
    "                MedicalImageTransforms.clahe_enhancement(clip_limit=2.0),\n",
    "                MedicalImageTransforms.random_gamma_correction(gamma_range=(0.9, 1.1)),\n",
    "            ], p=0.3)\n",
    "        )\n",
    "        \n",
    "    # 添加自定义变换\n",
    "    if enable_custom_transforms and phase == 'train':\n",
    "        additional_transforms.extend([\n",
    "            CustomLungTransforms.random_lung_simulation(),\n",
    "        ])\n",
    "        \n",
    "    # 构建完整流水线\n",
    "    transform_pipeline = pipeline_manager.build_transform_pipeline(\n",
    "        phase=phase,\n",
    "        additional_transforms=additional_transforms if additional_transforms else None\n",
    "    )\n",
    "    \n",
    "    return transform_pipeline\n",
    "\n",
    "def get_test_time_augmentation_transforms(config: Dict[str, Any],\n",
    "                                        num_augmentations: int = 5) -> List[A.Compose]:\n",
    "    \"\"\"\n",
    "    获取测试时增强（TTA）的变换列表\n",
    "    \n",
    "    Args:\n",
    "        config: 配置字典\n",
    "        num_augmentations: 增强数量\n",
    "        \n",
    "    Returns:\n",
    "        TTA变换列表\n",
    "    \"\"\"\n",
    "    image_size = config['data']['image_size']\n",
    "    \n",
    "    tta_transforms = []\n",
    "    \n",
    "    # 原始图像（无增强）\n",
    "    tta_transforms.append(\n",
    "        A.Compose([\n",
    "            A.Resize(height=image_size, width=image_size),\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # 水平翻转\n",
    "    tta_transforms.append(\n",
    "        A.Compose([\n",
    "            A.Resize(height=image_size, width=image_size),\n",
    "            A.HorizontalFlip(p=1.0),\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # 轻微旋转\n",
    "    for angle in [-5, 5]:\n",
    "        if len(tta_transforms) < num_augmentations:\n",
    "            tta_transforms.append(\n",
    "                A.Compose([\n",
    "                    A.Resize(height=image_size, width=image_size),\n",
    "                    A.Rotate(limit=(angle, angle), p=1.0),\n",
    "                ])\n",
    "            )\n",
    "            \n",
    "    # 轻微缩放\n",
    "    if len(tta_transforms) < num_augmentations:\n",
    "        tta_transforms.append(\n",
    "            A.Compose([\n",
    "                A.Resize(height=image_size, width=image_size),\n",
    "                A.RandomScale(scale_limit=0.1, p=1.0),\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "    return tta_transforms[:num_augmentations]\n",
    "\n",
    "# ==============================================================================\n",
    "# transforms/__init__.py\n",
    "\n",
    "from .pipeline import TransformPipeline\n",
    "from .medical_specific import MedicalImageTransforms\n",
    "from .custom_transforms import CustomLungTransforms\n",
    "from .factory import get_transforms, get_test_time_augmentation_transforms\n",
    "\n",
    "__all__ = [\n",
    "    'TransformPipeline',\n",
    "    'MedicalImageTransforms', \n",
    "    'CustomLungTransforms',\n",
    "    'get_transforms',\n",
    "    'get_test_time_augmentation_transforms'\n",
    "]\n",
    "\n",
    "# ==============================================================================\n",
    "# 测试脚本\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    sys.path.append('..')\n",
    "    from configs import ConfigLoader\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # 加载配置\n",
    "    config_loader = ConfigLoader()\n",
    "    config = config_loader.load_config('default')\n",
    "    \n",
    "    # 创建测试图像和掩码\n",
    "    test_image = np.random.randint(0, 255, (512, 512), dtype=np.uint8)\n",
    "    test_mask = np.zeros((512, 512), dtype=np.uint8)\n",
    "    test_mask[100:400, 100:400] = 255  # 简单的矩形掩码\n",
    "    \n",
    "    print(\"🔄 测试变换流水线...\")\n",
    "    \n",
    "    # 测试各阶段的变换\n",
    "    for phase in ['train', 'val', 'test']:\n",
    "        try:\n",
    "            transform = get_transforms(phase, config)\n",
    "            \n",
    "            # 应用变换\n",
    "            transformed = transform(image=test_image, mask=test_mask)\n",
    "            transformed_image = transformed['image']\n",
    "            transformed_mask = transformed['mask']\n",
    "            \n",
    "            print(f\"✅ {phase} 变换成功: \"\n",
    "                  f\"图像形状 {transformed_image.shape}, \"\n",
    "                  f\"掩码形状 {transformed_mask.shape}\")\n",
    "                  \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {phase} 变换失败: {e}\")\n",
    "            \n",
    "    # 测试TTA变换\n",
    "    print(\"\\n🔄 测试TTA变换...\")\n",
    "    try:\n",
    "        tta_transforms = get_test_time_augmentation_transforms(config, num_augmentations=3)\n",
    "        \n",
    "        for i, transform in enumerate(tta_transforms):\n",
    "            transformed = transform(image=test_image, mask=test_mask)\n",
    "            print(f\"✅ TTA {i+1} 变换成功: \"\n",
    "                  f\"图像形状 {transformed['image'].shape}\")\n",
    "                  \n",
    "    except Exception as e:\n",
    "        print(f\"❌ TTA变换失败: {e}\")\n",
    "        \n",
    "    print(\"\\n✅ 变换模块测试完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f23901-1bdf-48ef-bb22-d9e01b966d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e49f5-62b4-44ad-9b6a-f04533370778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd5a0a1-7557-4ad8-ba35-0edc87832826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749566bc-4e8e-4959-8c38-8553b011fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses.py\n",
    "# 统一损失函数接口，支持Dice、BCE、组合损失等\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import Dict, Any, Optional, Callable\n",
    "import logging\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Dice损失函数\n",
    "    适用于分割任务，处理类别不平衡问题\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, smooth: float = 1e-6, square_denominator: bool = False):\n",
    "        \"\"\"\n",
    "        初始化Dice损失\n",
    "        \n",
    "        Args:\n",
    "            smooth: 平滑项，避免除零\n",
    "            square_denominator: 是否对分母进行平方\n",
    "        \"\"\"\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.square_denominator = square_denominator\n",
    "        \n",
    "    def forward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        计算Dice损失\n",
    "        \n",
    "        Args:\n",
    "            predictions: 模型预测 [B, C, H, W] (logits)\n",
    "            targets: 真实标签 [B, C, H, W] (0-1)\n",
    "            \n",
    "        Returns:\n",
    "            Dice损失值\n",
    "        \"\"\"\n",
    "        # 应用sigmoid获取概率\n",
    "        predictions = torch.sigmoid(predictions)\n",
    "        \n",
    "        # 展平tensor\n",
    "        predictions = predictions.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # 计算交集\n",
    "        intersection = (predictions * targets).sum()\n",
    "        \n",
    "        # 计算分母\n",
    "        if self.square_denominator:\n",
    "            denominator = (predictions ** 2).sum() + (targets ** 2).sum()\n",
    "        else:\n",
    "            denominator = predictions.sum() + targets.sum()\n",
    "            \n",
    "        # 计算Dice系数\n",
    "        dice = (2. * intersection + self.smooth) / (denominator + self.smooth)\n",
    "        \n",
    "        # 返回Dice损失 (1 - Dice)\n",
    "        return 1 - dice\n",
    "\n",
    "class IoULoss(nn.Module):\n",
    "    \"\"\"\n",
    "    IoU (Intersection over Union) 损失函数\n",
    "    也称为Jaccard损失\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, smooth: float = 1e-6):\n",
    "        super(IoULoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        计算IoU损失\n",
    "        \n",
    "        Args:\n",
    "            predictions: 模型预测 [B, C, H, W] (logits)\n",
    "            targets: 真实标签 [B, C, H, W] (0-1)\n",
    "            \n",
    "        Returns:\n",
    "            IoU损失值\n",
    "        \"\"\"\n",
    "        # 应用sigmoid获取概率\n",
    "        predictions = torch.sigmoid(predictions)\n",
    "        \n",
    "        # 展平tensor\n",
    "        predictions = predictions.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # 计算交集和并集\n",
    "        intersection = (predictions * targets).sum()\n",
    "        union = predictions.sum() + targets.sum() - intersection\n",
    "        \n",
    "        # 计算IoU\n",
    "        iou = (intersection + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        # 返回IoU损失 (1 - IoU)\n",
    "        return 1 - iou\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal损失函数\n",
    "    解决类别不平衡和困难样本问题\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float = 1.0, gamma: float = 2.0, reduce: bool = True):\n",
    "        \"\"\"\n",
    "        初始化Focal损失\n",
    "        \n",
    "        Args:\n",
    "            alpha: 平衡因子\n",
    "            gamma: 聚焦参数\n",
    "            reduce: 是否对batch求平均\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduce = reduce\n",
    "        \n",
    "    def forward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        计算Focal损失\n",
    "        \n",
    "        Args:\n",
    "            predictions: 模型预测 [B, C, H, W] (logits)\n",
    "            targets: 真实标签 [B, C, H, W] (0-1)\n",
    "            \n",
    "        Returns:\n",
    "            Focal损失值\n",
    "        \"\"\"\n",
    "        # 计算BCE损失\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(predictions, targets, reduce=False)\n",
    "        \n",
    "        # 计算概率\n",
    "        probs = torch.sigmoid(predictions)\n",
    "        \n",
    "        # 计算调制因子\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)\n",
    "        modulating_factor = (1 - p_t) ** self.gamma\n",
    "        \n",
    "        # 计算alpha因子\n",
    "        alpha_factor = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "        \n",
    "        # 应用Focal权重\n",
    "        focal_loss = alpha_factor * modulating_factor * bce_loss\n",
    "        \n",
    "        if self.reduce:\n",
    "            return focal_loss.mean()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class TverskyLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Tversky损失函数\n",
    "    Dice损失的泛化，可以调节假正例和假负例的权重\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float = 0.5, beta: float = 0.5, smooth: float = 1e-6):\n",
    "        \"\"\"\n",
    "        初始化Tversky损失\n",
    "        \n",
    "        Args:\n",
    "            alpha: 假正例权重\n",
    "            beta: 假负例权重\n",
    "            smooth: 平滑项\n",
    "        \"\"\"\n",
    "        super(TverskyLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        计算Tversky损失\n",
    "        \n",
    "        Args:\n",
    "            predictions: 模型预测 [B, C, H, W] (logits)\n",
    "            targets: 真实标签 [B, C, H, W] (0-1)\n",
    "            \n",
    "        Returns:\n",
    "            Tversky损失值\n",
    "        \"\"\"\n",
    "        # 应用sigmoid获取概率\n",
    "        predictions = torch.sigmoid(predictions)\n",
    "        \n",
    "        # 展平tensor\n",
    "        predictions = predictions.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # 计算真正例、假正例、假负例\n",
    "        true_pos = (predictions * targets).sum()\n",
    "        false_pos = (predictions * (1 - targets)).sum()\n",
    "        false_neg = ((1 - predictions) * targets).sum()\n",
    "        \n",
    "        # 计算Tversky指数\n",
    "        tversky = (true_pos + self.smooth) / (\n",
    "            true_pos + self.alpha * false_pos + self.beta * false_neg + self.smooth\n",
    "        )\n",
    "        \n",
    "        # 返回Tversky损失\n",
    "        return 1 - tversky\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    组合损失函数\n",
    "    支持多种损失函数的加权组合\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 loss_functions: Dict[str, nn.Module],\n",
    "                 weights: Dict[str, float]):\n",
    "        \"\"\"\n",
    "        初始化组合损失\n",
    "        \n",
    "        Args:\n",
    "            loss_functions: 损失函数字典 {'name': loss_fn}\n",
    "            weights: 权重字典 {'name': weight}\n",
    "        \"\"\"\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.loss_functions = nn.ModuleDict(loss_functions)\n",
    "        self.weights = weights\n",
    "        \n",
    "        # 验证权重\n",
    "        if set(loss_functions.keys()) != set(weights.keys()):\n",
    "            raise ValueError(\"损失函数和权重的键必须匹配\")\n",
    "            \n",
    "    def forward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        计算组合损失\n",
    "        \n",
    "        Args:\n",
    "            predictions: 模型预测\n",
    "            targets: 真实标签\n",
    "            \n",
    "        Returns:\n",
    "            组合损失值\n",
    "        \"\"\"\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for name, loss_fn in self.loss_functions.items():\n",
    "            weight = self.weights[name]\n",
    "            loss_value = loss_fn(predictions, targets)\n",
    "            total_loss += weight * loss_value\n",
    "            \n",
    "        return total_loss\n",
    "\n",
    "class BoundaryLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    边界损失函数\n",
    "    关注分割边界的准确性\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, theta0: float = 3, theta: float = 5):\n",
    "        super(BoundaryLoss, self).__init__()\n",
    "        self.theta0 = theta0\n",
    "        self.theta = theta\n",
    "        \n",
    "    def forward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        计算边界损失\n",
    "        \n",
    "        Args:\n",
    "            predictions: 模型预测 [B, C, H, W] (logits)\n",
    "            targets: 真实标签 [B, C, H, W] (0-1)\n",
    "            \n",
    "        Returns:\n",
    "            边界损失值\n",
    "        \"\"\"\n",
    "        # 应用sigmoid获取概率\n",
    "        predictions = torch.sigmoid(predictions)\n",
    "        \n",
    "        # 计算距离变换 (简化版本)\n",
    "        # 在实际应用中，可能需要使用scipy.ndimage.distance_transform_edt\n",
    "        # 这里使用简化的边界检测\n",
    "        def get_boundary_loss(pred, target):\n",
    "            # 使用Sobel算子检测边界\n",
    "            sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], \n",
    "                                 dtype=torch.float32, device=pred.device).view(1, 1, 3, 3)\n",
    "            sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], \n",
    "                                 dtype=torch.float32, device=pred.device).view(1, 1, 3, 3)\n",
    "            \n",
    "            # 计算梯度\n",
    "            pred_grad_x = F.conv2d(pred.unsqueeze(1), sobel_x, padding=1)\n",
    "            pred_grad_y = F.conv2d(pred.unsqueeze(1), sobel_y, padding=1)\n",
    "            pred_boundary = torch.sqrt(pred_grad_x**2 + pred_grad_y**2)\n",
    "            \n",
    "            target_grad_x = F.conv2d(target.unsqueeze(1), sobel_x, padding=1)\n",
    "            target_grad_y = F.conv2d(target.unsqueeze(1), sobel_y, padding=1)\n",
    "            target_boundary = torch.sqrt(target_grad_x**2 + target_grad_y**2)\n",
    "            \n",
    "            # 计算边界损失\n",
    "            boundary_loss = F.mse_loss(pred_boundary, target_boundary)\n",
    "            return boundary_loss\n",
    "            \n",
    "        batch_size = predictions.shape[0]\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            loss = get_boundary_loss(predictions[i, 0], targets[i, 0])\n",
    "            total_loss += loss\n",
    "            \n",
    "        return total_loss / batch_size\n",
    "\n",
    "# ==============================================================================\n",
    "# 损失函数工厂\n",
    "\n",
    "class LossFactory:\n",
    "    \"\"\"损失函数工厂类\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_loss(loss_config: Dict[str, Any]) -> nn.Module:\n",
    "        \"\"\"\n",
    "        创建损失函数\n",
    "        \n",
    "        Args:\n",
    "            loss_config: 损失函数配置\n",
    "            \n",
    "        Returns:\n",
    "            损失函数实例\n",
    "        \"\"\"\n",
    "        loss_type = loss_config.get('type', 'combined')\n",
    "        \n",
    "        if loss_type == 'dice':\n",
    "            return DiceLoss(\n",
    "                smooth=loss_config.get('smooth', 1e-6),\n",
    "                square_denominator=loss_config.get('square_denominator', False)\n",
    "            )\n",
    "            \n",
    "        elif loss_type == 'bce':\n",
    "            return nn.BCEWithLogitsLoss(\n",
    "                weight=loss_config.get('weight'),\n",
    "                pos_weight=loss_config.get('pos_weight')\n",
    "            )\n",
    "            \n",
    "        elif loss_type == 'iou':\n",
    "            return IoULoss(\n",
    "                smooth=loss_config.get('smooth', 1e-6)\n",
    "            )\n",
    "            \n",
    "        elif loss_type == 'focal':\n",
    "            return FocalLoss(\n",
    "                alpha=loss_config.get('alpha', 1.0),\n",
    "                gamma=loss_config.get('gamma', 2.0),\n",
    "                reduce=loss_config.get('reduce', True)\n",
    "            )\n",
    "            \n",
    "        elif loss_type == 'tversky':\n",
    "            return TverskyLoss(\n",
    "                alpha=loss_config.get('alpha', 0.5),\n",
    "                beta=loss_config.get('beta', 0.5),\n",
    "                smooth=loss_config.get('smooth', 1e-6)\n",
    "            )\n",
    "            \n",
    "        elif loss_type == 'boundary':\n",
    "            return BoundaryLoss(\n",
    "                theta0=loss_config.get('theta0', 3),\n",
    "                theta=loss_config.get('theta', 5)\n",
    "            )\n",
    "            \n",
    "        elif loss_type == 'combined':\n",
    "            # 创建组合损失\n",
    "            loss_functions = {}\n",
    "            weights = {}\n",
    "            \n",
    "            # BCE损失\n",
    "            if 'bce' in loss_config.get('components', {}):\n",
    "                bce_config = loss_config['components']['bce']\n",
    "                loss_functions['bce'] = nn.BCEWithLogitsLoss()\n",
    "                weights['bce'] = bce_config.get('weight', 0.5)\n",
    "                \n",
    "            # Dice损失\n",
    "            if 'dice' in loss_config.get('components', {}):\n",
    "                dice_config = loss_config['components']['dice']\n",
    "                loss_functions['dice'] = DiceLoss(\n",
    "                    smooth=dice_config.get('smooth', 1e-6)\n",
    "                )\n",
    "                weights['dice'] = dice_config.get('weight', 0.5)\n",
    "                \n",
    "            # IoU损失\n",
    "            if 'iou' in loss_config.get('components', {}):\n",
    "                iou_config = loss_config['components']['iou']\n",
    "                loss_functions['iou'] = IoULoss(\n",
    "                    smooth=iou_config.get('smooth', 1e-6)\n",
    "                )\n",
    "                weights['iou'] = iou_config.get('weight', 0.3)\n",
    "                \n",
    "            # Focal损失\n",
    "            if 'focal' in loss_config.get('components', {}):\n",
    "                focal_config = loss_config['components']['focal']\n",
    "                loss_functions['focal'] = FocalLoss(\n",
    "                    alpha=focal_config.get('alpha', 1.0),\n",
    "                    gamma=focal_config.get('gamma', 2.0)\n",
    "                )\n",
    "                weights['focal'] = focal_config.get('weight', 0.2)\n",
    "                \n",
    "            # 边界损失\n",
    "            if 'boundary' in loss_config.get('components', {}):\n",
    "                boundary_config = loss_config['components']['boundary']\n",
    "                loss_functions['boundary'] = BoundaryLoss(\n",
    "                    theta0=boundary_config.get('theta0', 3),\n",
    "                    theta=boundary_config.get('theta', 5)\n",
    "                )\n",
    "                weights['boundary'] = boundary_config.get('weight', 0.1)\n",
    "                \n",
    "            if not loss_functions:\n",
    "                # 默认组合：BCE + Dice\n",
    "                loss_functions = {\n",
    "                    'bce': nn.BCEWithLogitsLoss(),\n",
    "                    'dice': DiceLoss()\n",
    "                }\n",
    "                weights = {'bce': 0.5, 'dice': 0.5}\n",
    "                \n",
    "            return CombinedLoss(loss_functions, weights)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"未知的损失函数类型: {loss_type}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 便捷函数\n",
    "\n",
    "def get_loss_fn(config: Dict[str, Any]) -> nn.Module:\n",
    "    \"\"\"\n",
    "    从配置获取损失函数\n",
    "    \n",
    "    Args:\n",
    "        config: 完整配置字典\n",
    "        \n",
    "    Returns:\n",
    "        损失函数实例\n",
    "    \"\"\"\n",
    "    # 从训练配置中提取损失权重，构建损失配置\n",
    "    training_config = config.get('training', {})\n",
    "    loss_weights = training_config.get('loss_weights', {'bce': 0.5, 'dice': 0.5})\n",
    "    \n",
    "    # 构建损失配置\n",
    "    loss_config = {\n",
    "        'type': 'combined',\n",
    "        'components': {}\n",
    "    }\n",
    "    \n",
    "    for loss_name, weight in loss_weights.items():\n",
    "        loss_config['components'][loss_name] = {'weight': weight}\n",
    "        \n",
    "    return LossFactory.create_loss(loss_config)\n",
    "\n",
    "def calculate_class_weights(dataset_stats: Dict[str, Any]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    计算类别权重\n",
    "    \n",
    "    Args:\n",
    "        dataset_stats: 数据集统计信息\n",
    "        \n",
    "    Returns:\n",
    "        类别权重tensor\n",
    "    \"\"\"\n",
    "    # 基于肺部像素比例计算权重\n",
    "    mean_lung_ratio = dataset_stats.get('mean_lung_ratio', 0.3)\n",
    "    \n",
    "    # 背景权重：肺部像素比例\n",
    "    # 前景权重：1 - 肺部像素比例\n",
    "    bg_weight = mean_lung_ratio\n",
    "    fg_weight = 1 - mean_lung_ratio\n",
    "    \n",
    "    # 归一化权重\n",
    "    total_weight = bg_weight + fg_weight\n",
    "    bg_weight = bg_weight / total_weight\n",
    "    fg_weight = fg_weight / total_weight\n",
    "    \n",
    "    # 返回正样本权重（用于BCEWithLogitsLoss的pos_weight）\n",
    "    pos_weight = torch.tensor([bg_weight / fg_weight])\n",
    "    \n",
    "    return pos_weight\n",
    "\n",
    "# ==============================================================================\n",
    "# 测试脚本\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 设置日志\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # 创建测试数据\n",
    "    batch_size, channels, height, width = 2, 1, 256, 256\n",
    "    predictions = torch.randn(batch_size, channels, height, width, requires_grad=True)\n",
    "    targets = torch.randint(0, 2, (batch_size, channels, height, width)).float()\n",
    "    \n",
    "    logger.info(\"🔄 测试损失函数...\")\n",
    "    \n",
    "    # 测试各种损失函数\n",
    "    loss_functions = {\n",
    "        'Dice': DiceLoss(),\n",
    "        'BCE': nn.BCEWithLogitsLoss(),\n",
    "        'IoU': IoULoss(),\n",
    "        'Focal': FocalLoss(),\n",
    "        'Tversky': TverskyLoss(),\n",
    "        'Boundary': BoundaryLoss()\n",
    "    }\n",
    "    \n",
    "    for name, loss_fn in loss_functions.items():\n",
    "        try:\n",
    "            loss_value = loss_fn(predictions, targets)\n",
    "            logger.info(f\"✅ {name} 损失: {loss_value.item():.4f}\")\n",
    "            \n",
    "            # 测试反向传播\n",
    "            loss_value.backward(retain_graph=True)\n",
    "            if predictions.grad is not None:\n",
    "                logger.info(f\"   梯度范数: {predictions.grad.norm().item():.4f}\")\n",
    "            predictions.grad = None  # 清除梯度\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ {name} 损失测试失败: {e}\")\n",
    "            \n",
    "    # 测试组合损失\n",
    "    logger.info(\"\\n🔄 测试组合损失...\")\n",
    "    try:\n",
    "        combined_loss_config = {\n",
    "            'type': 'combined',\n",
    "            'components': {\n",
    "                'bce': {'weight': 0.4},\n",
    "                'dice': {'weight': 0.6}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        combined_loss = LossFactory.create_loss(combined_loss_config)\n",
    "        loss_value = combined_loss(predictions, targets)\n",
    "        logger.info(f\"✅ 组合损失: {loss_value.item():.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ 组合损失测试失败: {e}\")\n",
    "        \n",
    "    # 测试配置加载\n",
    "    logger.info(\"\\n🔄 测试配置加载...\")\n",
    "    try:\n",
    "        test_config = {\n",
    "            'training': {\n",
    "                'loss_weights': {\n",
    "                    'bce': 0.3,\n",
    "                    'dice': 0.7\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        loss_fn = get_loss_fn(test_config)\n",
    "        loss_value = loss_fn(predictions, targets)\n",
    "        logger.info(f\"✅ 配置损失: {loss_value.item():.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ 配置损失测试失败: {e}\")\n",
    "        \n",
    "    logger.info(\"\\n✅ 损失函数模块测试完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b96bd6-070e-4e57-82de-5adecfd4cc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8ab815-0883-43b3-b457-ce50435a0b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47394149-e660-4c9e-8e27-de9acf42beb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88610ca6-bf21-4ff1-b832-1cd592f0094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics.py\n",
    "# 像素级评估指标，使用torchmetrics实现\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Dict, Any, List, Optional, Tuple, Union\n",
    "import logging\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# 尝试导入torchmetrics，如果没有则使用自定义实现\n",
    "try:\n",
    "    import torchmetrics\n",
    "    from torchmetrics import MetricCollection\n",
    "    TORCHMETRICS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TORCHMETRICS_AVAILABLE = False\n",
    "    logging.warning(\"torchmetrics未安装，将使用自定义实现\")\n",
    "\n",
    "class BaseMetric(ABC):\n",
    "    \"\"\"基础评估指标类\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.5):\n",
    "        self.threshold = threshold\n",
    "        self.reset()\n",
    "        \n",
    "    @abstractmethod\n",
    "    def update(self, predictions: torch.Tensor, targets: torch.Tensor):\n",
    "        \"\"\"更新指标状态\"\"\"\n",
    "        pass\n",
    "        \n",
    "    @abstractmethod\n",
    "    def compute(self) -> float:\n",
    "        \"\"\"计算最终指标值\"\"\"\n",
    "        pass\n",
    "        \n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        \"\"\"重置指标状态\"\"\"\n",
    "        pass\n",
    "\n",
    "class DiceScore(BaseMetric):\n",
    "    \"\"\"Dice系数 (F1-score for segmentation)\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.5, smooth: float = 1e-6):\n",
    "        self.smooth = smooth\n",
    "        super().__init__(threshold)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.total_intersection = 0.0\n",
    "        self.total_union = 0.0\n",
    "        \n",
    "    def update(self, predictions: torch.Tensor, targets: torch.Tensor):\n",
    "        \"\"\"\n",
    "        更新Dice系数计算\n",
    "        \n",
    "        Args:\n",
    "            predictions: 模型预测 [B, C, H, W] (logits或概率)\n",
    "            targets: 真实标签 [B, C, H, W] (0-1)\n",
    "        \"\"\"\n",
    "        # 确保预测值在[0,1]范围内\n",
    "        if predictions.max() > 1 or predictions.min() < 0:\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "            \n",
    "        # 二值化预测\n",
    "        predictions = (predictions > self.threshold).float()\n",
    "        targets = targets.float()\n",
    "        \n",
    "        # 展平tensor\n",
    "        predictions_flat = predictions.view(-1)\n",
    "        targets_flat = targets.view(-1)\n",
    "        \n",
    "        # 计算交集和并集\n",
    "        intersection = (predictions_flat * targets_flat).sum()\n",
    "        union = predictions_flat.sum() + targets_flat.sum()\n",
    "        \n",
    "        self.total_intersection += intersection.item()\n",
    "        self.total_union += union.item()\n",
    "        \n",
    "    def compute(self) -> float:\n",
    "        \"\"\"计算Dice系数\"\"\"\n",
    "        if self.total_union == 0:\n",
    "            return 1.0  # 如果没有正样本，返回1（完美分割）\n",
    "        return (2 * self.total_intersection + self.smooth) / (self.total_union + self.smooth)\n",
    "\n",
    "class IoUScore(BaseMetric):\n",
    "    \"\"\"IoU分数 (Intersection over Union / Jaccard Index)\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.5, smooth: float = 1e-6):\n",
    "        self.smooth = smooth\n",
    "        super().__init__(threshold)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.total_intersection = 0.0\n",
    "        self.total_union = 0.0\n",
    "        \n",
    "    def update(self, predictions: torch.Tensor, targets: torch.Tensor):\n",
    "        \"\"\"\n",
    "        更新IoU计算\n",
    "        \n",
    "        Args:\n",
    "            predictions: 模型预测 [B, C, H, W]\n",
    "            targets: 真实标签 [B, C, H, W]\n",
    "        \"\"\"\n",
    "        # 确保预测值在[0,1]范围内\n",
    "        if predictions.max() > 1 or predictions.min() < 0:\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "            \n",
    "        # 二值化预测\n",
    "        predictions = (predictions > self.threshold).float()\n",
    "        targets = targets.float()\n",
    "        \n",
    "        # 展平tensor\n",
    "        predictions_flat = predictions.view(-1)\n",
    "        targets_flat = targets.view(-1)\n",
    "        \n",
    "        # 计算交集和并集\n",
    "        intersection = (predictions_flat * targets_flat).sum()\n",
    "        union = predictions_flat.sum() + targets_flat.sum() - intersection\n",
    "        \n",
    "        self.total_intersection += intersection.item()\n",
    "        self.total_union += union.item()\n",
    "        \n",
    "    def compute(self) -> float:\n",
    "        \"\"\"计算IoU\"\"\"\n",
    "        if self.total_union == 0:\n",
    "            return 1.0\n",
    "        return (self.total_intersection + self.smooth) / (self.total_union + self.smooth)\n",
    "\n",
    "class PixelAccuracy(BaseMetric):\n",
    "    \"\"\"像素准确率\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.5):\n",
    "        super().__init__(threshold)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.correct_pixels = 0\n",
    "        self.total_pixels = 0\n",
    "        \n",
    "    def update(self, predictions: torch.Tensor, targets: torch.Tensor):\n",
    "        \"\"\"\n",
    "        更新像素准确率计算\n",
    "        \n",
    "        Args:\n",
    "            predictions: 模型预测 [B, C, H, W]\n",
    "            targets: 真实标签 [B, C, H, W]\n",
    "        \"\"\"\n",
    "        # 确保预测值在[0,1]范围内\n",
    "        if predictions.max() > 1 or predictions.min() < 0:\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "            \n",
    "        # 二值化预测\n",
    "        predictions = (predictions > self.threshold).float()\n",
    "        targets = targets.float()\n",
    "        \n",
    "        # 计算正确像素\n",
    "        correct = (predictions == targets).sum().item()\n",
    "        total = targets.numel()\n",
    "        \n",
    "        self.correct_pixels += correct\n",
    "        self.total_pixels += total\n",
    "        \n",
    "    def compute(self) -> float:\n",
    "        \"\"\"计算像素准确率\"\"\"\n",
    "        if self.total_pixels == 0:\n",
    "            return 0.0\n",
    "        return self.correct_pixels / self.total_pixels\n",
    "\n",
    "class Precision(BaseMetric):\n",
    "    \"\"\"精确率 (Positive Predictive Value)\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.5, smooth: float = 1e-6):\n",
    "        self.smooth = smooth\n",
    "        super().__init__(threshold)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.true_positives = 0.0\n",
    "        self.false_positives = 0.0\n",
    "        \n",
    "    def update(self, predictions: torch.Tensor, targets: torch.Tensor):\n",
    "        \"\"\"更新精确率计算\"\"\"\n",
    "        # 确保预测值在[0,1]范围内\n",
    "        if predictions.max() > 1 or predictions.min() < 0:\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "            \n",
    "        # 二值化预测\n",
    "        predictions = (predictions > self.threshold).float()\n",
    "        targets = targets.float()\n",
    "        \n",
    "        # 展平tensor\n",
    "        predictions_flat = predictions.view(-1)\n",
    "        targets_flat = targets.view(-1)\n",
    "        \n",
    "        # 计算TP和FP\n",
    "        tp = (predictions_flat * targets_flat).sum()\n",
    "        fp = (predictions_flat * (1 - targets_flat)).sum()\n",
    "        \n",
    "        self.true_positives += tp.item()\n",
    "        self.false_positives += fp.item()\n",
    "        \n",
    "    def compute(self) -> float:\n",
    "        \"\"\"计算精确率\"\"\"\n",
    "        denominator = self.true_positives + self.false_positives\n",
    "        if denominator == 0:\n",
    "            return 0.0\n",
    "        return (self.true_positives + self.smooth) / (denominator + self.smooth)\n",
    "\n",
    "class Recall(BaseMetric):\n",
    "    \"\"\"召回率 (Sensitivity / True Positive Rate)\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.5, smooth: float = 1e-6):\n",
    "        self.smooth = smooth\n",
    "        super().__init__(threshold)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.true_positives = 0.0\n",
    "        self.false_negatives = 0.0\n",
    "        \n",
    "    def update(self, predictions: torch.Tensor, targets: torch.Tensor):\n",
    "        \"\"\"更新召回率计算\"\"\"\n",
    "        # 确保预测值在[0,1]范围内\n",
    "        if predictions.max() > 1 or predictions.min() < 0:\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "            \n",
    "        # 二值化预测\n",
    "        predictions = (predictions > self.threshold).float()\n",
    "        targets = targets.float()\n",
    "        \n",
    "        # 展平tensor\n",
    "        predictions_flat = predictions.view(-1)\n",
    "        targets_flat = targets.view(-1)\n",
    "        \n",
    "        # 计算TP和FN\n",
    "        tp = (predictions_flat * targets_flat).sum()\n",
    "        fn = ((1 - predictions_flat) * targets_flat).sum()\n",
    "        \n",
    "        self.true_positives += tp.item()\n",
    "        self.false_negatives += fn.item()\n",
    "        \n",
    "    def compute(self) -> float:\n",
    "        \"\"\"计算召回率\"\"\"\n",
    "        denominator = self.true_positives + self.false_negatives\n",
    "        if denominator == 0:\n",
    "            return 0.0\n",
    "        return (self.true_positives + self.smooth) / (denominator + self.smooth)\n",
    "\n",
    "class Specificity(BaseMetric):\n",
    "    \"\"\"特异性 (True Negative Rate)\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.5, smooth: float = 1e-6):\n",
    "        self.smooth = smooth\n",
    "        super().__init__(threshold)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.true_negatives = 0.0\n",
    "        self.false_positives = 0.0\n",
    "        \n",
    "    def update(self, predictions: torch.Tensor, targets: torch.Tensor):\n",
    "        \"\"\"更新特异性计算\"\"\"\n",
    "        # 确保预测值在[0,1]范围内\n",
    "        if predictions.max() > 1 or predictions.min() < 0:\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "            \n",
    "        # 二值化预测\n",
    "        predictions = (predictions > self.threshold).float()\n",
    "        targets = targets.float()\n",
    "        \n",
    "        # 展平tensor\n",
    "        predictions_flat = predictions.view(-1)\n",
    "        targets_flat = targets.view(-1)\n",
    "        \n",
    "        # 计算TN和FP\n",
    "        tn = ((1 - predictions_flat) * (1 - targets_flat)).sum()\n",
    "        fp = (predictions_flat * (1 - targets_flat)).sum()\n",
    "        \n",
    "        self.true_negatives += tn.item()\n",
    "        self.false_positives += fp.item()\n",
    "        \n",
    "    def compute(self) -> float:\n",
    "        \"\"\"计算特异性\"\"\"\n",
    "        denominator = self.true_negatives + self.false_positives\n",
    "        if denominator == 0:\n",
    "            return 0.0\n",
    "        return (self.true_negatives + self.smooth) / (denominator + self.smooth)\n",
    "\n",
    "class HausdorffDistance(BaseMetric):\n",
    "    \"\"\"Hausdorff距离 (用于评估边界质量)\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.5, percentile: float = 95):\n",
    "        self.percentile = percentile\n",
    "        super().__init__(threshold)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.distances = []\n",
    "        \n",
    "    def update(self, predictions: torch.Tensor, targets: torch.Tensor):\n",
    "        \"\"\"更新Hausdorff距离计算\"\"\"\n",
    "        # 确保预测值在[0,1]范围内\n",
    "        if predictions.max() > 1 or predictions.min() < 0:\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "            \n",
    "        # 二值化预测\n",
    "        predictions = (predictions > self.threshold).float()\n",
    "        targets = targets.float()\n",
    "        \n",
    "        # 对batch中的每个样本计算距离\n",
    "        batch_size = predictions.shape[0]\n",
    "        for i in range(batch_size):\n",
    "            pred_i = predictions[i, 0].cpu().numpy()\n",
    "            target_i = targets[i, 0].cpu().numpy()\n",
    "            \n",
    "            try:\n",
    "                # 计算边界点\n",
    "                pred_coords = np.argwhere(pred_i > 0)\n",
    "                target_coords = np.argwhere(target_i > 0)\n",
    "                \n",
    "                if len(pred_coords) == 0 or len(target_coords) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                # 计算距离矩阵\n",
    "                distances_pt = np.sqrt(((pred_coords[:, None, :] - target_coords[None, :, :]) ** 2).sum(axis=2))\n",
    "                distances_tp = np.sqrt(((target_coords[:, None, :] - pred_coords[None, :, :]) ** 2).sum(axis=2))\n",
    "                \n",
    "                # 计算Hausdorff距离\n",
    "                hausdorff_pt = np.percentile(np.min(distances_pt, axis=1), self.percentile)\n",
    "                hausdorff_tp = np.percentile(np.min(distances_tp, axis=1), self.percentile)\n",
    "                hausdorff_dist = max(hausdorff_pt, hausdorff_tp)\n",
    "                \n",
    "                self.distances.append(hausdorff_dist)\n",
    "                \n",
    "            except Exception:\n",
    "                continue\n",
    "                \n",
    "    def compute(self) -> float:\n",
    "        \"\"\"计算平均Hausdorff距离\"\"\"\n",
    "        if not self.distances:\n",
    "            return float('inf')\n",
    "        return np.mean(self.distances)\n",
    "\n",
    "class SurfaceDice(BaseMetric):\n",
    "    \"\"\"表面Dice系数 (Surface DSC)\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.5, tolerance: float = 1.0):\n",
    "        self.tolerance = tolerance\n",
    "        super().__init__(threshold)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.surface_dices = []\n",
    "        \n",
    "    def update(self, predictions: torch.Tensor, targets: torch.Tensor):\n",
    "        \"\"\"更新表面Dice计算\"\"\"\n",
    "        # 确保预测值在[0,1]范围内\n",
    "        if predictions.max() > 1 or predictions.min() < 0:\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "            \n",
    "        # 二值化预测\n",
    "        predictions = (predictions > self.threshold).float()\n",
    "        targets = targets.float()\n",
    "        \n",
    "        batch_size = predictions.shape[0]\n",
    "        for i in range(batch_size):\n",
    "            pred_i = predictions[i, 0].cpu().numpy()\n",
    "            target_i = targets[i, 0].cpu().numpy()\n",
    "            \n",
    "            try:\n",
    "                # 计算边界\n",
    "                pred_boundary = self._get_boundary(pred_i)\n",
    "                target_boundary = self._get_boundary(target_i)\n",
    "                \n",
    "                if np.sum(pred_boundary) == 0 or np.sum(target_boundary) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                # 计算表面距离\n",
    "                pred_coords = np.argwhere(pred_boundary > 0)\n",
    "                target_coords = np.argwhere(target_boundary > 0)\n",
    "                \n",
    "                if len(pred_coords) == 0 or len(target_coords) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                # 计算距离矩阵\n",
    "                distances_pt = np.sqrt(((pred_coords[:, None, :] - target_coords[None, :, :]) ** 2).sum(axis=2))\n",
    "                distances_tp = np.sqrt(((target_coords[:, None, :] - pred_coords[None, :, :]) ** 2).sum(axis=2))\n",
    "                \n",
    "                # 在容忍度内的表面点\n",
    "                surface_pred = np.sum(np.min(distances_pt, axis=1) <= self.tolerance)\n",
    "                surface_target = np.sum(np.min(distances_tp, axis=1) <= self.tolerance)\n",
    "                \n",
    "                # 计算表面Dice\n",
    "                surface_dice = (surface_pred + surface_target) / (len(pred_coords) + len(target_coords))\n",
    "                self.surface_dices.append(surface_dice)\n",
    "                \n",
    "            except Exception:\n",
    "                continue\n",
    "                \n",
    "    def _get_boundary(self, mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"提取边界\"\"\"\n",
    "        from scipy import ndimage\n",
    "        \n",
    "        # 使用形态学操作提取边界\n",
    "        eroded = ndimage.binary_erosion(mask)\n",
    "        boundary = mask ^ eroded\n",
    "        return boundary.astype(np.uint8)\n",
    "        \n",
    "    def compute(self) -> float:\n",
    "        \"\"\"计算平均表面Dice\"\"\"\n",
    "        if not self.surface_dices:\n",
    "            return 0.0\n",
    "        return np.mean(self.surface_dices)\n",
    "\n",
    "class MetricsCalculator:\n",
    "    \"\"\"指标计算器，集成多种评估指标\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 threshold: float = 0.5,\n",
    "                 include_boundary_metrics: bool = False,\n",
    "                 device: Optional[torch.device] = None):\n",
    "        \"\"\"\n",
    "        初始化指标计算器\n",
    "        \n",
    "        Args:\n",
    "            threshold: 二值化阈值\n",
    "            include_boundary_metrics: 是否包含边界相关指标\n",
    "            device: 计算设备\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.include_boundary_metrics = include_boundary_metrics\n",
    "        self.device = device or torch.device('cpu')\n",
    "        \n",
    "        # 基础指标\n",
    "        self.metrics = {\n",
    "            'dice': DiceScore(threshold),\n",
    "            'iou': IoUScore(threshold),\n",
    "            'pixel_accuracy': PixelAccuracy(threshold),\n",
    "            'precision': Precision(threshold),\n",
    "            'recall': Recall(threshold),\n",
    "            'specificity': Specificity(threshold)\n",
    "        }\n",
    "        \n",
    "        # 边界指标（计算量大，可选）\n",
    "        if include_boundary_metrics:\n",
    "            self.metrics.update({\n",
    "                'hausdorff_95': HausdorffDistance(threshold, percentile=95),\n",
    "                'surface_dice': SurfaceDice(threshold, tolerance=1.0)\n",
    "            })\n",
    "            \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"重置所有指标\"\"\"\n",
    "        for metric in self.metrics.values():\n",
    "            metric.reset()\n",
    "            \n",
    "    def update(self, predictions: torch.Tensor, targets: torch.Tensor):\n",
    "        \"\"\"\n",
    "        更新所有指标\n",
    "        \n",
    "        Args:\n",
    "            predictions: 模型预测 [B, C, H, W]\n",
    "            targets: 真实标签 [B, C, H, W]\n",
    "        \"\"\"\n",
    "        # 移动到指定设备\n",
    "        predictions = predictions.to(self.device)\n",
    "        targets = targets.to(self.device)\n",
    "        \n",
    "        # 更新所有指标\n",
    "        for metric in self.metrics.values():\n",
    "            metric.update(predictions, targets)\n",
    "            \n",
    "    def compute(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        计算所有指标的最终值\n",
    "        \n",
    "        Returns:\n",
    "            指标字典\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        for name, metric in self.metrics.items():\n",
    "            try:\n",
    "                results[name] = metric.compute()\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"计算指标 {name} 时出错: {e}\")\n",
    "                results[name] = 0.0\n",
    "                \n",
    "        return results\n",
    "        \n",
    "    def compute_and_reset(self) -> Dict[str, float]:\n",
    "        \"\"\"计算指标并重置\"\"\"\n",
    "        results = self.compute()\n",
    "        self.reset()\n",
    "        return results\n",
    "\n",
    "# ==============================================================================\n",
    "# TorchMetrics集成 (如果可用)\n",
    "\n",
    "if TORCHMETRICS_AVAILABLE:\n",
    "    class TorchMetricsCalculator:\n",
    "        \"\"\"使用TorchMetrics的指标计算器\"\"\"\n",
    "        \n",
    "        def __init__(self, \n",
    "                     threshold: float = 0.5,\n",
    "                     num_classes: int = 1,\n",
    "                     device: Optional[torch.device] = None):\n",
    "            \"\"\"\n",
    "            初始化TorchMetrics计算器\n",
    "            \n",
    "            Args:\n",
    "                threshold: 二值化阈值\n",
    "                num_classes: 类别数（二分类为1）\n",
    "                device: 计算设备\n",
    "            \"\"\"\n",
    "            self.threshold = threshold\n",
    "            self.device = device or torch.device('cpu')\n",
    "            \n",
    "            # 创建指标集合\n",
    "            self.metrics = MetricCollection({\n",
    "                'dice': torchmetrics.Dice(\n",
    "                    num_classes=num_classes,\n",
    "                    threshold=threshold,\n",
    "                    average='macro'\n",
    "                ),\n",
    "                'iou': torchmetrics.JaccardIndex(\n",
    "                    num_classes=num_classes + 1,  # 包含背景类\n",
    "                    threshold=threshold,\n",
    "                    average='macro'\n",
    "                ),\n",
    "                'precision': torchmetrics.Precision(\n",
    "                    num_classes=num_classes + 1,\n",
    "                    threshold=threshold,\n",
    "                    average='macro'\n",
    "                ),\n",
    "                'recall': torchmetrics.Recall(\n",
    "                    num_classes=num_classes + 1,\n",
    "                    threshold=threshold,\n",
    "                    average='macro'\n",
    "                ),\n",
    "                'specificity': torchmetrics.Specificity(\n",
    "                    num_classes=num_classes + 1,\n",
    "                    threshold=threshold,\n",
    "                    average='macro'\n",
    "                ),\n",
    "                'accuracy': torchmetrics.Accuracy(\n",
    "                    num_classes=num_classes + 1,\n",
    "                    threshold=threshold,\n",
    "                    average='macro'\n",
    "                )\n",
    "            }).to(self.device)\n",
    "            \n",
    "        def update(self, predictions: torch.Tensor, targets: torch.Tensor):\n",
    "            \"\"\"更新指标\"\"\"\n",
    "            predictions = predictions.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            \n",
    "            # TorchMetrics期望整数标签\n",
    "            targets_int = targets.long()\n",
    "            \n",
    "            self.metrics.update(predictions, targets_int)\n",
    "            \n",
    "        def compute(self) -> Dict[str, float]:\n",
    "            \"\"\"计算指标\"\"\"\n",
    "            results = self.metrics.compute()\n",
    "            # 转换为Python float\n",
    "            return {k: v.item() if isinstance(v, torch.Tensor) else v \n",
    "                   for k, v in results.items()}\n",
    "            \n",
    "        def reset(self):\n",
    "            \"\"\"重置指标\"\"\"\n",
    "            self.metrics.reset()\n",
    "            \n",
    "        def compute_and_reset(self) -> Dict[str, float]:\n",
    "            \"\"\"计算并重置\"\"\"\n",
    "            results = self.compute()\n",
    "            self.reset()\n",
    "            return results\n",
    "\n",
    "# ==============================================================================\n",
    "# 便捷函数\n",
    "\n",
    "def create_metrics_calculator(config: Dict[str, Any], \n",
    "                            use_torchmetrics: bool = None) -> Union[MetricsCalculator, 'TorchMetricsCalculator']:\n",
    "    \"\"\"\n",
    "    创建指标计算器\n",
    "    \n",
    "    Args:\n",
    "        config: 配置字典\n",
    "        use_torchmetrics: 是否使用TorchMetrics，None时自动检测\n",
    "        \n",
    "    Returns:\n",
    "        指标计算器实例\n",
    "    \"\"\"\n",
    "    if use_torchmetrics is None:\n",
    "        use_torchmetrics = TORCHMETRICS_AVAILABLE\n",
    "        \n",
    "    threshold = config.get('evaluation', {}).get('threshold', 0.5)\n",
    "    include_boundary = config.get('evaluation', {}).get('include_boundary_metrics', False)\n",
    "    \n",
    "    if use_torchmetrics and TORCHMETRICS_AVAILABLE:\n",
    "        return TorchMetricsCalculator(threshold=threshold)\n",
    "    else:\n",
    "        return MetricsCalculator(\n",
    "            threshold=threshold,\n",
    "            include_boundary_metrics=include_boundary\n",
    "        )\n",
    "\n",
    "def evaluate_predictions(predictions: torch.Tensor,\n",
    "                        targets: torch.Tensor,\n",
    "                        threshold: float = 0.5,\n",
    "                        include_boundary: bool = False) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    快速评估预测结果\n",
    "    \n",
    "    Args:\n",
    "        predictions: 模型预测\n",
    "        targets: 真实标签\n",
    "        threshold: 二值化阈值\n",
    "        include_boundary: 是否包含边界指标\n",
    "        \n",
    "    Returns:\n",
    "        评估结果字典\n",
    "    \"\"\"\n",
    "    calculator = MetricsCalculator(\n",
    "        threshold=threshold,\n",
    "        include_boundary_metrics=include_boundary\n",
    "    )\n",
    "    \n",
    "    calculator.update(predictions, targets)\n",
    "    return calculator.compute()\n",
    "\n",
    "# ==============================================================================\n",
    "# 测试脚本\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 设置日志\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # 创建测试数据\n",
    "    batch_size, channels, height, width = 2, 1, 128, 128\n",
    "    \n",
    "    # 创建具有明确分割模式的测试数据\n",
    "    predictions = torch.zeros(batch_size, channels, height, width)\n",
    "    targets = torch.zeros(batch_size, channels, height, width)\n",
    "    \n",
    "    # 添加一些分割区域\n",
    "    predictions[0, 0, 30:80, 30:80] = 0.8  # 高置信度预测\n",
    "    predictions[1, 0, 40:90, 40:90] = 0.6  # 中等置信度预测\n",
    "    \n",
    "    targets[0, 0, 32:78, 32:78] = 1.0      # 略小的真实区域\n",
    "    targets[1, 0, 35:85, 35:85] = 1.0      # 略有偏移的真实区域\n",
    "    \n",
    "    logger.info(\"🔄 测试评估指标...\")\n",
    "    \n",
    "    # 测试自定义指标\n",
    "    logger.info(\"\\n📊 测试自定义指标计算器:\")\n",
    "    try:\n",
    "        calculator = MetricsCalculator(threshold=0.5, include_boundary_metrics=False)\n",
    "        calculator.update(predictions, targets)\n",
    "        results = calculator.compute()\n",
    "        \n",
    "        for metric_name, value in results.items():\n",
    "            logger.info(f\"  {metric_name}: {value:.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ 自定义指标测试失败: {e}\")\n",
    "        \n",
    "    # 测试TorchMetrics（如果可用）\n",
    "    if TORCHMETRICS_AVAILABLE:\n",
    "        logger.info(\"\\n📊 测试TorchMetrics计算器:\")\n",
    "        try:\n",
    "            torch_calculator = TorchMetricsCalculator(threshold=0.5)\n",
    "            torch_calculator.update(predictions, targets)\n",
    "            torch_results = torch_calculator.compute()\n",
    "            \n",
    "            for metric_name, value in torch_results.items():\n",
    "                logger.info(f\"  {metric_name}: {value:.4f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ TorchMetrics测试失败: {e}\")\n",
    "    else:\n",
    "        logger.info(\"\\n⚠️  TorchMetrics未安装，跳过相关测试\")\n",
    "        \n",
    "    # 测试快速评估函数\n",
    "    logger.info(\"\\n⚡ 测试快速评估函数:\")\n",
    "    try:\n",
    "        quick_results = evaluate_predictions(predictions, targets, threshold=0.5)\n",
    "        \n",
    "        for metric_name, value in quick_results.items():\n",
    "            logger.info(f\"  {metric_name}: {value:.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ 快速评估测试失败: {e}\")\n",
    "        \n",
    "    # 测试边界指标\n",
    "    logger.info(\"\\n🎯 测试边界指标:\")\n",
    "    try:\n",
    "        boundary_calculator = MetricsCalculator(\n",
    "            threshold=0.5, \n",
    "            include_boundary_metrics=True\n",
    "        )\n",
    "        boundary_calculator.update(predictions, targets)\n",
    "        boundary_results = boundary_calculator.compute()\n",
    "        \n",
    "        for metric_name, value in boundary_results.items():\n",
    "            if 'hausdorff' in metric_name or 'surface' in metric_name:\n",
    "                logger.info(f\"  {metric_name}: {value:.4f}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ 边界指标测试失败: {e}\")\n",
    "        \n",
    "    logger.info(\"\\n✅ 评估指标模块测试完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce19b36-fc59-43ac-a60f-df6daf99ce97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c10c5-b09c-4e4e-9d3a-65bbee873a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf14f25-a68b-45e3-aea7-d674c8f0a6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90beef7f-9d06-466d-85d7-34126a6985b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine/train_loop.py\n",
    "# 通用训练循环\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Dict, Any, Optional, Callable, Tuple, List\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "class TrainingEngine:\n",
    "    \"\"\"通用训练引擎\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model: nn.Module,\n",
    "                 criterion: nn.Module,\n",
    "                 optimizer: optim.Optimizer,\n",
    "                 device: torch.device,\n",
    "                 config: Dict[str, Any],\n",
    "                 scheduler: Optional[optim.lr_scheduler._LRScheduler] = None,\n",
    "                 metrics_calculator: Optional[Callable] = None,\n",
    "                 early_stopping: Optional[Callable] = None,\n",
    "                 checkpoint_manager: Optional[Callable] = None):\n",
    "        \"\"\"\n",
    "        初始化训练引擎\n",
    "        \n",
    "        Args:\n",
    "            model: 要训练的模型\n",
    "            criterion: 损失函数\n",
    "            optimizer: 优化器\n",
    "            device: 训练设备\n",
    "            config: 配置字典\n",
    "            scheduler: 学习率调度器\n",
    "            metrics_calculator: 指标计算器\n",
    "            early_stopping: 早停机制\n",
    "            checkpoint_manager: 检查点管理器\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        self.scheduler = scheduler\n",
    "        self.metrics_calculator = metrics_calculator\n",
    "        self.early_stopping = early_stopping\n",
    "        self.checkpoint_manager = checkpoint_manager\n",
    "        \n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # 训练状态\n",
    "        self.current_epoch = 0\n",
    "        self.global_step = 0\n",
    "        self.best_metric = float('inf')  # 假设越小越好\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_metrics': [],\n",
    "            'val_metrics': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "        \n",
    "    def train_epoch(self, \n",
    "                   train_loader: DataLoader,\n",
    "                   epoch: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        训练一个epoch\n",
    "        \n",
    "        Args:\n",
    "            train_loader: 训练数据加载器\n",
    "            epoch: 当前epoch\n",
    "            \n",
    "        Returns:\n",
    "            训练指标字典\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        num_batches = len(train_loader)\n",
    "        \n",
    "        # 重置指标计算器\n",
    "        if self.metrics_calculator:\n",
    "            self.metrics_calculator.reset()\n",
    "            \n",
    "        # 进度条\n",
    "        pbar = tqdm(train_loader, desc=f'Training Epoch {epoch}', leave=False)\n",
    "        \n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            # 数据移动到设备\n",
    "            images = batch['image'].to(self.device, non_blocking=True)\n",
    "            masks = batch['mask'].to(self.device, non_blocking=True)\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, masks)\n",
    "            \n",
    "            # 反向传播\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # 梯度裁剪（可选）\n",
    "            if self.config.get('training', {}).get('gradient_clip_val'):\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(),\n",
    "                    self.config['training']['gradient_clip_val']\n",
    "                )\n",
    "                \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # 更新统计\n",
    "            running_loss += loss.item()\n",
    "            self.global_step += 1\n",
    "            \n",
    "            # 更新指标\n",
    "            if self.metrics_calculator:\n",
    "                self.metrics_calculator.update(outputs, masks)\n",
    "                \n",
    "            # 更新进度条\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'avg_loss': f'{running_loss / (batch_idx + 1):.4f}'\n",
    "            })\n",
    "            \n",
    "        # 计算epoch级别的指标\n",
    "        epoch_loss = running_loss / num_batches\n",
    "        epoch_metrics = {}\n",
    "        \n",
    "        if self.metrics_calculator:\n",
    "            epoch_metrics = self.metrics_calculator.compute_and_reset()\n",
    "            \n",
    "        # 记录学习率\n",
    "        current_lr = self.optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        results = {\n",
    "            'loss': epoch_loss,\n",
    "            'lr': current_lr,\n",
    "            **epoch_metrics\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    def validate_epoch(self, \n",
    "                      val_loader: DataLoader,\n",
    "                      epoch: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        验证一个epoch\n",
    "        \n",
    "        Args:\n",
    "            val_loader: 验证数据加载器\n",
    "            epoch: 当前epoch\n",
    "            \n",
    "        Returns:\n",
    "            验证指标字典\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        num_batches = len(val_loader)\n",
    "        \n",
    "        # 重置指标计算器\n",
    "        if self.metrics_calculator:\n",
    "            self.metrics_calculator.reset()\n",
    "            \n",
    "        # 进度条\n",
    "        pbar = tqdm(val_loader, desc=f'Validation Epoch {epoch}', leave=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(pbar):\n",
    "                # 数据移动到设备\n",
    "                images = batch['image'].to(self.device, non_blocking=True)\n",
    "                masks = batch['mask'].to(self.device, non_blocking=True)\n",
    "                \n",
    "                # 前向传播\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, masks)\n",
    "                \n",
    "                # 更新统计\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                # 更新指标\n",
    "                if self.metrics_calculator:\n",
    "                    self.metrics_calculator.update(outputs, masks)\n",
    "                    \n",
    "                # 更新进度条\n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'avg_loss': f'{running_loss / (batch_idx + 1):.4f}'\n",
    "                })\n",
    "                \n",
    "        # 计算epoch级别的指标\n",
    "        epoch_loss = running_loss / num_batches\n",
    "        epoch_metrics = {}\n",
    "        \n",
    "        if self.metrics_calculator:\n",
    "            epoch_metrics = self.metrics_calculator.compute_and_reset()\n",
    "            \n",
    "        results = {\n",
    "            'loss': epoch_loss,\n",
    "            **epoch_metrics\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    def fit(self, \n",
    "           train_loader: DataLoader,\n",
    "           val_loader: DataLoader,\n",
    "           num_epochs: int,\n",
    "           resume_from_checkpoint: Optional[str] = None) -> Dict[str, List]:\n",
    "        \"\"\"\n",
    "        完整训练流程\n",
    "        \n",
    "        Args:\n",
    "            train_loader: 训练数据加载器\n",
    "            val_loader: 验证数据加载器\n",
    "            num_epochs: 训练轮数\n",
    "            resume_from_checkpoint: 恢复训练的检查点路径\n",
    "            \n",
    "        Returns:\n",
    "            训练历史字典\n",
    "        \"\"\"\n",
    "        # 恢复训练状态\n",
    "        start_epoch = 0\n",
    "        if resume_from_checkpoint and Path(resume_from_checkpoint).exists():\n",
    "            start_epoch = self._load_checkpoint(resume_from_checkpoint)\n",
    "            self.logger.info(f\"从检查点恢复训练: epoch {start_epoch}\")\n",
    "            \n",
    "        self.logger.info(f\"开始训练: {start_epoch} -> {num_epochs} epochs\")\n",
    "        \n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            self.current_epoch = epoch\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            # 训练阶段\n",
    "            train_results = self.train_epoch(train_loader, epoch)\n",
    "            \n",
    "            # 验证阶段\n",
    "            val_results = self.validate_epoch(val_loader, epoch)\n",
    "            \n",
    "            # 学习率调度\n",
    "            if self.scheduler:\n",
    "                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    self.scheduler.step(val_results['loss'])\n",
    "                else:\n",
    "                    self.scheduler.step()\n",
    "                    \n",
    "            # 记录历史\n",
    "            self.training_history['train_loss'].append(train_results['loss'])\n",
    "            self.training_history['val_loss'].append(val_results['loss'])\n",
    "            self.training_history['learning_rates'].append(train_results['lr'])\n",
    "            \n",
    "            # 记录指标\n",
    "            train_metrics = {k: v for k, v in train_results.items() if k not in ['loss', 'lr']}\n",
    "            val_metrics = {k: v for k, v in val_results.items() if k != 'loss'}\n",
    "            \n",
    "            self.training_history['train_metrics'].append(train_metrics)\n",
    "            self.training_history['val_metrics'].append(val_metrics)\n",
    "            \n",
    "            # 计算epoch时间\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            \n",
    "            # 打印epoch结果\n",
    "            self._log_epoch_results(epoch, train_results, val_results, epoch_time)\n",
    "            \n",
    "            # 保存检查点\n",
    "            if self.checkpoint_manager:\n",
    "                is_best = val_results['loss'] < self.best_metric\n",
    "                if is_best:\n",
    "                    self.best_metric = val_results['loss']\n",
    "                    \n",
    "                self.checkpoint_manager.save_checkpoint(\n",
    "                    epoch=epoch,\n",
    "                    model=self.model,\n",
    "                    optimizer=self.optimizer,\n",
    "                    scheduler=self.scheduler,\n",
    "                    metrics=val_results,\n",
    "                    is_best=is_best\n",
    "                )\n",
    "                \n",
    "            # 早停检查\n",
    "            if self.early_stopping:\n",
    "                if self.early_stopping(val_results['loss'], self.model):\n",
    "                    self.logger.info(f\"早停触发在epoch {epoch}\")\n",
    "                    break\n",
    "                    \n",
    "        self.logger.info(\"训练完成!\")\n",
    "        return self.training_history\n",
    "        \n",
    "    def _log_epoch_results(self, \n",
    "                          epoch: int,\n",
    "                          train_results: Dict[str, float],\n",
    "                          val_results: Dict[str, float],\n",
    "                          epoch_time: float):\n",
    "        \"\"\"记录epoch结果\"\"\"\n",
    "        log_str = f\"Epoch [{epoch:3d}] \"\n",
    "        log_str += f\"Time: {epoch_time:.1f}s \"\n",
    "        log_str += f\"Train Loss: {train_results['loss']:.4f} \"\n",
    "        log_str += f\"Val Loss: {val_results['loss']:.4f} \"\n",
    "        log_str += f\"LR: {train_results['lr']:.2e} \"\n",
    "        \n",
    "        # 添加主要指标\n",
    "        for metric_name in ['dice', 'iou']:\n",
    "            if metric_name in val_results:\n",
    "                log_str += f\"Val {metric_name.upper()}: {val_results[metric_name]:.4f} \"\n",
    "                \n",
    "        self.logger.info(log_str)\n",
    "        \n",
    "    def _load_checkpoint(self, checkpoint_path: str) -> int:\n",
    "        \"\"\"加载检查点\"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        \n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d4268e-b867-4801-a65b-c991d51a7518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7046ff4d-cdea-4b5d-b639-3970de0d33d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eef642-e9e4-4fa8-81e3-73a32a80a00c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab12d1a-c02b-4502-bb4a-3729eb4e8392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scripts/train.py\n",
    "# 训练脚本\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# 添加项目根目录到路径\n",
    "sys.path.append(str(Path(__file__).parent.parent))\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from configs import ConfigLoader\n",
    "from datasets import ChestXrayDataset, DataLoaderFactory\n",
    "from transforms import get_transforms\n",
    "from losses import get_loss_fn\n",
    "from metrics import create_metrics_calculator\n",
    "from engine import TrainingEngine, EarlyStopping, CheckpointManager\n",
    "from utils import set_seed, setup_logging, get_device, ModelSummary\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"解析命令行参数\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"训练肺部分割模型\")\n",
    "    \n",
    "    parser.add_argument('--config', type=str, default='default',\n",
    "                       help='配置文件名称（不含.yaml后缀）')\n",
    "    parser.add_argument('--model', type=str, default='unet',\n",
    "                       help='模型类型')\n",
    "    parser.add_argument('--resume', type=str, default=None,\n",
    "                       help='恢复训练的检查点路径')\n",
    "    parser.add_argument('--debug', action='store_true',\n",
    "                       help='调试模式')\n",
    "    parser.add_argument('--gpu', type=int, default=None,\n",
    "                       help='指定GPU ID')\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "def create_model(model_config: dict, device: torch.device):\n",
    "    \"\"\"创建模型\"\"\"\n",
    "    model_name = model_config['name'].lower()\n",
    "    \n",
    "    if model_name == 'unet':\n",
    "        from models.unet import UNet\n",
    "        model = UNet(\n",
    "            in_channels=model_config['in_channels'],\n",
    "            out_channels=model_config['out_channels'],\n",
    "            features=model_config.get('features', [64, 128, 256, 512]),\n",
    "            dropout_rate=model_config.get('dropout_rate', 0.3),\n",
    "            bilinear=model_config.get('bilinear', False)\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"未知的模型类型: {model_name}\")\n",
    "        \n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    args = parse_args()\n",
    "    \n",
    "    # 设置日志\n",
    "    setup_logging(debug=args.debug)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    try:\n",
    "        # 加载配置\n",
    "        config_loader = ConfigLoader()\n",
    "        config = config_loader.load_config(args.config)\n",
    "        \n",
    "        # 覆盖模型配置\n",
    "        config['model']['name'] = args.model\n",
    "        \n",
    "        # 设置随机种子\n",
    "        set_seed(config['experiment']['seed'])\n",
    "        \n",
    "        # 获取设备\n",
    "        device = get_device(args.gpu)\n",
    "        logger.info(f\"使用设备: {device}\")\n",
    "        \n",
    "        # 创建数据变换\n",
    "        transforms = {\n",
    "            'train': get_transforms('train', config),\n",
    "            'val': get_transforms('val', config),\n",
    "            'test': get_transforms('test', config)\n",
    "        }\n",
    "        \n",
    "        # 创建数据加载器\n",
    "        factory = DataLoaderFactory(config)\n",
    "        dataloaders = factory.create_all_dataloaders(transforms)\n",
    "        \n",
    "        logger.info(f\"数据加载完成:\")\n",
    "        for split, loader in dataloaders.items():\n",
    "            logger.info(f\"  {split}: {len(loader.dataset)} 样本, {len(loader)} 批次\")\n",
    "            \n",
    "        # 创建模型\n",
    "        model = create_model(config['model'], device)\n",
    "        \n",
    "        # 打印模型摘要\n",
    "        summary = ModelSummary(model)\n",
    "        summary.print_summary(input_size=(1, config['data']['image_size'], config['data']['image_size']))\n",
    "        \n",
    "        # 创建损失函数\n",
    "        criterion = get_loss_fn(config)\n",
    "        logger.info(f\"损失函数: {type(criterion).__name__}\")\n",
    "        \n",
    "        # 创建优化器\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=config['training']['learning_rate'],\n",
    "            weight_decay=config['training']['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # 创建学习率调度器\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            patience=config['training']['scheduler_patience'],\n",
    "            factor=config['training']['scheduler_factor'],\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # 创建指标计算器\n",
    "        metrics_calculator = create_metrics_calculator(config)\n",
    "        \n",
    "        # 创建早停机制\n",
    "        early_stopping = EarlyStopping(\n",
    "            patience=config['training']['early_stopping']['patience'],\n",
    "            min_delta=config['training']['early_stopping']['min_delta'],\n",
    "            restore_best_weights=config['training']['early_stopping']['restore_best_weights']\n",
    "        )\n",
    "        \n",
    "        # 创建检查点管理器\n",
    "        checkpoint_manager = CheckpointManager(\n",
    "            checkpoint_dir=config['logging']['checkpoint_dir'],\n",
    "            save_best_only=config['logging']['save_best_only'],\n",
    "            save_every_n_epochs=config['logging']['save_every_n_epochs']\n",
    "        )\n",
    "        \n",
    "        # 创建训练引擎\n",
    "        trainer = TrainingEngine(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            config=config,\n",
    "            scheduler=scheduler,\n",
    "            metrics_calculator=metrics_calculator,\n",
    "            early_stopping=early_stopping,\n",
    "            checkpoint_manager=checkpoint_manager\n",
    "        )\n",
    "        \n",
    "        # 开始训练\n",
    "        history = trainer.fit(\n",
    "            train_loader=dataloaders['train'],\n",
    "            val_loader=dataloaders['val'],\n",
    "            num_epochs=config['training']['num_epochs'],\n",
    "            resume_from_checkpoint=args.resume\n",
    "        )\n",
    "        \n",
    "        logger.info(\"✅ 训练完成!\")\n",
    "        \n",
    "        # 保存训练历史\n",
    "        import json\n",
    "        history_path = Path(config['logging']['output_dir']) / 'training_history.json'\n",
    "        with open(history_path, 'w') as f:\n",
    "            json.dump(history, f, indent=2)\n",
    "        logger.info(f\"训练历史保存至: {history_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ 训练失败: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# ==============================================================================\n",
    "# scripts/evaluate.py\n",
    "# 评估脚本\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "sys.path.append(str(Path(__file__).parent.parent))\n",
    "\n",
    "import torch\n",
    "from configs import ConfigLoader\n",
    "from datasets import ChestXrayDataset, DataLoaderFactory\n",
    "from transforms import get_transforms\n",
    "from metrics import create_metrics_calculator\n",
    "from engine import InferenceEngine\n",
    "from utils import setup_logging, get_device\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"解析命令行参数\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"评估肺部分割模型\")\n",
    "    \n",
    "    parser.add_argument('--config', type=str, default='default',\n",
    "                       help='配置文件名称')\n",
    "    parser.add_argument('--checkpoint', type=str, required=True,\n",
    "                       help='模型检查点路径')\n",
    "    parser.add_argument('--model', type=str, default='unet',\n",
    "                       help='模型类型')\n",
    "    parser.add_argument('--split', type=str, default='test',\n",
    "                       choices=['train', 'val', 'test'],\n",
    "                       help='评估的数据分割')\n",
    "    parser.add_argument('--output', type=str, default=None,\n",
    "                       help='结果输出路径')\n",
    "    parser.add_argument('--gpu', type=int, default=None,\n",
    "                       help='指定GPU ID')\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    args = parse_args()\n",
    "    \n",
    "    # 设置日志\n",
    "    setup_logging()\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    try:\n",
    "        # 加载配置\n",
    "        config_loader = ConfigLoader()\n",
    "        config = config_loader.load_config(args.config)\n",
    "        config['model']['name'] = args.model\n",
    "        \n",
    "        # 获取设备\n",
    "        device = get_device(args.gpu)\n",
    "        logger.info(f\"使用设备: {device}\")\n",
    "        \n",
    "        # 创建模型\n",
    "        from scripts.train import create_model\n",
    "        model = create_model(config['model'], device)\n",
    "        \n",
    "        # 创建推理引擎\n",
    "        inference_engine = InferenceEngine(model, device, config)\n",
    "        inference_engine.load_checkpoint(args.checkpoint)\n",
    "        \n",
    "        # 创建数据集和数据加载器\n",
    "        transform = get_transforms(args.split, config)\n",
    "        dataset = ChestXrayDataset(config, split=args.split, transform=transform)\n",
    "        \n",
    "        factory = DataLoaderFactory(config)\n",
    "        dataloader = factory.create_dataloader(dataset, shuffle=False)\n",
    "        \n",
    "        logger.info(f\"评估数据集: {len(dataset)} 样本\")\n",
    "        \n",
    "        # 创建指标计算器\n",
    "        metrics_calculator = create_metrics_calculator(config)\n",
    "        \n",
    "        # 进行推理和评估\n",
    "        logger.info(\"开始评估...\")\n",
    "        all_predictions = []\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            # 推理\n",
    "            results = inference_engine.predict_batch(batch)\n",
    "            all_predictions.append(results)\n",
    "            \n",
    "            # 更新指标\n",
    "            metrics_calculator.update(\n",
    "                results['logits'].to(device),\n",
    "                batch['mask'].to(device)\n",
    "            )\n",
    "            \n",
    "        # 计算最终指标\n",
    "        final_metrics = metrics_calculator.compute()\n",
    "        \n",
    "        # 打印结果\n",
    "        logger.info(\"📊 评估结果:\")\n",
    "        for metric_name, value in final_metrics.items():\n",
    "            logger.info(f\"  {metric_name.upper()}: {value:.4f}\")\n",
    "            \n",
    "        # 保存结果\n",
    "        if args.output:\n",
    "            output_path = Path(args.output)\n",
    "        else:\n",
    "            output_path = Path(config['logging']['results_dir']) / f\"{args.model}_{args.split}_metrics.json\"\n",
    "            \n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        results_data = {\n",
    "            'model': args.model,\n",
    "            'checkpoint': args.checkpoint,\n",
    "            'split': args.split,\n",
    "            'num_samples': len(dataset),\n",
    "            'metrics': final_metrics,\n",
    "            'config': config\n",
    "        }\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(results_data, f, indent=2)\n",
    "            \n",
    "        logger.info(f\"✅ 评估完成，结果保存至: {output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ 评估失败: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# ==============================================================================\n",
    "# scripts/visualize.py\n",
    "# 可视化脚本\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path(__file__).parent.parent))\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from configs import ConfigLoader\n",
    "from datasets import ChestXrayDataset, DataLoaderFactory\n",
    "from transforms import get_transforms\n",
    "from engine import InferenceEngine\n",
    "from utils import setup_logging, get_device\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"解析命令行参数\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"可视化肺部分割结果\")\n",
    "    \n",
    "    parser.add_argument('--config', type=str, default='default',\n",
    "                       help='配置文件名称')\n",
    "    parser.add_argument('--checkpoint', type=str, required=True,\n",
    "                       help='模型检查点路径')\n",
    "    parser.add_argument('--model', type=str, default='unet',\n",
    "                       help='模型类型')\n",
    "    parser.add_argument('--split', type=str, default='test',\n",
    "                       choices=['train', 'val', 'test'],\n",
    "                       help='可视化的数据分割')\n",
    "    parser.add_argument('--num-samples', type=int, default=5,\n",
    "                       help='可视化样本数量')\n",
    "    parser.add_argument('--output-dir', type=str, default=None,\n",
    "                       help='输出目录')\n",
    "    parser.add_argument('--gpu', type=int, default=None,\n",
    "                       help='指定GPU ID')\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "def visualize_predictions(images, masks, predictions, image_ids, output_dir, num_samples=5):\n",
    "    \"\"\"可视化预测结果\"\"\"\n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
    "    \n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "        \n",
    "    for i in range(min(num_samples, len(images))):\n",
    "        image = images[i].squeeze().numpy()\n",
    "        mask = masks[i].squeeze().numpy()\n",
    "        prediction = predictions[i].squeeze().numpy()\n",
    "        image_id = image_ids[i] if i < len(image_ids) else f\"sample_{i}\"\n",
    "        \n",
    "        # 原图\n",
    "        axes[i, 0].imshow(image, cmap='gray')\n",
    "        axes[i, 0].set_title(f'Image: {image_id}')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # 真实掩码\n",
    "        axes[i, 1].imshow(mask, cmap='gray')\n",
    "        axes[i, 1].set_title('Ground Truth')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # 预测掩码\n",
    "        axes[i, 2].imshow(prediction, cmap='gray')\n",
    "        axes[i, 2].set_title('Prediction')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        # 叠加显示\n",
    "        overlay = np.stack([image, image, image], axis=-1)\n",
    "        # 绿色显示真实掩码，红色显示预测掩码\n",
    "        overlay[:, :, 1] = np.maximum(overlay[:, :, 1], mask * 0.5)  # 绿色\n",
    "        overlay[:, :, 0] = np.maximum(overlay[:, :, 0], prediction * 0.5)  # 红色\n",
    "        \n",
    "        axes[i, 3].imshow(overlay)\n",
    "        axes[i, 3].set_title('Overlay (GT=Green, Pred=Red)')\n",
    "        axes[i, 3].axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 保存图像\n",
    "    if output_dir:\n",
    "        output_path = Path(output_dir) / 'predictions_visualization.png'\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"可视化结果保存至: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    args = parse_args()\n",
    "    \n",
    "    # 设置日志\n",
    "    setup_logging()\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    try:\n",
    "        # 加载配置\n",
    "        config_loader = ConfigLoader()\n",
    "        config = config_loader.load_config(args.config)\n",
    "        config['model']['name'] = args.model\n",
    "        \n",
    "        # 获取设备\n",
    "        device = get_device(args.gpu)\n",
    "        \n",
    "        # 创建模型和推理引擎\n",
    "        from scripts.train import create_model\n",
    "        model = create_model(config['model'], device)\n",
    "        inference_engine = InferenceEngine(model, device, config)\n",
    "        inference_engine.load_checkpoint(args.checkpoint)\n",
    "        \n",
    "        # 创建数据集\n",
    "        transform = get_transforms(args.split, config)\n",
    "        dataset = ChestXrayDataset(config, split=args.split, transform=transform)\n",
    "        \n",
    "        factory = DataLoaderFactory(config)\n",
    "        dataloader = factory.create_dataloader(dataset, shuffle=False, batch_size=args.num_samples)\n",
    "        \n",
    "        # 获取第一个批次进行可视化\n",
    "        batch = next(iter(dataloader))\n",
    "        \n",
    "        # 进行预测\n",
    "        results = inference_engine.predict_batch(batch)\n",
    "        \n",
    "        # 设置输出目录\n",
    "        if args.output_dir:\n",
    "            output_dir = args.output_dir\n",
    "        else:\n",
    "            output_dir = Path(config['logging']['vis_dir']) / args.model\n",
    "            \n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 可视化\n",
    "        visualize_predictions(\n",
    "            images=batch['image'],\n",
    "            masks=batch['mask'],\n",
    "            predictions=results['predictions'],\n",
    "            image_ids=batch.get('image_id', []),\n",
    "            output_dir=output_dir,\n",
    "            num_samples=args.num_samples\n",
    "        )\n",
    "        \n",
    "        logger.info(\"✅ 可视化完成!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ 可视化失败: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# ==============================================================================\n",
    "# tests/test_dataset.py\n",
    "# 数据集测试\n",
    "\n",
    "import unittest\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "sys.path.append(str(Path(__file__).parent.parent))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import ChestXrayDataset, DataLoaderFactory\n",
    "from configs import ConfigLoader\n",
    "\n",
    "class TestChestXrayDataset(unittest.TestCase):\n",
    "    \"\"\"测试胸片数据集\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"设置测试环境\"\"\"\n",
    "        # 创建临时目录和配置\n",
    "        self.temp_dir = tempfile.mkdtemp()\n",
    "        self.config = {\n",
    "            'data': {\n",
    "                'root_dir': self.temp_dir,\n",
    "                'image_dir': 'image',\n",
    "                'mask_dir': 'mask',\n",
    "                'split_file': 'split.csv',\n",
    "                'image_size': 256,\n",
    "                'num_workers': 0,\n",
    "                'pin_memory': False\n",
    "            },\n",
    "            'training': {\n",
    "                'batch_size': 2\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 创建测试数据\n",
    "        self._create_test_data()\n",
    "        \n",
    "    def tearDown(self):\n",
    "        \"\"\"清理测试环境\"\"\"\n",
    "        shutil.rmtree(self.temp_dir)\n",
    "        \n",
    "    def _create_test_data(self):\n",
    "        \"\"\"创建测试数据\"\"\"\n",
    "        import pandas as pd\n",
    "        import cv2\n",
    "        \n",
    "        # 创建目录\n",
    "        image_dir = Path(self.temp_dir) / 'image'\n",
    "        mask_dir = Path(self.temp_dir) / 'mask'\n",
    "        image_dir.mkdir(parents=True)\n",
    "        mask_dir.mkdir(parents=True)\n",
    "        \n",
    "        # 创建测试图像和掩码\n",
    "        for i in range(5):\n",
    "            # 创建随机图像\n",
    "            image = np.random.randint(0, 255, (256, 256), dtype=np.uint8)\n",
    "            mask = np.random.randint(0, 2, (256, 256), dtype=np.uint8) * 255\n",
    "            \n",
    "            image_path = image_dir / f'test_{i:03d}.png'\n",
    "            mask_path = mask_dir / f'test_{i:03d}.png'\n",
    "            \n",
    "            cv2.imwrite(str(image_path), image)\n",
    "            cv2.imwrite(str(mask_path), mask)\n",
    "            \n",
    "        # 创建分割文件\n",
    "        split_data = []\n",
    "        for i in range(5):\n",
    "            split_data.append({\n",
    "                'image_id': f'test_{i:03d}',\n",
    "                'image_path': f'image/test_{i:03d}.png',\n",
    "                'mask_path': f'mask/test_{i:03d}.png',\n",
    "                'split': 'train' if i < 3 else 'val',\n",
    "                'lung_ratio': 0.3 + i * 0.1\n",
    "            })\n",
    "            \n",
    "        split_df = pd.DataFrame(split_data)\n",
    "        split_df.to_csv(Path(self.temp_dir) / 'split.csv', index=False)\n",
    "        \n",
    "    def test_dataset_creation(self):\n",
    "        \"\"\"测试数据集创建\"\"\"\n",
    "        dataset = ChestXrayDataset(self.config, split='train')\n",
    "        self.assertEqual(len(dataset), 3)  # 3个训练样本\n",
    "        \n",
    "    def test_dataset_getitem(self):\n",
    "        \"\"\"测试数据获取\"\"\"\n",
    "        dataset = ChestXrayDataset(self.config, split='train')\n",
    "        sample = dataset[0]\n",
    "        \n",
    "        # 检查返回格式\n",
    "        self.assertIn('image', sample)\n",
    "        self.assertIn('mask', sample)\n",
    "        self.assertIn('image_id', sample)\n",
    "        \n",
    "        # 检查tensor形状\n",
    "        self.assertEqual(len(sample['image'].shape), 3)  # [C, H, W]\n",
    "        self.assertEqual(len(sample['mask'].shape), 3)   # [C, H, W]\n",
    "        \n",
    "    def test_dataloader_factory(self):\n",
    "        \"\"\"测试数据加载器工厂\"\"\"\n",
    "        factory = DataLoaderFactory(self.config)\n",
    "        \n",
    "        dataset = ChestXrayDataset(self.config, split='train')\n",
    "        dataloader = factory.create_dataloader(dataset)\n",
    "        \n",
    "        # 获取一个批次\n",
    "        batch = next(iter(dataloader))\n",
    "        \n",
    "        # 检查批次形状\n",
    "        self.assertEqual(batch['image'].shape[0], 2)  # batch_size\n",
    "        self.assertEqual(batch['mask'].shape[0], 2)\n",
    "\n",
    "class TestLossFunctions(unittest.TestCase):\n",
    "    \"\"\"测试损失函数\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"设置测试数据\"\"\"\n",
    "        self.batch_size = 2\n",
    "        self.height, self.width = 64, 64\n",
    "        \n",
    "        # 创建测试数据\n",
    "        self.predictions = torch.randn(self.batch_size, 1, self.height, self.width, requires_grad=True)\n",
    "        self.targets = torch.randint(0, 2, (self.batch_size, 1, self.height, self.width)).float()\n",
    "        \n",
    "    def test_dice_loss(self):\n",
    "        \"\"\"测试Dice损失\"\"\"\n",
    "        from losses import DiceLoss\n",
    "        \n",
    "        dice_loss = DiceLoss()\n",
    "        loss = dice_loss(self.predictions, self.targets)\n",
    "        \n",
    "        # 检查损失值\n",
    "        self.assertTrue(0 <= loss.item() <= 1)\n",
    "        \n",
    "        # 检查梯度\n",
    "        loss.backward()\n",
    "        self.assertIsNotNone(self.predictions.grad)\n",
    "        \n",
    "    def test_combined_loss(self):\n",
    "        \"\"\"测试组合损失\"\"\"\n",
    "        from losses import get_loss_fn\n",
    "        \n",
    "        config = {\n",
    "            'training': {\n",
    "                'loss_weights': {\n",
    "                    'bce': 0.4,\n",
    "                    'dice': 0.6\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        loss_fn = get_loss_fn(config)\n",
    "        loss = loss_fn(self.predictions, self.targets)\n",
    "        \n",
    "        self.assertIsInstance(loss.item(), float)\n",
    "\n",
    "class TestMetrics(unittest.TestCase):\n",
    "    \"\"\"测试评估指标\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"设置测试数据\"\"\"\n",
    "        self.predictions = torch.sigmoid(torch.randn(2, 1, 64, 64))\n",
    "        self.targets = torch.randint(0, 2, (2, 1, 64, 64)).float()\n",
    "        \n",
    "    def test_dice_score(self):\n",
    "        \"\"\"测试Dice分数\"\"\"\n",
    "        from metrics import DiceScore\n",
    "        \n",
    "        dice_metric = DiceScore()\n",
    "        dice_metric.update(self.predictions, self.targets)\n",
    "        score = dice_metric.compute()\n",
    "        \n",
    "        self.assertTrue(0 <= score <= 1)\n",
    "        \n",
    "    def test_metrics_calculator(self):\n",
    "        \"\"\"测试指标计算器\"\"\"\n",
    "        from metrics import MetricsCalculator\n",
    "        \n",
    "        calculator = MetricsCalculator()\n",
    "        calculator.update(self.predictions, self.targets)\n",
    "        results = calculator.compute()\n",
    "        \n",
    "        # 检查返回的指标\n",
    "        expected_metrics = ['dice', 'iou', 'pixel_accuracy', 'precision', 'recall']\n",
    "        for metric in expected_metrics:\n",
    "            self.assertIn(metric, results)\n",
    "            self.assertTrue(0 <= results[metric] <= 1)\n",
    "\n",
    "class TestTrainingEngine(unittest.TestCase):\n",
    "    \"\"\"测试训练引擎\"\"\"\n",
    "    \n",
    "    def test_early_stopping(self):\n",
    "        \"\"\"测试早停机制\"\"\"\n",
    "        from engine import EarlyStopping\n",
    "        \n",
    "        early_stopping = EarlyStopping(patience=2, min_delta=0.01)\n",
    "        \n",
    "        # 模拟训练过程\n",
    "        model = torch.nn.Linear(10, 1)\n",
    "        scores = [1.0, 0.8, 0.79, 0.785, 0.784]\n",
    "        \n",
    "        should_stop = False\n",
    "        for score in scores:\n",
    "            should_stop = early_stopping(score, model)\n",
    "            if should_stop:\n",
    "                break\n",
    "                \n",
    "        self.assertTrue(should_stop)  # 应该在最后触发早停\n",
    "        \n",
    "    def test_checkpoint_manager(self):\n",
    "        \"\"\"测试检查点管理器\"\"\"\n",
    "        from engine import CheckpointManager\n",
    "        import tempfile\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            checkpoint_manager = CheckpointManager(temp_dir)\n",
    "            \n",
    "            model = torch.nn.Linear(10, 1)\n",
    "            optimizer = torch.optim.Adam(model.parameters())\n",
    "            \n",
    "            # 保存检查点\n",
    "            checkpoint_manager.save_checkpoint(\n",
    "                epoch=0,\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                metrics={'loss': 0.5},\n",
    "                is_best=True\n",
    "            )\n",
    "            \n",
    "            # 检查文件是否存在\n",
    "            best_path = checkpoint_manager.get_best_checkpoint_path()\n",
    "            self.assertIsNotNone(best_path)\n",
    "            self.assertTrue(Path(best_path).exists())\n",
    "\n",
    "# ==============================================================================\n",
    "# utils/common.py\n",
    "# 通用工具函数\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    设置随机种子以确保结果可复现\n",
    "    \n",
    "    Args:\n",
    "        seed: 随机种子值\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # 设置确定性算法\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # 设置环境变量\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def setup_logging(log_level: str = 'INFO', \n",
    "                 log_file: Optional[str] = None,\n",
    "                 debug: bool = False):\n",
    "    \"\"\"\n",
    "    设置日志系统\n",
    "    \n",
    "    Args:\n",
    "        log_level: 日志级别\n",
    "        log_file: 日志文件路径\n",
    "        debug: 是否为调试模式\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        log_level = 'DEBUG'\n",
    "        \n",
    "    # 配置日志格式\n",
    "    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    date_format = '%Y-%m-%d %H:%M:%S'\n",
    "    \n",
    "    # 基础配置\n",
    "    logging.basicConfig(\n",
    "        level=getattr(logging, log_level.upper()),\n",
    "        format=log_format,\n",
    "        datefmt=date_format,\n",
    "        handlers=[]\n",
    "    )\n",
    "    \n",
    "    # 控制台输出\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(logging.Formatter(log_format, date_format))\n",
    "    logging.getLogger().addHandler(console_handler)\n",
    "    \n",
    "    # 文件输出\n",
    "    if log_file:\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setFormatter(logging.Formatter(log_format, date_format))\n",
    "        logging.getLogger().addHandler(file_handler)\n",
    "\n",
    "def get_device(gpu_id: Optional[int] = None) -> torch.device:\n",
    "    \"\"\"\n",
    "    获取计算设备\n",
    "    \n",
    "    Args:\n",
    "        gpu_id: GPU ID，None时自动选择\n",
    "        \n",
    "    Returns:\n",
    "        torch设备对象\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        if gpu_id is not None:\n",
    "            if gpu_id >= torch.cuda.device_count():\n",
    "                raise ValueError(f\"GPU {gpu_id} 不存在，可用GPU数量: {torch.cuda.device_count()}\")\n",
    "            device = torch.device(f'cuda:{gpu_id}')\n",
    "        else:\n",
    "            device = torch.device('cuda')\n",
    "        \n",
    "        # 打印GPU信息\n",
    "        gpu_name = torch.cuda.get_device_name(device)\n",
    "        gpu_memory = torch.cuda.get_device_properties(device).total_memory / 1024**3\n",
    "        logging.info(f\"使用GPU: {gpu_name} ({gpu_memory:.1f}GB)\")\n",
    "        \n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        logging.info(\"使用CPU进行计算\")\n",
    "        \n",
    "    return device\n",
    "\n",
    "def count_parameters(model: torch.nn.Module) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    统计模型参数数量\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch模型\n",
    "        \n",
    "    Returns:\n",
    "        (总参数数量, 可训练参数数量)\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    return total_params, trainable_params\n",
    "\n",
    "class ModelSummary:\n",
    "    \"\"\"模型摘要工具\"\"\"\n",
    "    \n",
    "    def __init__(self, model: torch.nn.Module):\n",
    "        self.model = model\n",
    "        \n",
    "    def print_summary(self, input_size: Tuple[int, ...], device: str = 'cpu'):\n",
    "        \"\"\"\n",
    "        打印模型摘要\n",
    "        \n",
    "        Args:\n",
    "            input_size: 输入尺寸 (C, H, W)\n",
    "            device: 设备\n",
    "        \"\"\"\n",
    "        model = self.model.to(device)\n",
    "        \n",
    "        # 统计参数\n",
    "        total_params, trainable_params = count_parameters(model)\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(f\"模型摘要: {model.__class__.__name__}\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"输入尺寸: {input_size}\")\n",
    "        print(f\"总参数数量: {total_params:,}\")\n",
    "        print(f\"可训练参数: {trainable_params:,}\")\n",
    "        print(f\"模型大小: {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
    "        \n",
    "        # 计算模型输出尺寸\n",
    "        try:\n",
    "            dummy_input = torch.randn(1, *input_size).to(device)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = model(dummy_input)\n",
    "            print(f\"输出尺寸: {tuple(output.shape[1:])}\")\n",
    "        except Exception as e:\n",
    "            print(f\"无法计算输出尺寸: {e}\")\n",
    "            \n",
    "        print(\"=\" * 80)\n",
    "\n",
    "def save_plot(fig, filepath: Union[str, Path], dpi: int = 300):\n",
    "    \"\"\"\n",
    "    保存图像\n",
    "    \n",
    "    Args:\n",
    "        fig: matplotlib图像对象\n",
    "        filepath: 保存路径\n",
    "        dpi: 分辨率\n",
    "    \"\"\"\n",
    "    filepath = Path(filepath)\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    fig.savefig(filepath, dpi=dpi, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "def format_time(seconds: float) -> str:\n",
    "    \"\"\"\n",
    "    格式化时间\n",
    "    \n",
    "    Args:\n",
    "        seconds: 秒数\n",
    "        \n",
    "    Returns:\n",
    "        格式化的时间字符串\n",
    "    \"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.1f}s\"\n",
    "    elif seconds < 3600:\n",
    "        minutes = seconds // 60\n",
    "        seconds = seconds % 60\n",
    "        return f\"{int(minutes)}m {seconds:.1f}s\"\n",
    "    else:\n",
    "        hours = seconds // 3600\n",
    "        minutes = (seconds % 3600) // 60\n",
    "        seconds = seconds % 60\n",
    "        return f\"{int(hours)}h {int(minutes)}m {seconds:.1f}s\"\n",
    "\n",
    "# ==============================================================================\n",
    "# utils/__init__.py\n",
    "\n",
    "from .common import (\n",
    "    set_seed,\n",
    "    setup_logging,\n",
    "    get_device,\n",
    "    count_parameters,\n",
    "    ModelSummary,\n",
    "    save_plot,\n",
    "    format_time\n",
    ")\n",
    "\n",
    "__all__ = [\n",
    "    'set_seed',\n",
    "    'setup_logging', \n",
    "    'get_device',\n",
    "    'count_parameters',\n",
    "    'ModelSummary',\n",
    "    'save_plot',\n",
    "    'format_time'\n",
    "]\n",
    "\n",
    "# ==============================================================================\n",
    "# 运行测试脚本\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 运行单元测试\n",
    "    import unittest\n",
    "    \n",
    "    # 创建测试套件\n",
    "    test_loader = unittest.TestLoader()\n",
    "    test_suite = unittest.TestSuite()\n",
    "    \n",
    "    # 添加测试类\n",
    "    test_classes = [\n",
    "        TestChestXrayDataset,\n",
    "        TestLossFunctions,\n",
    "        TestMetrics,\n",
    "        TestTrainingEngine\n",
    "    ]\n",
    "    \n",
    "    for test_class in test_classes:\n",
    "        tests = test_loader.loadTestsFromTestCase(test_class)\n",
    "        test_suite.addTests(tests)\n",
    "        \n",
    "    # 运行测试\n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    result = runner.run(test_suite)\n",
    "    \n",
    "    # 输出结果\n",
    "    if result.wasSuccessful():\n",
    "        print(\"\\n✅ 所有测试通过!\")\n",
    "    else:\n",
    "        print(f\"\\n❌ {len(result.failures)} 个测试失败, {len(result.errors)} 个错误\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c10696-58f2-4a19-b9fe-f74b16ac183b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bbbda1-bb6c-4054-aa72-57e740934264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0703b7dc-0bc5-444c-a86c-9e8885bcbd41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f58287c-c4df-4697-95bf-f723501e9a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/unet.py\n",
    "# 经典2D U-Net实现，4倍下采样，单通道输入，二分类输出\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional\n",
    "import logging\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"双卷积层：Conv2d -> BatchNorm -> ReLU -> Conv2d -> BatchNorm -> ReLU\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, mid_channels: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        初始化双卷积层\n",
    "        \n",
    "        Args:\n",
    "            in_channels: 输入通道数\n",
    "            out_channels: 输出通道数\n",
    "            mid_channels: 中间通道数，None时等于out_channels\n",
    "        \"\"\"\n",
    "        super(DoubleConv, self).__init__()\n",
    "        \n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "            \n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"下采样层：MaxPool2d -> DoubleConv\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super(Down, self).__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"上采样层：支持双线性插值或转置卷积\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, bilinear: bool = True):\n",
    "        \"\"\"\n",
    "        初始化上采样层\n",
    "        \n",
    "        Args:\n",
    "            in_channels: 输入通道数\n",
    "            out_channels: 输出通道数\n",
    "            bilinear: 是否使用双线性插值上采样，False时使用转置卷积\n",
    "        \"\"\"\n",
    "        super(Up, self).__init__()\n",
    "        \n",
    "        if bilinear:\n",
    "            # 双线性插值上采样\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            # 转置卷积上采样\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "            \n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Args:\n",
    "            x1: 来自下层的特征图（低分辨率）\n",
    "            x2: 来自跳跃连接的特征图（高分辨率）\n",
    "        \"\"\"\n",
    "        # 上采样x1\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # 处理尺寸不匹配的情况\n",
    "        # 输入格式：[N, C, H, W]\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        \n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                       diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        # 拼接特征图\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        \n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    \"\"\"输出卷积层：1x1卷积\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    经典U-Net网络\n",
    "    \n",
    "    参考论文: U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "    https://arxiv.org/abs/1505.04597\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels: int = 1,\n",
    "                 out_channels: int = 1,\n",
    "                 features: List[int] = [64, 128, 256, 512],\n",
    "                 bilinear: bool = False,\n",
    "                 dropout_rate: float = 0.0):\n",
    "        \"\"\"\n",
    "        初始化U-Net\n",
    "        \n",
    "        Args:\n",
    "            in_channels: 输入通道数（灰度图像为1）\n",
    "            out_channels: 输出通道数（二分类为1）\n",
    "            features: 每层的特征通道数 [64, 128, 256, 512]\n",
    "            bilinear: 是否使用双线性插值上采样\n",
    "            dropout_rate: Dropout比率，用于正则化\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.features = features\n",
    "        self.bilinear = bilinear\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # 输入卷积\n",
    "        self.inc = DoubleConv(in_channels, features[0])\n",
    "        \n",
    "        # 下采样路径（编码器）\n",
    "        self.down1 = Down(features[0], features[1])\n",
    "        self.down2 = Down(features[1], features[2])\n",
    "        self.down3 = Down(features[2], features[3])\n",
    "        \n",
    "        # 瓶颈层（最深层）\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(features[3], features[3] * 2 // factor)\n",
    "        \n",
    "        # Dropout层用于正则化\n",
    "        if dropout_rate > 0:\n",
    "            self.dropout = nn.Dropout2d(p=dropout_rate)\n",
    "        else:\n",
    "            self.dropout = nn.Identity()\n",
    "            \n",
    "        # 上采样路径（解码器）\n",
    "        self.up1 = Up(features[3] * 2, features[3] // factor, bilinear)\n",
    "        self.up2 = Up(features[3], features[2] // factor, bilinear)\n",
    "        self.up3 = Up(features[2], features[1] // factor, bilinear)\n",
    "        self.up4 = Up(features[1], features[0], bilinear)\n",
    "        \n",
    "        # 输出卷积\n",
    "        self.outc = OutConv(features[0], out_channels)\n",
    "        \n",
    "        # 权重初始化\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Args:\n",
    "            x: 输入图像 [N, C, H, W]\n",
    "            \n",
    "        Returns:\n",
    "            输出logits [N, out_channels, H, W]\n",
    "        \"\"\"\n",
    "        # 编码器路径\n",
    "        x1 = self.inc(x)        # [N, 64, H, W]\n",
    "        x2 = self.down1(x1)     # [N, 128, H/2, W/2]\n",
    "        x3 = self.down2(x2)     # [N, 256, H/4, W/4]\n",
    "        x4 = self.down3(x3)     # [N, 512, H/8, W/8]\n",
    "        x5 = self.down4(x4)     # [N, 1024/512, H/16, W/16]\n",
    "        \n",
    "        # 在瓶颈层应用Dropout\n",
    "        x5 = self.dropout(x5)\n",
    "        \n",
    "        # 解码器路径\n",
    "        x = self.up1(x5, x4)    # [N, 512, H/8, W/8]\n",
    "        x = self.up2(x, x3)     # [N, 256, H/4, W/4]\n",
    "        x = self.up3(x, x2)     # [N, 128, H/2, W/2]\n",
    "        x = self.up4(x, x1)     # [N, 64, H, W]\n",
    "        \n",
    "        # 输出层\n",
    "        logits = self.outc(x)   # [N, out_channels, H, W]\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"权重初始化\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "                    \n",
    "    def get_model_info(self) -> dict:\n",
    "        \"\"\"获取模型信息\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        return {\n",
    "            'model_name': 'UNet',\n",
    "            'in_channels': self.in_channels,\n",
    "            'out_channels': self.out_channels,\n",
    "            'features': self.features,\n",
    "            'bilinear': self.bilinear,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'model_size_mb': total_params * 4 / 1024 / 1024  # 假设float32\n",
    "        }\n",
    "        \n",
    "    def get_feature_maps(self, x):\n",
    "        \"\"\"\n",
    "        获取中间特征图（用于可视化和分析）\n",
    "        \n",
    "        Args:\n",
    "            x: 输入图像\n",
    "            \n",
    "        Returns:\n",
    "            各层特征图字典\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # 编码器特征\n",
    "        x1 = self.inc(x)\n",
    "        features['enc1'] = x1\n",
    "        \n",
    "        x2 = self.down1(x1)\n",
    "        features['enc2'] = x2\n",
    "        \n",
    "        x3 = self.down2(x2)\n",
    "        features['enc3'] = x3\n",
    "        \n",
    "        x4 = self.down3(x3)\n",
    "        features['enc4'] = x4\n",
    "        \n",
    "        x5 = self.down4(x4)\n",
    "        x5 = self.dropout(x5)\n",
    "        features['bottleneck'] = x5\n",
    "        \n",
    "        # 解码器特征\n",
    "        x = self.up1(x5, x4)\n",
    "        features['dec1'] = x\n",
    "        \n",
    "        x = self.up2(x, x3)\n",
    "        features['dec2'] = x\n",
    "        \n",
    "        x = self.up3(x, x2)\n",
    "        features['dec3'] = x\n",
    "        \n",
    "        x = self.up4(x, x1)\n",
    "        features['dec4'] = x\n",
    "        \n",
    "        # 输出\n",
    "        logits = self.outc(x)\n",
    "        features['output'] = logits\n",
    "        \n",
    "        return features\n",
    "\n",
    "# ==============================================================================\n",
    "# models/__init__.py\n",
    "\n",
    "from .unet import UNet\n",
    "\n",
    "__all__ = ['UNet']\n",
    "\n",
    "# ==============================================================================\n",
    "# 使用示例和测试\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import torch\n",
    "    import logging\n",
    "    \n",
    "    # 设置日志\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # 测试U-Net模型\n",
    "    logger.info(\"🔄 测试U-Net模型...\")\n",
    "    \n",
    "    # 创建模型\n",
    "    model = UNet(\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        features=[64, 128, 256, 512],\n",
    "        bilinear=False,\n",
    "        dropout_rate=0.3\n",
    "    )\n",
    "    \n",
    "    # 打印模型信息\n",
    "    model_info = model.get_model_info()\n",
    "    logger.info(\"📊 模型信息:\")\n",
    "    for key, value in model_info.items():\n",
    "        logger.info(f\"  {key}: {value}\")\n",
    "    \n",
    "    # 测试前向传播\n",
    "    batch_size = 2\n",
    "    height, width = 512, 512\n",
    "    \n",
    "    dummy_input = torch.randn(batch_size, 1, height, width)\n",
    "    logger.info(f\"输入形状: {dummy_input.shape}\")\n",
    "    \n",
    "    try:\n",
    "        # 前向传播\n",
    "        with torch.no_grad():\n",
    "            output = model(dummy_input)\n",
    "        \n",
    "        logger.info(f\"输出形状: {output.shape}\")\n",
    "        logger.info(f\"输出值范围: [{output.min().item():.4f}, {output.max().item():.4f}]\")\n",
    "        \n",
    "        # 测试梯度计算\n",
    "        dummy_input.requires_grad_(True)\n",
    "        output = model(dummy_input)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        logger.info(\"✅ 梯度计算正常\")\n",
    "        \n",
    "        # 测试特征图提取\n",
    "        features = model.get_feature_maps(dummy_input[:1])  # 使用单个样本\n",
    "        logger.info(\"📈 特征图形状:\")\n",
    "        for name, feature in features.items():\n",
    "            logger.info(f\"  {name}: {feature.shape}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ 模型测试失败: {e}\")\n",
    "        raise\n",
    "        \n",
    "    # 测试不同配置\n",
    "    logger.info(\"\\n🔄 测试不同配置...\")\n",
    "    \n",
    "    configs = [\n",
    "        {'bilinear': True, 'dropout_rate': 0.0},\n",
    "        {'features': [32, 64, 128, 256], 'dropout_rate': 0.5},\n",
    "        {'in_channels': 3, 'out_channels': 2}  # 多通道输入/输出\n",
    "    ]\n",
    "    \n",
    "    for i, config in enumerate(configs):\n",
    "        try:\n",
    "            test_model = UNet(**config)\n",
    "            test_input = torch.randn(1, config.get('in_channels', 1), 256, 256)\n",
    "            test_output = test_model(test_input)\n",
    "            \n",
    "            logger.info(f\"✅ 配置 {i+1} 测试通过: {test_output.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ 配置 {i+1} 测试失败: {e}\")\n",
    "            \n",
    "    logger.info(\"\\n✅ U-Net模型测试完成！\")\n",
    "    \n",
    "    # 计算模型复杂度\n",
    "    def count_flops(model, input_size):\n",
    "        \"\"\"简单的FLOPs估算\"\"\"\n",
    "        # 这是一个简化的估算，实际FLOPs会更复杂\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        # 假设每个参数在前向传播中参与一次乘法运算\n",
    "        flops = total_params * input_size[2] * input_size[3] / (512 * 512)  # 标准化到512x512\n",
    "        return flops\n",
    "        \n",
    "    flops = count_flops(model, (1, 1, 512, 512))\n",
    "    logger.info(f\"📊 估算FLOPs: {flops:.2e}\")\n",
    "    logger.info(f\"📊 模型大小: {model_info['model_size_mb']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e78dae-0206-4133-a395-4b2c0b0bdc66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be98ba9a-d9a2-4d72-8157-b660aa22b4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e33d3-0848-4c26-a384-d9c6c5376bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5483d8f2-8a73-4c2f-98a2-ee6b01f96e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs/unet.yaml\n",
    "# U-Net专属配置文件\n",
    "\n",
    "# 继承默认配置\n",
    "defaults:\n",
    "  - default\n",
    "\n",
    "# 实验信息\n",
    "experiment:\n",
    "  name: \"unet_baseline\"\n",
    "  tags: [\"unet\", \"baseline\", \"lung_segmentation\"]\n",
    "  seed: 42\n",
    "  deterministic: true\n",
    "  \n",
    "  # Weights & Biases配置\n",
    "  wandb:\n",
    "    enabled: false  # 设置为true启用wandb日志\n",
    "    project: \"lung_segmentation\"\n",
    "    entity: null\n",
    "    group: \"unet_experiments\"\n",
    "\n",
    "# 模型配置\n",
    "model:\n",
    "  name: \"unet\"\n",
    "  in_channels: 1\n",
    "  out_channels: 1\n",
    "  \n",
    "  # U-Net特定参数\n",
    "  features: [64, 128, 256, 512]  # 每层特征通道数\n",
    "  bilinear: false                # 使用转置卷积而非双线性插值\n",
    "  dropout_rate: 0.3              # Dropout正则化\n",
    "\n",
    "# 数据配置（覆盖默认值）\n",
    "data:\n",
    "  root_dir: \"data/chest_xray\"\n",
    "  image_dir: \"image\"\n",
    "  mask_dir: \"mask\" \n",
    "  metadata_file: \"metadata.csv\"\n",
    "  split_file: \"split.csv\"\n",
    "  \n",
    "  # 图像预处理\n",
    "  image_size: 512\n",
    "  num_workers: 4\n",
    "  pin_memory: true\n",
    "  \n",
    "  # 数据分割比例\n",
    "  train_ratio: 0.7\n",
    "  val_ratio: 0.15\n",
    "  test_ratio: 0.15\n",
    "\n",
    "# 训练配置\n",
    "training:\n",
    "  # 基础训练参数\n",
    "  batch_size: 8\n",
    "  num_epochs: 100\n",
    "  learning_rate: 1e-4\n",
    "  weight_decay: 1e-5\n",
    "  \n",
    "  # 梯度相关\n",
    "  gradient_clip_val: 1.0  # 梯度裁剪阈值\n",
    "  \n",
    "  # 优化器配置\n",
    "  optimizer: \"adam\"\n",
    "  optimizer_params:\n",
    "    betas: [0.9, 0.999]\n",
    "    eps: 1e-8\n",
    "  \n",
    "  # 学习率调度器\n",
    "  scheduler: \"reduce_on_plateau\"\n",
    "  scheduler_patience: 10\n",
    "  scheduler_factor: 0.5\n",
    "  scheduler_min_lr: 1e-7\n",
    "  \n",
    "  # 早停配置\n",
    "  early_stopping:\n",
    "    patience: 20\n",
    "    min_delta: 0.001\n",
    "    restore_best_weights: true\n",
    "    monitor: \"val_loss\"  # 监控指标\n",
    "    mode: \"min\"          # 越小越好\n",
    "  \n",
    "  # 损失函数权重\n",
    "  loss_weights:\n",
    "    bce: 0.4\n",
    "    dice: 0.6\n",
    "\n",
    "# 数据增强配置（U-Net适用）\n",
    "augmentation:\n",
    "  # 几何变换\n",
    "  horizontal_flip: 0.5\n",
    "  \n",
    "  shift_scale_rotate:\n",
    "    shift_limit: 0.1\n",
    "    scale_limit: 0.1\n",
    "    rotate_limit: 15\n",
    "    p: 0.5\n",
    "  \n",
    "  # 像素级变换\n",
    "  brightness_contrast:\n",
    "    brightness_limit: 0.2\n",
    "    contrast_limit: 0.2\n",
    "    p: 0.5\n",
    "  \n",
    "  # 弹性变换（医学图像友好）\n",
    "  elastic_transform:\n",
    "    alpha: 1\n",
    "    sigma: 50\n",
    "    alpha_affine: 50\n",
    "    p: 0.3\n",
    "  \n",
    "  # 噪声和模糊\n",
    "  noise_blur:\n",
    "    gauss_noise_var: [10, 50]\n",
    "    gauss_blur_limit: 3\n",
    "    p: 0.3\n",
    "\n",
    "# 评估配置\n",
    "evaluation:\n",
    "  # 评估指标阈值\n",
    "  threshold: 0.5\n",
    "  \n",
    "  # 是否包含边界指标（计算较慢）\n",
    "  include_boundary_metrics: false\n",
    "  \n",
    "  # 测试时增强\n",
    "  tta:\n",
    "    enabled: false\n",
    "    num_augmentations: 5\n",
    "\n",
    "# 日志配置\n",
    "logging:\n",
    "  log_dir: \"logs/unet\"\n",
    "  checkpoint_dir: \"checkpoints/unet\"\n",
    "  output_dir: \"outputs/unet\"\n",
    "  vis_dir: \"outputs/vis/unet\"\n",
    "  results_dir: \"results/unet\"\n",
    "  \n",
    "  # 保存策略\n",
    "  save_every_n_epochs: 10\n",
    "  save_best_only: false\n",
    "  monitor_metric: \"val_loss\"\n",
    "  \n",
    "  # 日志级别\n",
    "  log_level: \"INFO\"\n",
    "  \n",
    "  # 可视化配置\n",
    "  visualize_every_n_epochs: 10\n",
    "  num_viz_samples: 5\n",
    "\n",
    "# 硬件配置\n",
    "hardware:\n",
    "  # GPU配置\n",
    "  gpu_id: null  # null表示自动选择\n",
    "  mixed_precision: false  # 是否使用混合精度训练\n",
    "  \n",
    "  # 内存优化\n",
    "  pin_memory: true\n",
    "  non_blocking: true\n",
    "\n",
    "# 验证配置\n",
    "validation:\n",
    "  # 验证频率\n",
    "  val_every_n_epochs: 1\n",
    "  \n",
    "  # 验证时的批次大小（可以更大）\n",
    "  val_batch_size: 16\n",
    "  \n",
    "  # 是否在验证时保存预测结果\n",
    "  save_predictions: false\n",
    "\n",
    "# 模型特定的超参数\n",
    "hyperparams:\n",
    "  # 损失函数参数\n",
    "  dice_smooth: 1e-6\n",
    "  focal_alpha: 1.0\n",
    "  focal_gamma: 2.0\n",
    "  \n",
    "  # 正则化参数\n",
    "  label_smoothing: 0.0\n",
    "  \n",
    "  # 数据相关\n",
    "  normalize_input: true\n",
    "  normalize_method: \"min_max\"  # \"min_max\", \"z_score\", \"none\"\n",
    "\n",
    "# 调试和开发配置\n",
    "debug:\n",
    "  # 快速测试模式\n",
    "  fast_dev_run: false  # 只运行几个batch进行测试\n",
    "  overfit_batches: 0   # 用于过拟合测试的batch数量\n",
    "  \n",
    "  # 日志详细程度\n",
    "  log_every_n_steps: 50\n",
    "  \n",
    "  # 性能分析\n",
    "  profile: false\n",
    "\n",
    "# 继续训练配置\n",
    "resume:\n",
    "  # 自动恢复训练\n",
    "  auto_resume: true\n",
    "  resume_from_checkpoint: null  # 指定检查点路径\n",
    "  \n",
    "  # 是否只加载权重（不加载优化器状态）\n",
    "  weights_only: false\n",
    "\n",
    "# ==============================================================================\n",
    "# configs/unet_small.yaml (小模型配置，用于快速测试)\n",
    "\n",
    "defaults:\n",
    "  - unet\n",
    "\n",
    "experiment:\n",
    "  name: \"unet_small\"\n",
    "  tags: [\"unet\", \"small\", \"debug\"]\n",
    "\n",
    "model:\n",
    "  features: [32, 64, 128, 256]  # 减少特征通道数\n",
    "  dropout_rate: 0.2\n",
    "\n",
    "data:\n",
    "  image_size: 256  # 减少图像尺寸\n",
    "\n",
    "training:\n",
    "  batch_size: 16\n",
    "  num_epochs: 20\n",
    "  learning_rate: 5e-4\n",
    "\n",
    "augmentation:\n",
    "  # 减少增强强度\n",
    "  shift_scale_rotate:\n",
    "    shift_limit: 0.05\n",
    "    scale_limit: 0.05\n",
    "    rotate_limit: 10\n",
    "    p: 0.3\n",
    "    \n",
    "  elastic_transform:\n",
    "    p: 0.1  # 减少弹性变换概率\n",
    "\n",
    "# ==============================================================================\n",
    "# configs/unet_large.yaml (大模型配置，用于最佳性能)\n",
    "\n",
    "defaults:\n",
    "  - unet\n",
    "\n",
    "experiment:\n",
    "  name: \"unet_large\"\n",
    "  tags: [\"unet\", \"large\", \"high_performance\"]\n",
    "\n",
    "model:\n",
    "  features: [64, 128, 256, 512, 1024]  # 增加层数\n",
    "  dropout_rate: 0.4\n",
    "\n",
    "data:\n",
    "  image_size: 512\n",
    "\n",
    "training:\n",
    "  batch_size: 4      # 减少batch size以适应更大的模型\n",
    "  num_epochs: 150\n",
    "  learning_rate: 5e-5  # 更小的学习率\n",
    "  \n",
    "  early_stopping:\n",
    "    patience: 30\n",
    "\n",
    "# 启用混合精度训练\n",
    "hardware:\n",
    "  mixed_precision: true\n",
    "\n",
    "# ==============================================================================\n",
    "# configs/unet_debug.yaml (调试配置)\n",
    "\n",
    "defaults:\n",
    "  - unet\n",
    "\n",
    "experiment:\n",
    "  name: \"unet_debug\"\n",
    "  tags: [\"unet\", \"debug\"]\n",
    "\n",
    "# 快速调试设置\n",
    "debug:\n",
    "  fast_dev_run: true\n",
    "  overfit_batches: 2\n",
    "  log_every_n_steps: 1\n",
    "\n",
    "training:\n",
    "  batch_size: 2\n",
    "  num_epochs: 3\n",
    "  \n",
    "data:\n",
    "  image_size: 128\n",
    "  num_workers: 0  # 避免多进程问题\n",
    "\n",
    "logging:\n",
    "  log_level: \"DEBUG\"\n",
    "  save_every_n_epochs: 1\n",
    "\n",
    "# ==============================================================================\n",
    "# 配置文件验证和使用示例\n",
    "\n",
    "\"\"\"\n",
    "使用示例:\n",
    "\n",
    "1. 标准训练:\n",
    "   python scripts/train.py --config unet --model unet\n",
    "\n",
    "2. 小模型快速测试:\n",
    "   python scripts/train.py --config unet_small --model unet\n",
    "\n",
    "3. 大模型高性能训练:\n",
    "   python scripts/train.py --config unet_large --model unet\n",
    "\n",
    "4. 调试模式:\n",
    "   python scripts/train.py --config unet_debug --model unet\n",
    "\n",
    "5. 自定义参数:\n",
    "   python scripts/train.py --config unet --model unet \\\n",
    "     training.batch_size=16 training.learning_rate=5e-4\n",
    "\n",
    "配置优先级:\n",
    "1. 命令行参数 (最高)\n",
    "2. 用户指定的配置文件\n",
    "3. 默认配置文件 (最低)\n",
    "\n",
    "重要参数说明:\n",
    "- features: 控制模型大小和容量\n",
    "- dropout_rate: 防止过拟合，医学图像通常需要较高值\n",
    "- batch_size: 根据GPU内存调整\n",
    "- learning_rate: U-Net通常使用1e-4到1e-5\n",
    "- loss_weights: BCE+Dice组合，Dice权重可以更高\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a765ea-81c8-4b8c-a2a5-29b3c65876f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b143fb-869b-4816-bd33-a6534b0c56ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b73eb0-f845-4f24-897b-7fed447d63ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db0e271-3f76-4016-b0ac-a7d2f6d0637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scripts/train.py\n",
    "# 训练脚本\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# 添加项目根目录到路径\n",
    "sys.path.append(str(Path(__file__).parent.parent))\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from configs import ConfigLoader\n",
    "from datasets import ChestXrayDataset, DataLoaderFactory\n",
    "from transforms import get_transforms\n",
    "from losses import get_loss_fn\n",
    "from metrics import create_metrics_calculator\n",
    "from engine import TrainingEngine, EarlyStopping, CheckpointManager\n",
    "from utils import set_seed, setup_logging, get_device, ModelSummary\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"解析命令行参数\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"训练肺部分割模型\")\n",
    "    \n",
    "    parser.add_argument('--config', type=str, default='default',\n",
    "                       help='配置文件名称（不含.yaml后缀）')\n",
    "    parser.add_argument('--model', type=str, default='unet',\n",
    "                       help='模型类型')\n",
    "    parser.add_argument('--resume', type=str, default=None,\n",
    "                       help='恢复训练的检查点路径')\n",
    "    parser.add_argument('--debug', action='store_true',\n",
    "                       help='调试模式')\n",
    "    parser.add_argument('--gpu', type=int, default=None,\n",
    "                       help='指定GPU ID')\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "def create_model(model_config: dict, device: torch.device):\n",
    "    \"\"\"创建模型\"\"\"\n",
    "    model_name = model_config['name'].lower()\n",
    "    \n",
    "    if model_name == 'unet':\n",
    "        from models.unet import UNet\n",
    "        model = UNet(\n",
    "            in_channels=model_config['in_channels'],\n",
    "            out_channels=model_config['out_channels'],\n",
    "            features=model_config.get('features', [64, 128, 256, 512]),\n",
    "            dropout_rate=model_config.get('dropout_rate', 0.3),\n",
    "            bilinear=model_config.get('bilinear', False)\n",
    "        )\n",
    "    elif model_name == 'attention_unet':\n",
    "        from models.attention_unet import AttentionUNet\n",
    "        model = AttentionUNet(\n",
    "            in_channels=model_config['in_channels'],\n",
    "            out_channels=model_config['out_channels'],\n",
    "            features=model_config.get('features', [64, 128, 256, 512]),\n",
    "            dropout_rate=model_config.get('dropout_rate', 0.3)\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"未知的模型类型: {model_name}\")\n",
    "        \n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    args = parse_args()\n",
    "    \n",
    "    # 设置日志\n",
    "    setup_logging(debug=args.debug)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    try:\n",
    "        # 加载配置\n",
    "        config_loader = ConfigLoader()\n",
    "        config = config_loader.load_config(args.config)\n",
    "        \n",
    "        # 覆盖模型配置\n",
    "        config['model']['name'] = args.model\n",
    "        \n",
    "        # 设置随机种子\n",
    "        set_seed(config['experiment']['seed'])\n",
    "        \n",
    "        # 获取设备\n",
    "        device = get_device(args.gpu)\n",
    "        logger.info(f\"使用设备: {device}\")\n",
    "        \n",
    "        # 创建数据变换\n",
    "        transforms = {\n",
    "            'train': get_transforms('train', config),\n",
    "            'val': get_transforms('val', config),\n",
    "            'test': get_transforms('test', config)\n",
    "        }\n",
    "        \n",
    "        # 创建数据加载器\n",
    "        factory = DataLoaderFactory(config)\n",
    "        dataloaders = factory.create_all_dataloaders(transforms)\n",
    "        \n",
    "        logger.info(f\"数据加载完成:\")\n",
    "        for split, loader in dataloaders.items():\n",
    "            logger.info(f\"  {split}: {len(loader.dataset)} 样本, {len(loader)} 批次\")\n",
    "            \n",
    "        # 创建模型\n",
    "        model = create_model(config['model'], device)\n",
    "        \n",
    "        # 打印模型摘要\n",
    "        summary = ModelSummary(model)\n",
    "        summary.print_summary(input_size=(1, config['data']['image_size'], config['data']['image_size']))\n",
    "        \n",
    "        # 创建损失函数\n",
    "        criterion = get_loss_fn(config)\n",
    "        logger.info(f\"损失函数: {type(criterion).__name__}\")\n",
    "        \n",
    "        # 创建优化器\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=config['training']['learning_rate'],\n",
    "            weight_decay=config['training']['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # 创建学习率调度器\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            patience=config['training']['scheduler_patience'],\n",
    "            factor=config['training']['scheduler_factor'],\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # 创建指标计算器\n",
    "        metrics_calculator = create_metrics_calculator(config)\n",
    "        \n",
    "        # 创建早停机制\n",
    "        early_stopping = EarlyStopping(\n",
    "            patience=config['training']['early_stopping']['patience'],\n",
    "            min_delta=config['training']['early_stopping']['min_delta'],\n",
    "            restore_best_weights=config['training']['early_stopping']['restore_best_weights']\n",
    "        )\n",
    "        \n",
    "        # 创建检查点管理器\n",
    "        checkpoint_manager = CheckpointManager(\n",
    "            checkpoint_dir=config['logging']['checkpoint_dir'],\n",
    "            save_best_only=config['logging']['save_best_only'],\n",
    "            save_every_n_epochs=config['logging']['save_every_n_epochs']\n",
    "        )\n",
    "        \n",
    "        # 创建训练引擎\n",
    "        trainer = TrainingEngine(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            config=config,\n",
    "            scheduler=scheduler,\n",
    "            metrics_calculator=metrics_calculator,\n",
    "            early_stopping=early_stopping,\n",
    "            checkpoint_manager=checkpoint_manager\n",
    "        )\n",
    "        \n",
    "        # 开始训练\n",
    "        history = trainer.fit(\n",
    "            train_loader=dataloaders['train'],\n",
    "            val_loader=dataloaders['val'],\n",
    "            num_epochs=config['training']['num_epochs'],\n",
    "            resume_from_checkpoint=args.resume\n",
    "        )\n",
    "        \n",
    "        logger.info(\"✅ 训练完成!\")\n",
    "        \n",
    "        # 保存训练历史\n",
    "        import json\n",
    "        history_path = Path(config['logging']['output_dir']) / 'training_history.json'\n",
    "        with open(history_path, 'w') as f:\n",
    "            json.dump(history, f, indent=2)\n",
    "        logger.info(f\"训练历史保存至: {history_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ 训练失败: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# ==============================================================================\n",
    "# scripts/evaluate.py\n",
    "# 评估脚本\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "sys.path.append(str(Path(__file__).parent.parent))\n",
    "\n",
    "import torch\n",
    "from configs import ConfigLoader\n",
    "from datasets import ChestXrayDataset, DataLoaderFactory\n",
    "from transforms import get_transforms\n",
    "from metrics import create_metrics_calculator\n",
    "from engine import InferenceEngine\n",
    "from utils import setup_logging, get_device\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"解析命令行参数\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"评估肺部分割模型\")\n",
    "    \n",
    "    parser.add_argument('--config', type=str, default='default',\n",
    "                       help='配置文件名称')\n",
    "    parser.add_argument('--checkpoint', type=str, required=True,\n",
    "                       help='模型检查点路径')\n",
    "    parser.add_argument('--model', type=str, default='unet',\n",
    "                       help='模型类型')\n",
    "    parser.add_argument('--split', type=str, default='test',\n",
    "                       choices=['train', 'val', 'test'],\n",
    "                       help='评估的数据分割')\n",
    "    parser.add_argument('--output', type=str, default=None,\n",
    "                       help='结果输出路径')\n",
    "    parser.add_argument('--gpu', type=int, default=None,\n",
    "                       help='指定GPU ID')\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    args = parse_args()\n",
    "    \n",
    "    # 设置日志\n",
    "    setup_logging()\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    try:\n",
    "        # 加载配置\n",
    "        config_loader = ConfigLoader()\n",
    "        config = config_loader.load_config(args.config)\n",
    "        config['model']['name'] = args.model\n",
    "        \n",
    "        # 获取设备\n",
    "        device = get_device(args.gpu)\n",
    "        logger.info(f\"使用设备: {device}\")\n",
    "        \n",
    "        # 创建模型\n",
    "        from scripts.train import create_model\n",
    "        model = create_model(config['model'], device)\n",
    "        \n",
    "        # 创建推理引擎\n",
    "        inference_engine = InferenceEngine(model, device, config)\n",
    "        inference_engine.load_checkpoint(args.checkpoint)\n",
    "        \n",
    "        # 创建数据集和数据加载器\n",
    "        transform = get_transforms(args.split, config)\n",
    "        dataset = ChestXrayDataset(config, split=args.split, transform=transform)\n",
    "        \n",
    "        factory = DataLoaderFactory(config)\n",
    "        dataloader = factory.create_dataloader(dataset, shuffle=False)\n",
    "        \n",
    "        logger.info(f\"评估数据集: {len(dataset)} 样本\")\n",
    "        \n",
    "        # 创建指标计算器\n",
    "        metrics_calculator = create_metrics_calculator(config)\n",
    "        \n",
    "        # 进行推理和评估\n",
    "        logger.info(\"开始评估...\")\n",
    "        all_predictions = []\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            # 推理\n",
    "            results = inference_engine.predict_batch(batch)\n",
    "            all_predictions.append(results)\n",
    "            \n",
    "            # 更新指标\n",
    "            metrics_calculator.update(\n",
    "                results['logits'].to(device),\n",
    "                batch['mask'].to(device)\n",
    "            )\n",
    "            \n",
    "        # 计算最终指标\n",
    "        final_metrics = metrics_calculator.compute()\n",
    "        \n",
    "        # 打印结果\n",
    "        logger.info(\"📊 评估结果:\")\n",
    "        for metric_name, value in final_metrics.items():\n",
    "            logger.info(f\"  {metric_name.upper()}: {value:.4f}\")\n",
    "            \n",
    "        # 保存结果\n",
    "        if args.output:\n",
    "            output_path = Path(args.output)\n",
    "        else:\n",
    "            output_path = Path(config['logging']['results_dir']) / f\"{args.model}_{args.split}_metrics.json\"\n",
    "            \n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        results_data = {\n",
    "            'model': args.model,\n",
    "            'checkpoint': args.checkpoint,\n",
    "            'split': args.split,\n",
    "            'num_samples': len(dataset),\n",
    "            'metrics': final_metrics,\n",
    "            'config': config\n",
    "        }\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(results_data, f, indent=2)\n",
    "            \n",
    "        logger.info(f\"✅ 评估完成，结果保存至: {output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ 评估失败: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# ==============================================================================\n",
    "# scripts/visualize.py\n",
    "# 可视化脚本\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path(__file__).parent.parent))\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from configs import ConfigLoader\n",
    "from datasets import ChestXrayDataset, DataLoaderFactory\n",
    "from transforms import get_transforms\n",
    "from engine import InferenceEngine\n",
    "from utils import setup_logging, get_device\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"解析命令行参数\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"可视化肺部分割结果\")\n",
    "    \n",
    "    parser.add_argument('--config', type=str, default='default',\n",
    "                       help='配置文件名称')\n",
    "    parser.add_argument('--checkpoint', type=str, required=True,\n",
    "                       help='模型检查点路径')\n",
    "    parser.add_argument('--model', type=str, default='unet',\n",
    "                       help='模型类型')\n",
    "    parser.add_argument('--split', type=str, default='test',\n",
    "                       choices=['train', 'val', 'test'],\n",
    "                       help='可视化的数据分割')\n",
    "    parser.add_argument('--num-samples', type=int, default=5,\n",
    "                       help='可视化样本数量')\n",
    "    parser.add_argument('--output-dir', type=str, default=None,\n",
    "                       help='输出目录')\n",
    "    parser.add_argument('--gpu', type=int, default=None,\n",
    "                       help='指定GPU ID')\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "def visualize_predictions(images, masks, predictions, image_ids, output_dir, num_samples=5):\n",
    "    \"\"\"可视化预测结果\"\"\"\n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
    "    \n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "        \n",
    "    for i in range(min(num_samples, len(images))):\n",
    "        image = images[i].squeeze().numpy()\n",
    "        mask = masks[i].squeeze().numpy()\n",
    "        prediction = predictions[i].squeeze().numpy()\n",
    "        image_id = image_ids[i] if i < len(image_ids) else f\"sample_{i}\"\n",
    "        \n",
    "        # 原图\n",
    "        axes[i, 0].imshow(image, cmap='gray')\n",
    "        axes[i, 0].set_title(f'Image: {image_id}')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # 真实掩码\n",
    "        axes[i, 1].imshow(mask, cmap='gray')\n",
    "        axes[i, 1].set_title('Ground Truth')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # 预测掩码\n",
    "        axes[i, 2].imshow(prediction, cmap='gray')\n",
    "        axes[i, 2].set_title('Prediction')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        # 叠加显示\n",
    "        overlay = np.stack([image, image, image], axis=-1)\n",
    "        # 绿色显示真实掩码，红色显示预测掩码\n",
    "        overlay[:, :, 1] = np.maximum(overlay[:, :, 1], mask * 0.5)  # 绿色\n",
    "        overlay[:, :, 0] = np.maximum(overlay[:, :, 0], prediction * 0.5)  # 红色\n",
    "        \n",
    "        axes[i, 3].imshow(overlay)\n",
    "        axes[i, 3].set_title('Overlay (GT=Green, Pred=Red)')\n",
    "        axes[i, 3].axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 保存图像\n",
    "    if output_dir:\n",
    "        output_path = Path(output_dir) / 'predictions_visualization.png'\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"可视化结果保存至: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    args = parse_args()\n",
    "    \n",
    "    # 设置日志\n",
    "    setup_logging()\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    try:\n",
    "        # 加载配置\n",
    "        config_loader = ConfigLoader()\n",
    "        config = config_loader.load_config(args.config)\n",
    "        config['model']['name'] = args.model\n",
    "        \n",
    "        # 获取设备\n",
    "        device = get_device(args.gpu)\n",
    "        \n",
    "        # 创建模型和推理引擎\n",
    "        from scripts.train import create_model\n",
    "        model = create_model(config['model'], device)\n",
    "        inference_engine = InferenceEngine(model, device, config)\n",
    "        inference_engine.load_checkpoint(args.checkpoint)\n",
    "        \n",
    "        # 创建数据集\n",
    "        transform = get_transforms(args.split, config)\n",
    "        dataset = ChestXrayDataset(config, split=args.split, transform=transform)\n",
    "        \n",
    "        factory = DataLoaderFactory(config)\n",
    "        dataloader = factory.create_dataloader(dataset, shuffle=False, batch_size=args.num_samples)\n",
    "        \n",
    "        # 获取第一个批次进行可视化\n",
    "        batch = next(iter(dataloader))\n",
    "        \n",
    "        # 进行预测\n",
    "        results = inference_engine.predict_batch(batch)\n",
    "        \n",
    "        # 设置输出目录\n",
    "        if args.output_dir:\n",
    "            output_dir = args.output_dir\n",
    "        else:\n",
    "            output_dir = Path(config['logging']['vis_dir']) / args.model\n",
    "            \n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 可视化\n",
    "        visualize_predictions(\n",
    "            images=batch['image'],\n",
    "            masks=batch['mask'],\n",
    "            predictions=results['predictions'],\n",
    "            image_ids=batch.get('image_id', []),\n",
    "            output_dir=output_dir,\n",
    "            num_samples=args.num_samples\n",
    "        )\n",
    "        \n",
    "        logger.info(\"✅ 可视化完成!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ 可视化失败: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# ==============================================================================\n",
    "# tests/test_dataset.py\n",
    "# 数据集测试\n",
    "\n",
    "import unittest\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "sys.path.append(str(Path(__file__).parent.parent))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import ChestXrayDataset, DataLoaderFactory\n",
    "from configs import ConfigLoader\n",
    "\n",
    "class TestChestXrayDataset(unittest.TestCase):\n",
    "    \"\"\"测试胸片数据集\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"设置测试环境\"\"\"\n",
    "        # 创建临时目录和配置\n",
    "        self.temp_dir = tempfile.mkdtemp()\n",
    "        self.config = {\n",
    "            'data': {\n",
    "                'root_dir': self.temp_dir,\n",
    "                'image_dir': 'image',\n",
    "                'mask_dir': 'mask',\n",
    "                'split_file': 'split.csv',\n",
    "                'image_size': 256,\n",
    "                'num_workers': 0,\n",
    "                'pin_memory': False\n",
    "            },\n",
    "            'training': {\n",
    "                'batch_size': 2\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 创建测试数据\n",
    "        self._create_test_data()\n",
    "        \n",
    "    def tearDown(self):\n",
    "        \"\"\"清理测试环境\"\"\"\n",
    "        shutil.rmtree(self.temp_dir)\n",
    "        \n",
    "    def _create_test_data(self):\n",
    "        \"\"\"创建测试数据\"\"\"\n",
    "        import pandas as pd\n",
    "        import cv2\n",
    "        \n",
    "        # 创建目录\n",
    "        image_dir = Path(self.temp_dir) / 'image'\n",
    "        mask_dir = Path(self.temp_dir) / 'mask'\n",
    "        image_dir.mkdir(parents=True)\n",
    "        mask_dir.mkdir(parents=True)\n",
    "        \n",
    "        # 创建测试图像和掩码\n",
    "        for i in range(5):\n",
    "            # 创建随机图像\n",
    "            image = np.random.randint(0, 255, (256, 256), dtype=np.uint8)\n",
    "            mask = np.random.randint(0, 2, (256, 256), dtype=np.uint8) * 255\n",
    "            \n",
    "            image_path = image_dir / f'test_{i:03d}.png'\n",
    "            mask_path = mask_dir / f'test_{i:03d}.png'\n",
    "            \n",
    "            cv2.imwrite(str(image_path), image)\n",
    "            cv2.imwrite(str(mask_path), mask)\n",
    "            \n",
    "        # 创建分割文件\n",
    "        split_data = []\n",
    "        for i in range(5):\n",
    "            split_data.append({\n",
    "                'image_id': f'test_{i:03d}',\n",
    "                'image_path': f'image/test_{i:03d}.png',\n",
    "                'mask_path': f'mask/test_{i:03d}.png',\n",
    "                'split': 'train' if i < 3 else 'val',\n",
    "                'lung_ratio': 0.3 + i * 0.1\n",
    "            })\n",
    "            \n",
    "        split_df = pd.DataFrame(split_data)\n",
    "        split_df.to_csv(Path(self.temp_dir) / 'split.csv', index=False)\n",
    "        \n",
    "    def test_dataset_creation(self):\n",
    "        \"\"\"测试数据集创建\"\"\"\n",
    "        dataset = ChestXrayDataset(self.config, split='train')\n",
    "        self.assertEqual(len(dataset), 3)  # 3个训练样本\n",
    "        \n",
    "    def test_dataset_getitem(self):\n",
    "        \"\"\"测试数据获取\"\"\"\n",
    "        dataset = ChestXrayDataset(self.config, split='train')\n",
    "        sample = dataset[0]\n",
    "        \n",
    "        # 检查返回格式\n",
    "        self.assertIn('image', sample)\n",
    "        self.assertIn('mask', sample)\n",
    "        self.assertIn('image_id', sample)\n",
    "        \n",
    "        # 检查tensor形状\n",
    "        self.assertEqual(len(sample['image'].shape), 3)  # [C, H, W]\n",
    "        self.assertEqual(len(sample['mask'].shape), 3)   # [C, H, W]\n",
    "        \n",
    "    def test_dataloader_factory(self):\n",
    "        \"\"\"测试数据加载器工厂\"\"\"\n",
    "        factory = DataLoaderFactory(self.config)\n",
    "        \n",
    "        dataset = ChestXrayDataset(self.config, split='train')\n",
    "        dataloader = factory.create_dataloader(dataset)\n",
    "        \n",
    "        # 获取一个批次\n",
    "        batch = next(iter(dataloader))\n",
    "        \n",
    "        # 检查批次形状\n",
    "        self.assertEqual(batch['image'].shape[0], 2)  # batch_size\n",
    "        self.assertEqual(batch['mask'].shape[0], 2)\n",
    "\n",
    "class TestLossFunctions(unittest.TestCase):\n",
    "    \"\"\"测试损失函数\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"设置测试数据\"\"\"\n",
    "        self.batch_size = 2\n",
    "        self.height, self.width = 64, 64\n",
    "        \n",
    "        # 创建测试数据\n",
    "        self.predictions = torch.randn(self.batch_size, 1, self.height, self.width, requires_grad=True)\n",
    "        self.targets = torch.randint(0, 2, (self.batch_size, 1, self.height, self.width)).float()\n",
    "        \n",
    "    def test_dice_loss(self):\n",
    "        \"\"\"测试Dice损失\"\"\"\n",
    "        from losses import DiceLoss\n",
    "        \n",
    "        dice_loss = DiceLoss()\n",
    "        loss = dice_loss(self.predictions, self.targets)\n",
    "        \n",
    "        # 检查损失值\n",
    "        self.assertTrue(0 <= loss.item() <= 1)\n",
    "        \n",
    "        # 检查梯度\n",
    "        loss.backward()\n",
    "        self.assertIsNotNone(self.predictions.grad)\n",
    "        \n",
    "    def test_combined_loss(self):\n",
    "        \"\"\"测试组合损失\"\"\"\n",
    "        from losses import get_loss_fn\n",
    "        \n",
    "        config = {\n",
    "            'training': {\n",
    "                'loss_weights': {\n",
    "                    'bce': 0.4,\n",
    "                    'dice': 0.6\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        loss_fn = get_loss_fn(config)\n",
    "        loss = loss_fn(self.predictions, self.targets)\n",
    "        \n",
    "        self.assertIsInstance(loss.item(), float)\n",
    "\n",
    "class TestMetrics(unittest.TestCase):\n",
    "    \"\"\"测试评估指标\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"设置测试数据\"\"\"\n",
    "        self.predictions = torch.sigmoid(torch.randn(2, 1, 64, 64))\n",
    "        self.targets = torch.randint(0, 2, (2, 1, 64, 64)).float()\n",
    "        \n",
    "    def test_dice_score(self):\n",
    "        \"\"\"测试Dice分数\"\"\"\n",
    "        from metrics import DiceScore\n",
    "        \n",
    "        dice_metric = DiceScore()\n",
    "        dice_metric.update(self.predictions, self.targets)\n",
    "        score = dice_metric.compute()\n",
    "        \n",
    "        self.assertTrue(0 <= score <= 1)\n",
    "        \n",
    "    def test_metrics_calculator(self):\n",
    "        \"\"\"测试指标计算器\"\"\"\n",
    "        from metrics import MetricsCalculator\n",
    "        \n",
    "        calculator = MetricsCalculator()\n",
    "        calculator.update(self.predictions, self.targets)\n",
    "        results = calculator.compute()\n",
    "        \n",
    "        # 检查返回的指标\n",
    "        expected_metrics = ['dice', 'iou', 'pixel_accuracy', 'precision', 'recall']\n",
    "        for metric in expected_metrics:\n",
    "            self.assertIn(metric, results)\n",
    "            self.assertTrue(0 <= results[metric] <= 1)\n",
    "\n",
    "class TestTrainingEngine(unittest.TestCase):\n",
    "    \"\"\"测试训练引擎\"\"\"\n",
    "    \n",
    "    def test_early_stopping(self):\n",
    "        \"\"\"测试早停机制\"\"\"\n",
    "        from engine import EarlyStopping\n",
    "        \n",
    "        early_stopping = EarlyStopping(patience=2, min_delta=0.01)\n",
    "        \n",
    "        # 模拟训练过程\n",
    "        model = torch.nn.Linear(10, 1)\n",
    "        scores = [1.0, 0.8, 0.79, 0.785, 0.784]\n",
    "        \n",
    "        should_stop = False\n",
    "        for score in scores:\n",
    "            should_stop = early_stopping(score, model)\n",
    "            if should_stop:\n",
    "                break\n",
    "                \n",
    "        self.assertTrue(should_stop)  # 应该在最后触发早停\n",
    "        \n",
    "    def test_checkpoint_manager(self):\n",
    "        \"\"\"测试检查点管理器\"\"\"\n",
    "        from engine import CheckpointManager\n",
    "        import tempfile\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            checkpoint_manager = CheckpointManager(temp_dir)\n",
    "            \n",
    "            model = torch.nn.Linear(10, 1)\n",
    "            optimizer = torch.optim.Adam(model.parameters())\n",
    "            \n",
    "            # 保存检查点\n",
    "            checkpoint_manager.save_checkpoint(\n",
    "                epoch=0,\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                metrics={'loss': 0.5},\n",
    "                is_best=True\n",
    "            )\n",
    "            \n",
    "            # 检查文件是否存在\n",
    "            best_path = checkpoint_manager.get_best_checkpoint_path()\n",
    "            self.assertIsNotNone(best_path)\n",
    "            self.assertTrue(Path(best_path).exists())\n",
    "\n",
    "# ==============================================================================\n",
    "# utils/common.py\n",
    "# 通用工具函数\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    设置随机种子以确保结果可复现\n",
    "    \n",
    "    Args:\n",
    "        seed: 随机种子值\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # 设置确定性算法\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # 设置环境变量\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def setup_logging(log_level: str = 'INFO', \n",
    "                 log_file: Optional[str] = None,\n",
    "                 debug: bool = False):\n",
    "    \"\"\"\n",
    "    设置日志系统\n",
    "    \n",
    "    Args:\n",
    "        log_level: 日志级别\n",
    "        log_file: 日志文件路径\n",
    "        debug: 是否为调试模式\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        log_level = 'DEBUG'\n",
    "        \n",
    "    # 配置日志格式\n",
    "    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    date_format = '%Y-%m-%d %H:%M:%S'\n",
    "    \n",
    "    # 基础配置\n",
    "    logging.basicConfig(\n",
    "        level=getattr(logging, log_level.upper()),\n",
    "        format=log_format,\n",
    "        datefmt=date_format,\n",
    "        handlers=[]\n",
    "    )\n",
    "    \n",
    "    # 控制台输出\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(logging.Formatter(log_format, date_format))\n",
    "    logging.getLogger().addHandler(console_handler)\n",
    "    \n",
    "    # 文件输出\n",
    "    if log_file:\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setFormatter(logging.Formatter(log_format, date_format))\n",
    "        logging.getLogger().addHandler(file_handler)\n",
    "\n",
    "def get_device(gpu_id: Optional[int] = None) -> torch.device:\n",
    "    \"\"\"\n",
    "    获取计算设备\n",
    "    \n",
    "    Args:\n",
    "        gpu_id: GPU ID，None时自动选择\n",
    "        \n",
    "    Returns:\n",
    "        torch设备对象\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        if gpu_id is not None:\n",
    "            if gpu_id >= torch.cuda.device_count():\n",
    "                raise ValueError(f\"GPU {gpu_id} 不存在，可用GPU数量: {torch.cuda.device_count()}\")\n",
    "            device = torch.device(f'cuda:{gpu_id}')\n",
    "        else:\n",
    "            device = torch.device('cuda')\n",
    "        \n",
    "        # 打印GPU信息\n",
    "        gpu_name = torch.cuda.get_device_name(device)\n",
    "        gpu_memory = torch.cuda.get_device_properties(device).total_memory / 1024**3\n",
    "        logging.info(f\"使用GPU: {gpu_name} ({gpu_memory:.1f}GB)\")\n",
    "        \n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        logging.info(\"使用CPU进行计算\")\n",
    "        \n",
    "    return device\n",
    "\n",
    "def count_parameters(model: torch.nn.Module) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    统计模型参数数量\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch模型\n",
    "        \n",
    "    Returns:\n",
    "        (总参数数量, 可训练参数数量)\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    return total_params, trainable_params\n",
    "\n",
    "class ModelSummary:\n",
    "    \"\"\"模型摘要工具\"\"\"\n",
    "    \n",
    "    def __init__(self, model: torch.nn.Module):\n",
    "        self.model = model\n",
    "        \n",
    "    def print_summary(self, input_size: Tuple[int, ...], device: str = 'cpu'):\n",
    "        \"\"\"\n",
    "        打印模型摘要\n",
    "        \n",
    "        Args:\n",
    "            input_size: 输入尺寸 (C, H, W)\n",
    "            device: 设备\n",
    "        \"\"\"\n",
    "        model = self.model.to(device)\n",
    "        \n",
    "        # 统计参数\n",
    "        total_params, trainable_params = count_parameters(model)\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(f\"模型摘要: {model.__class__.__name__}\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"输入尺寸: {input_size}\")\n",
    "        print(f\"总参数数量: {total_params:,}\")\n",
    "        print(f\"可训练参数: {trainable_params:,}\")\n",
    "        print(f\"模型大小: {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
    "        \n",
    "        # 计算模型输出尺寸\n",
    "        try:\n",
    "            dummy_input = torch.randn(1, *input_size).to(device)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = model(dummy_input)\n",
    "            print(f\"输出尺寸: {tuple(output.shape[1:])}\")\n",
    "        except Exception as e:\n",
    "            print(f\"无法计算输出尺寸: {e}\")\n",
    "            \n",
    "        print(\"=\" * 80)\n",
    "\n",
    "def save_plot(fig, filepath: Union[str, Path], dpi: int = 300):\n",
    "    \"\"\"\n",
    "    保存图像\n",
    "    \n",
    "    Args:\n",
    "        fig: matplotlib图像对象\n",
    "        filepath: 保存路径\n",
    "        dpi: 分辨率\n",
    "    \"\"\"\n",
    "    filepath = Path(filepath)\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    fig.savefig(filepath, dpi=dpi, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "def format_time(seconds: float) -> str:\n",
    "    \"\"\"\n",
    "    格式化时间\n",
    "    \n",
    "    Args:\n",
    "        seconds: 秒数\n",
    "        \n",
    "    Returns:\n",
    "        格式化的时间字符串\n",
    "    \"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.1f}s\"\n",
    "    elif seconds < 3600:\n",
    "        minutes = seconds // 60\n",
    "        seconds = seconds % 60\n",
    "        return f\"{int(minutes)}m {seconds:.1f}s\"\n",
    "    else:\n",
    "        hours = seconds // 3600\n",
    "        minutes = (seconds % 3600) // 60\n",
    "        seconds = seconds % 60\n",
    "        return f\"{int(hours)}h {int(minutes)}m {seconds:.1f}s\"\n",
    "\n",
    "# ==============================================================================\n",
    "# utils/__init__.py\n",
    "\n",
    "from .common import (\n",
    "    set_seed,\n",
    "    setup_logging,\n",
    "    get_device,\n",
    "    count_parameters,\n",
    "    ModelSummary,\n",
    "    save_plot,\n",
    "    format_time\n",
    ")\n",
    "\n",
    "__all__ = [\n",
    "    'set_seed',\n",
    "    'setup_logging', \n",
    "    'get_device',\n",
    "    'count_parameters',\n",
    "    'ModelSummary',\n",
    "    'save_plot',\n",
    "    'format_time'\n",
    "]\n",
    "\n",
    "# ==============================================================================\n",
    "# 运行测试脚本\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 运行单元测试\n",
    "    import unittest\n",
    "    \n",
    "    # 创建测试套件\n",
    "    test_loader = unittest.TestLoader()\n",
    "    test_suite = unittest.TestSuite()\n",
    "    \n",
    "    # 添加测试类\n",
    "    test_classes = [\n",
    "        TestChestXrayDataset,\n",
    "        TestLossFunctions,\n",
    "        TestMetrics,\n",
    "        TestTrainingEngine\n",
    "    ]\n",
    "    \n",
    "    for test_class in test_classes:\n",
    "        tests = test_loader.loadTestsFromTestCase(test_class)\n",
    "        test_suite.addTests(tests)\n",
    "        \n",
    "    # 运行测试\n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    result = runner.run(test_suite)\n",
    "    \n",
    "    # 输出结果\n",
    "    if result.wasSuccessful():\n",
    "        print(\"\\n✅ 所有测试通过!\")\n",
    "    else:\n",
    "        print(f\"\\n❌ {len(result.failures)} 个测试失败, {len(result.errors)} 个错误\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea3c4aa-0d48-4b5a-851c-5a01ee23caf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720cc9a4-0b91-4799-ab8f-e57184f2e972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e2466-5ab8-4a57-87f0-c53d4f0fed89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d2bfa-128c-4acc-8622-de8ff3c9b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scripts/visualize_enhanced.py\n",
    "# 增强的可视化脚本，支持多种可视化模式\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "sys.path.append(str(Path(__file__).parent.parent))\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import cv2\n",
    "from configs import ConfigLoader\n",
    "from datasets import ChestXrayDataset, DataLoaderFactory\n",
    "from transforms import get_transforms\n",
    "from engine import InferenceEngine\n",
    "from metrics import evaluate_predictions\n",
    "from utils import setup_logging, get_device\n",
    "import seaborn as sns\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"解析命令行参数\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"增强的肺部分割结果可视化\")\n",
    "    \n",
    "    parser.add_argument('--config', type=str, default='unet',\n",
    "                       help='配置文件名称')\n",
    "    parser.add_argument('--checkpoint', type=str, required=True,\n",
    "                       help='模型检查点路径')\n",
    "    parser.add_argument('--model', type=str, default='unet',\n",
    "                       help='模型类型')\n",
    "    parser.add_argument('--split', type=str, default='test',\n",
    "                       choices=['train', 'val', 'test'],\n",
    "                       help='可视化的数据分割')\n",
    "    parser.add_argument('--num-samples', type=int, default=8,\n",
    "                       help='可视化样本数量')\n",
    "    parser.add_argument('--output-dir', type=str, default=None,\n",
    "                       help='输出目录')\n",
    "    parser.add_argument('--mode', type=str, default='comprehensive',\n",
    "                       choices=['basic', 'comprehensive', 'error_analysis', 'feature_maps'],\n",
    "                       help='可视化模式')\n",
    "    parser.add_argument('--threshold', type=float, default=0.5,\n",
    "                       help='二值化阈值')\n",
    "    parser.add_argument('--save-individual', action='store_true',\n",
    "                       help='是否保存单独的图像文件')\n",
    "    parser.add_argument('--gpu', type=int, default=None,\n",
    "                       help='指定GPU ID')\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "class EnhancedVisualizer:\n",
    "    \"\"\"增强的可视化器\"\"\"\n",
    "    \n",
    "    def __init__(self, config, model, device, output_dir):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 设置绘图样式\n",
    "        plt.style.use('default')\n",
    "        sns.set_palette(\"husl\")\n",
    "        \n",
    "    def visualize_basic_predictions(self, images, masks, predictions, image_ids, threshold=0.5):\n",
    "        \"\"\"基础预测可视化\"\"\"\n",
    "        num_samples = len(images)\n",
    "        fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
    "        \n",
    "        if num_samples == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "            \n",
    "        for i in range(num_samples):\n",
    "            image = images[i].squeeze().numpy()\n",
    "            mask = masks[i].squeeze().numpy()\n",
    "            pred_prob = predictions[i].squeeze().numpy()\n",
    "            pred_binary = (pred_prob > threshold).astype(np.float32)\n",
    "            image_id = image_ids[i] if i < len(image_ids) else f\"sample_{i}\"\n",
    "            \n",
    "            # 计算指标\n",
    "            dice = self._calculate_dice(pred_binary, mask)\n",
    "            iou = self._calculate_iou(pred_binary, mask)\n",
    "            \n",
    "            # 原图\n",
    "            axes[i, 0].imshow(image, cmap='gray')\n",
    "            axes[i, 0].set_title(f'Original\\n{image_id}')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            # 真实掩码\n",
    "            axes[i, 1].imshow(mask, cmap='Reds', alpha=0.8)\n",
    "            axes[i, 1].set_title('Ground Truth')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            # 预测概率\n",
    "            im = axes[i, 2].imshow(pred_prob, cmap='Blues', vmin=0, vmax=1)\n",
    "            axes[i, 2].set_title(f'Prediction Prob\\n(Max: {pred_prob.max():.3f})')\n",
    "            axes[i, 2].axis('off')\n",
    "            plt.colorbar(im, ax=axes[i, 2], fraction=0.046, pad=0.04)\n",
    "            \n",
    "            # 叠加显示\n",
    "            overlay = self._create_overlay(image, mask, pred_binary)\n",
    "            axes[i, 3].imshow(overlay)\n",
    "            axes[i, 3].set_title(f'Overlay\\nDice: {dice:.3f}, IoU: {iou:.3f}')\n",
    "            axes[i, 3].axis('off')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 保存图像\n",
    "        save_path = self.output_dir / 'basic_predictions.png'\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return save_path\n",
    "        \n",
    "    def visualize_comprehensive_analysis(self, images, masks, predictions, image_ids, threshold=0.5):\n",
    "        \"\"\"综合分析可视化\"\"\"\n",
    "        num_samples = min(len(images), 6)  # 限制样本数量以保持清晰度\n",
    "        \n",
    "        fig = plt.figure(figsize=(20, 4*num_samples))\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            image = images[i].squeeze().numpy()\n",
    "            mask = masks[i].squeeze().numpy()\n",
    "            pred_prob = predictions[i].squeeze().numpy()\n",
    "            pred_binary = (pred_prob > threshold).astype(np.float32)\n",
    "            image_id = image_ids[i] if i < len(image_ids) else f\"sample_{i}\"\n",
    "            \n",
    "            # 计算详细指标\n",
    "            metrics = self._calculate_detailed_metrics(pred_binary, mask)\n",
    "            \n",
    "            # 创建子图\n",
    "            base_idx = i * 6 + 1\n",
    "            \n",
    "            # 1. 原图 + 轮廓\n",
    "            ax1 = plt.subplot(num_samples, 6, base_idx)\n",
    "            ax1.imshow(image, cmap='gray')\n",
    "            \n",
    "            # 添加轮廓\n",
    "            gt_contours = self._get_contours(mask)\n",
    "            pred_contours = self._get_contours(pred_binary)\n",
    "            \n",
    "            for contour in gt_contours:\n",
    "                ax1.plot(contour[:, 0], contour[:, 1], 'g-', linewidth=2, label='GT' if contour is gt_contours[0] else \"\")\n",
    "            for contour in pred_contours:\n",
    "                ax1.plot(contour[:, 0], contour[:, 1], 'r--', linewidth=2, label='Pred' if contour is pred_contours[0] else \"\")\n",
    "                \n",
    "            ax1.set_title(f'Contours\\n{image_id}')\n",
    "            ax1.axis('off')\n",
    "            if i == 0:\n",
    "                ax1.legend()\n",
    "            \n",
    "            # 2. 错误分析\n",
    "            ax2 = plt.subplot(num_samples, 6, base_idx + 1)\n",
    "            error_map = self._create_error_map(pred_binary, mask)\n",
    "            ax2.imshow(error_map)\n",
    "            ax2.set_title('Error Analysis\\nTP(W) FP(R) FN(B)')\n",
    "            ax2.axis('off')\n",
    "            \n",
    "            # 3. 概率分布\n",
    "            ax3 = plt.subplot(num_samples, 6, base_idx + 2)\n",
    "            mask_probs = pred_prob[mask > 0.5]\n",
    "            bg_probs = pred_prob[mask <= 0.5]\n",
    "            \n",
    "            ax3.hist(bg_probs.flatten(), bins=50, alpha=0.7, label='Background', density=True)\n",
    "            ax3.hist(mask_probs.flatten(), bins=50, alpha=0.7, label='Lung', density=True)\n",
    "            ax3.axvline(threshold, color='red', linestyle='--', label=f'Threshold={threshold}')\n",
    "            ax3.set_title('Probability Distribution')\n",
    "            ax3.set_xlabel('Probability')\n",
    "            ax3.legend()\n",
    "            \n",
    "            # 4. 距离变换\n",
    "            ax4 = plt.subplot(num_samples, 6, base_idx + 3)\n",
    "            distance_map = self._compute_distance_transform(mask, pred_binary)\n",
    "            im = ax4.imshow(distance_map, cmap='RdBu_r')\n",
    "            ax4.set_title('Distance Error')\n",
    "            ax4.axis('off')\n",
    "            plt.colorbar(im, ax=ax4, fraction=0.046, pad=0.04)\n",
    "            \n",
    "            # 5. 指标雷达图\n",
    "            ax5 = plt.subplot(num_samples, 6, base_idx + 4, projection='polar')\n",
    "            self._plot_metrics_radar(ax5, metrics)\n",
    "            ax5.set_title('Metrics Radar')\n",
    "            \n",
    "            # 6. 详细信息\n",
    "            ax6 = plt.subplot(num_samples, 6, base_idx + 5)\n",
    "            ax6.axis('off')\n",
    "            \n",
    "            info_text = f\"\"\"\n",
    "            Sample: {image_id}\n",
    "            \n",
    "            Dice: {metrics['dice']:.3f}\n",
    "            IoU: {metrics['iou']:.3f}\n",
    "            Precision: {metrics['precision']:.3f}\n",
    "            Recall: {metrics['recall']:.3f}\n",
    "            Specificity: {metrics['specificity']:.3f}\n",
    "            \n",
    "            Pred Area: {pred_binary.sum():.0f}\n",
    "            GT Area: {mask.sum():.0f}\n",
    "            Area Ratio: {pred_binary.sum()/mask.sum():.2f}\n",
    "            \"\"\"\n",
    "            \n",
    "            ax6.text(0.1, 0.9, info_text, transform=ax6.transAxes, \n",
    "                    verticalalignment='top', fontsize=10, fontfamily='monospace')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 保存图像\n",
    "        save_path = self.output_dir / 'comprehensive_analysis.png'\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return save_path\n",
    "        \n",
    "    def visualize_error_analysis(self, images, masks, predictions, image_ids, threshold=0.5):\n",
    "        \"\"\"错误分析可视化\"\"\"\n",
    "        # 计算所有样本的指标\n",
    "        all_metrics = []\n",
    "        for i in range(len(images)):\n",
    "            pred_binary = (predictions[i].squeeze().numpy() > threshold).astype(np.float32)\n",
    "            mask = masks[i].squeeze().numpy()\n",
    "            metrics = self._calculate_detailed_metrics(pred_binary, mask)\n",
    "            metrics['image_id'] = image_ids[i] if i < len(image_ids) else f\"sample_{i}\"\n",
    "            all_metrics.append(metrics)\n",
    "            \n",
    "        # 按Dice分数排序\n",
    "        all_metrics.sort(key=lambda x: x['dice'])\n",
    "        \n",
    "        # 选择最好、中等、最差的样本\n",
    "        indices = [0, len(all_metrics)//2, -1]\n",
    "        selected_metrics = [all_metrics[i] for i in indices]\n",
    "        selected_images = [images[self._find_image_index(m['image_id'], image_ids)] for m in selected_metrics]\n",
    "        selected_masks = [masks[self._find_image_index(m['image_id'], image_ids)] for m in selected_metrics]\n",
    "        selected_preds = [predictions[self._find_image_index(m['image_id'], image_ids)] for m in selected_metrics]\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 5, figsize=(20, 12))\n",
    "        titles = ['Worst', 'Medium', 'Best']\n",
    "        \n",
    "        for i, (img, mask, pred, metrics) in enumerate(zip(selected_images, selected_masks, selected_preds, selected_metrics)):\n",
    "            image = img.squeeze().numpy()\n",
    "            mask = mask.squeeze().numpy()\n",
    "            pred_prob = pred.squeeze().numpy()\n",
    "            pred_binary = (pred_prob > threshold).astype(np.float32)\n",
    "            \n",
    "            # 原图\n",
    "            axes[i, 0].imshow(image, cmap='gray')\n",
    "            axes[i, 0].set_title(f'{titles[i]}\\n{metrics[\"image_id\"]}')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            # 真实掩码\n",
    "            axes[i, 1].imshow(mask, cmap='Reds', alpha=0.8)\n",
    "            axes[i, 1].set_title('Ground Truth')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            # 预测掩码\n",
    "            axes[i, 2].imshow(pred_binary, cmap='Blues', alpha=0.8)\n",
    "            axes[i, 2].set_title('Prediction')\n",
    "            axes[i, 2].axis('off')\n",
    "            \n",
    "            # 错误分析\n",
    "            error_map = self._create_error_map(pred_binary, mask)\n",
    "            axes[i, 3].imshow(error_map)\n",
    "            axes[i, 3].set_title('Error Map')\n",
    "            axes[i, 3].axis('off')\n",
    "            \n",
    "            # 指标条形图\n",
    "            metric_names = ['Dice', 'IoU', 'Precision', 'Recall']\n",
    "            metric_values = [metrics[name.lower()] for name in metric_names]\n",
    "            \n",
    "            bars = axes[i, 4].bar(metric_names, metric_values, \n",
    "                                color=['red' if v < 0.5 else 'orange' if v < 0.7 else 'green' for v in metric_values])\n",
    "            axes[i, 4].set_ylim(0, 1)\n",
    "            axes[i, 4].set_title('Metrics')\n",
    "            axes[i, 4].tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # 在条形图上添加数值\n",
    "            for bar, value in zip(bars, metric_values):\n",
    "                axes[i, 4].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                               f'{value:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "                               \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 保存图像\n",
    "        save_path = self.output_dir / 'error_analysis.png'\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # 生成统计报告\n",
    "        self._generate_statistics_report(all_metrics)\n",
    "        \n",
    "        return save_path\n",
    "        \n",
    "    def visualize_feature_maps(self, images, model_with_hooks, image_ids):\n",
    "        \"\"\"特征图可视化（仅适用于支持的模型）\"\"\"\n",
    "        if not hasattr(model_with_hooks, 'get_feature_maps'):\n",
    "            logging.warning(\"模型不支持特征图提取\")\n",
    "            return None\n",
    "            \n",
    "        fig, axes = plt.subplots(2, 6, figsize=(24, 8))\n",
    "        \n",
    "        # 选择第一个样本\n",
    "        sample_image = images[0:1]  # 保持batch维度\n",
    "        image_id = image_ids[0] if len(image_ids) > 0 else \"sample_0\"\n",
    "        \n",
    "        # 获取特征图\n",
    "        with torch.no_grad():\n",
    "            features = model_with_hooks.get_feature_maps(sample_image)\n",
    "            \n",
    "        # 显示原图\n",
    "        axes[0, 0].imshow(sample_image[0, 0].cpu().numpy(), cmap='gray')\n",
    "        axes[0, 0].set_title(f'Input\\n{image_id}')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # 显示编码器特征\n",
    "        encoder_features = ['enc1', 'enc2', 'enc3', 'enc4', 'bottleneck']\n",
    "        for i, feat_name in enumerate(encoder_features):\n",
    "            if i < 5:  # 前5个位置\n",
    "                feat_map = features[feat_name][0, 0].cpu().numpy()  # 取第一个通道\n",
    "                axes[0, i+1].imshow(feat_map, cmap='viridis')\n",
    "                axes[0, i+1].set_title(f'{feat_name.upper()}\\n{feat_map.shape}')\n",
    "                axes[0, i+1].axis('off')\n",
    "                \n",
    "        # 显示解码器特征\n",
    "        decoder_features = ['dec1', 'dec2', 'dec3', 'dec4', 'output']\n",
    "        for i, feat_name in enumerate(decoder_features):\n",
    "            feat_map = features[feat_name][0, 0].cpu().numpy()\n",
    "            axes[1, i].imshow(feat_map, cmap='plasma')\n",
    "            axes[1, i].set_title(f'{feat_name.upper()}\\n{feat_map.shape}')\n",
    "            axes[1, i].axis('off')\n",
    "            \n",
    "        # 最后一个位置显示激活统计\n",
    "        axes[1, 5].axis('off')\n",
    "        \n",
    "        # 计算激活统计\n",
    "        stats_text = \"Activation Statistics:\\n\\n\"\n",
    "        for name, feat in features.items():\n",
    "            mean_act = feat.mean().item()\n",
    "            std_act = feat.std().item()\n",
    "            max_act = feat.max().item()\n",
    "            stats_text += f\"{name}:\\n  Mean: {mean_act:.3f}\\n  Std: {std_act:.3f}\\n  Max: {max_act:.3f}\\n\\n\"\n",
    "            \n",
    "        axes[1, 5].text(0.1, 0.9, stats_text, transform=axes[1, 5].transAxes,\n",
    "                       verticalalignment='top', fontsize=8, fontfamily='monospace')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 保存图像\n",
    "        save_path = self.output_dir / 'feature_maps.png'\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return save_path\n",
    "        \n",
    "    def _calculate_dice(self, pred, target):\n",
    "        \"\"\"计算Dice系数\"\"\"\n",
    "        smooth = 1e-6\n",
    "        intersection = (pred * target).sum()\n",
    "        return (2 * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "        \n",
    "    def _calculate_iou(self, pred, target):\n",
    "        \"\"\"计算IoU\"\"\"\n",
    "        smooth = 1e-6\n",
    "        intersection = (pred * target).sum()\n",
    "        union = pred.sum() + target.sum() - intersection\n",
    "        return (intersection + smooth) / (union + smooth)\n",
    "        \n",
    "    def _calculate_detailed_metrics(self, pred, target):\n",
    "        \"\"\"计算详细指标\"\"\"\n",
    "        smooth = 1e-6\n",
    "        \n",
    "        # 基础指标\n",
    "        tp = (pred * target).sum()\n",
    "        fp = (pred * (1 - target)).sum()\n",
    "        fn = ((1 - pred) * target).sum()\n",
    "        tn = ((1 - pred) * (1 - target)).sum()\n",
    "        \n",
    "        dice = (2 * tp + smooth) / (2 * tp + fp + fn + smooth)\n",
    "        iou = (tp + smooth) / (tp + fp + fn + smooth)\n",
    "        precision = (tp + smooth) / (tp + fp + smooth)\n",
    "        recall = (tp + smooth) / (tp + fn + smooth)\n",
    "        specificity = (tn + smooth) / (tn + fp + smooth)\n",
    "        \n",
    "        return {\n",
    "            'dice': dice,\n",
    "            'iou': iou,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'specificity': specificity\n",
    "        }\n",
    "        \n",
    "    def _create_overlay(self, image, mask, prediction):\n",
    "        \"\"\"创建叠加图像\"\"\"\n",
    "        overlay = np.stack([image, image, image], axis=-1)\n",
    "        \n",
    "        # 绿色显示真实掩码，红色显示预测掩码\n",
    "        overlay[:, :, 1] = np.maximum(overlay[:, :, 1], mask * 0.6)  # 绿色\n",
    "        overlay[:, :, 0] = np.maximum(overlay[:, :, 0], prediction * 0.6)  # 红色\n",
    "        \n",
    "        # 黄色显示重叠区域\n",
    "        overlap = mask * prediction\n",
    "        overlay[:, :, 0] = np.maximum(overlay[:, :, 0], overlap * 0.8)\n",
    "        overlay[:, :, 1] = np.maximum(overlay[:, :, 1], overlap * 0.8)\n",
    "        \n",
    "        return overlay\n",
    "        \n",
    "    def _create_error_map(self, pred, target):\n",
    "        \"\"\"创建错误映射图\"\"\"\n",
    "        # True Positive: 白色, False Positive: 红色, False Negative: 蓝色, True Negative: 黑色\n",
    "        tp = pred * target\n",
    "        fp = pred * (1 - target)\n",
    "        fn = (1 - pred) * target\n",
    "        tn = (1 - pred) * (1 - target)\n",
    "        \n",
    "        error_map = np.zeros((*pred.shape, 3))\n",
    "        error_map[:, :, 0] = fp  # 红色分量: False Positive\n",
    "        error_map[:, :, 1] = tp  # 绿色分量: True Positive  \n",
    "        error_map[:, :, 2] = fn  # 蓝色分量: False Negative\n",
    "        \n",
    "        return error_map\n",
    "        \n",
    "    def _get_contours(self, mask):\n",
    "        \"\"\"获取轮廓\"\"\"\n",
    "        contours, _ = cv2.findContours((mask > 0.5).astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        return [contour.squeeze() for contour in contours if len(contour) > 3]\n",
    "        \n",
    "    def _compute_distance_transform(self, mask, pred):\n",
    "        \"\"\"计算距离变换误差\"\"\"\n",
    "        from scipy.ndimage import distance_transform_edt\n",
    "        \n",
    "        # 计算到边界的距离\n",
    "        mask_dt = distance_transform_edt(mask > 0.5)\n",
    "        pred_dt = distance_transform_edt(pred > 0.5)\n",
    "        \n",
    "        # 计算距离误差\n",
    "        distance_error = mask_dt - pred_dt\n",
    "        \n",
    "        return distance_error\n",
    "        \n",
    "    def _plot_metrics_radar(self, ax, metrics):\n",
    "        \"\"\"绘制指标雷达图\"\"\"\n",
    "        metric_names = ['Dice', 'IoU', 'Precision', 'Recall', 'Specificity']\n",
    "        metric_values = [metrics[name.lower()] for name in metric_names]\n",
    "        \n",
    "        # 添加第一个点到末尾以闭合雷达图\n",
    "        metric_values += metric_values[:1]\n",
    "        \n",
    "        # 计算角度\n",
    "        angles = np.linspace(0, 2 * np.pi, len(metric_names), endpoint=False).tolist()\n",
    "        angles += angles[:1]\n",
    "        \n",
    "        # 绘制雷达图\n",
    "        ax.plot(angles, metric_values, 'o-', linewidth=2)\n",
    "        ax.fill(angles, metric_values, alpha=0.25)\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(metric_names)\n",
    "        ax.set_ylim(0, 1)\n",
    "        \n",
    "    def _find_image_index(self, image_id, image_ids):\n",
    "        \"\"\"找到图像ID对应的索引\"\"\"\n",
    "        try:\n",
    "            return image_ids.index(image_id)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "            \n",
    "    def _generate_statistics_report(self, all_metrics):\n",
    "        \"\"\"生成统计报告\"\"\"\n",
    "        report_path = self.output_dir / 'statistics_report.json'\n",
    "        \n",
    "        # 计算统计信息\n",
    "        dice_scores = [m['dice'] for m in all_metrics]\n",
    "        iou_scores = [m['iou'] for m in all_metrics]\n",
    "        \n",
    "        stats = {\n",
    "            'total_samples': len(all_metrics),\n",
    "            'dice_statistics': {\n",
    "                'mean': np.mean(dice_scores),\n",
    "                'std': np.std(dice_scores),\n",
    "                'min': np.min(dice_scores),\n",
    "                'max': np.max(dice_scores),\n",
    "                'median': np.median(dice_scores),\n",
    "                'q25': np.percentile(dice_scores, 25),\n",
    "                'q75': np.percentile(dice_scores, 75)\n",
    "            },\n",
    "            'iou_statistics': {\n",
    "                'mean': np.mean(iou_scores),\n",
    "                'std': np.std(iou_scores),\n",
    "                'min': np.min(iou_scores),\n",
    "                'max': np.max(iou_scores),\n",
    "                'median': np.median(iou_scores),\n",
    "                'q25': np.percentile(iou_scores, 25),\n",
    "                'q75': np.percentile(iou_scores, 75)\n",
    "            },\n",
    "            'performance_distribution': {\n",
    "                'excellent_samples': len([d for d in dice_scores if d > 0.9]),\n",
    "                'good_samples': len([d for d in dice_scores if 0.7 < d <= 0.9]),\n",
    "                'fair_samples': len([d for d in dice_scores if 0.5 < d <= 0.7]),\n",
    "                'poor_samples': len([d for d in dice_scores if d <= 0.5])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(report_path, 'w') as f:\n",
    "            json.dump(stats, f, indent=2)\n",
    "            \n",
    "        logging.info(f\"统计报告保存至: {report_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    args = parse_args()\n",
    "    \n",
    "    # 设置日志\n",
    "    setup_logging()\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    try:\n",
    "        # 加载配置\n",
    "        config_loader = ConfigLoader()\n",
    "        config = config_loader.load_config(args.config)\n",
    "        config['model']['name'] = args.model\n",
    "        \n",
    "        # 获取设备\n",
    "        device = get_device(args.gpu)\n",
    "        \n",
    "        # 创建模型和推理引擎\n",
    "        from scripts.train import create_model\n",
    "        model = create_model(config['model'], device)\n",
    "        inference_engine = InferenceEngine(model, device, config)\n",
    "        inference_engine.load_checkpoint(args.checkpoint)\n",
    "        \n",
    "        # 创建数据集\n",
    "        transform = get_transforms(args.split, config)\n",
    "        dataset = ChestXrayDataset(config, split=args.split, transform=transform)\n",
    "        \n",
    "        factory = DataLoaderFactory(config)\n",
    "        dataloader = factory.create_dataloader(dataset, shuffle=False, batch_size=args.num_samples)\n",
    "        \n",
    "        # 获取数据进行可视化\n",
    "        batch = next(iter(dataloader))\n",
    "        \n",
    "        # 进行预测\n",
    "        results = inference_engine.predict_batch(batch, threshold=args.threshold)\n",
    "        \n",
    "        # 设置输出目录\n",
    "        if args.output_dir:\n",
    "            output_dir = args.output_dir\n",
    "        else:\n",
    "            output_dir = Path(config['logging']['vis_dir']) / args.model\n",
    "            \n",
    "        # 创建可视化器\n",
    "        visualizer = EnhancedVisualizer(config, model, device, output_dir)\n",
    "        \n",
    "        # 根据模式进行可视化\n",
    "        logger.info(f\"开始 {args.mode} 可视化...\")\n",
    "        \n",
    "        if args.mode == 'basic':\n",
    "            save_path = visualizer.visualize_basic_predictions(\n",
    "                batch['image'], batch['mask'], results['probabilities'],\n",
    "                batch.get('image_id', []), args.threshold\n",
    "            )\n",
    "        elif args.mode == 'comprehensive':\n",
    "            save_path = visualizer.visualize_comprehensive_analysis(\n",
    "                batch['image'], batch['mask'], results['probabilities'],\n",
    "                batch.get('image_id', []), args.threshold\n",
    "            )\n",
    "        elif args.mode == 'error_analysis':\n",
    "            save_path = visualizer.visualize_error_analysis(\n",
    "                batch['image'], batch['mask'], results['probabilities'],\n",
    "                batch.get('image_id', []), args.threshold\n",
    "            )\n",
    "        elif args.mode == 'feature_maps':\n",
    "            save_path = visualizer.visualize_feature_maps(\n",
    "                batch['image'], model, batch.get('image_id', [])\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"未知的可视化模式: {args.mode}\")\n",
    "            \n",
    "        logger.info(f\"✅ {args.mode} 可视化完成!\")\n",
    "        if save_path:\n",
    "            logger.info(f\"结果保存至: {save_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ 可视化失败: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de504a-a237-4d81-b28b-dc6d523fcf29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9dea09-4227-4739-90ab-aa99bfe0c164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af95f1c2-11ed-4fa6-a070-b3dbecfdafb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab89716-0d03-4368-a242-1334493c603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/unet_baseline_experiment.ipynb\n",
    "# U-Net基线实验记录和分析\n",
    "\n",
    "\"\"\"\n",
    "# U-Net Baseline Experiment\n",
    "肺部分割基线实验 - 完整的训练、评估和分析流程\n",
    "\n",
    "## 实验目标\n",
    "1. 建立U-Net基线模型性能\n",
    "2. 验证数据预处理和增强策略\n",
    "3. 分析模型行为和失败案例\n",
    "4. 为后续改进提供基准\n",
    "\n",
    "## 实验设置\n",
    "- 模型: 经典U-Net (4层下采样)\n",
    "- 数据: 胸片肺部分割数据集\n",
    "- 损失: BCE + Dice (0.4 + 0.6)\n",
    "- 优化器: Adam (lr=1e-4)\n",
    "- 批次大小: 8\n",
    "- 训练轮次: 100\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 添加项目路径\n",
    "project_root = Path.cwd().parent  # 假设notebook在notebooks/目录下\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# 项目模块\n",
    "from configs import ConfigLoader\n",
    "from models.unet import UNet\n",
    "from datasets import ChestXrayDataset, DataLoaderFactory\n",
    "from transforms import get_transforms\n",
    "from losses import get_loss_fn\n",
    "from metrics import create_metrics_calculator, evaluate_predictions\n",
    "from engine import TrainingEngine, InferenceEngine, EarlyStopping, CheckpointManager\n",
    "from utils import set_seed, setup_logging, get_device, ModelSummary\n",
    "\n",
    "# 设置绘图风格\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"🚀 U-Net基线实验开始\")\n",
    "print(f\"📅 实验时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. 环境设置和配置加载\n",
    "# ============================================================================\n",
    "\n",
    "# 设置随机种子\n",
    "set_seed(42)\n",
    "\n",
    "# 设置日志\n",
    "setup_logging(debug=False)\n",
    "\n",
    "# 加载配置\n",
    "config_loader = ConfigLoader()\n",
    "config = config_loader.load_config('unet')\n",
    "\n",
    "# 获取设备\n",
    "device = get_device()\n",
    "print(f\"🔧 使用设备: {device}\")\n",
    "\n",
    "# 创建输出目录\n",
    "experiment_dir = Path(\"experiments\") / \"unet_baseline\" / datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"📁 实验目录: {experiment_dir}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. 数据分析和可视化\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n📊 数据集分析\")\n",
    "\n",
    "# 创建数据变换\n",
    "transforms = {\n",
    "    'train': get_transforms('train', config),\n",
    "    'val': get_transforms('val', config),\n",
    "    'test': get_transforms('test', config)\n",
    "}\n",
    "\n",
    "# 创建数据集\n",
    "datasets = {}\n",
    "factory = DataLoaderFactory(config)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    try:\n",
    "        dataset = ChestXrayDataset(config, split=split, transform=transforms[split])\n",
    "        datasets[split] = dataset\n",
    "        print(f\"✅ {split}: {len(dataset)} 样本\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {split} 数据集创建失败: {e}\")\n",
    "\n",
    "# 数据分布分析\n",
    "if datasets:\n",
    "    print(\"\\n📈 数据分布分析\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 样本数量分布\n",
    "    splits = list(datasets.keys())\n",
    "    counts = [len(dataset) for dataset in datasets.values()]\n",
    "    \n",
    "    axes[0, 0].bar(splits, counts, color=['blue', 'orange', 'green'])\n",
    "    axes[0, 0].set_title('Dataset Split Distribution')\n",
    "    axes[0, 0].set_ylabel('Number of Samples')\n",
    "    \n",
    "    # 添加数值标签\n",
    "    for i, v in enumerate(counts):\n",
    "        axes[0, 0].text(i, v + max(counts) * 0.01, str(v), ha='center', va='bottom')\n",
    "    \n",
    "    # 肺部像素比例分布\n",
    "    all_lung_ratios = []\n",
    "    for split, dataset in datasets.items():\n",
    "        if hasattr(dataset, 'data') and 'lung_ratio' in dataset.data.columns:\n",
    "            lung_ratios = dataset.data['lung_ratio'].values\n",
    "            axes[0, 1].hist(lung_ratios, bins=30, alpha=0.7, label=split, density=True)\n",
    "            all_lung_ratios.extend(lung_ratios)\n",
    "    \n",
    "    axes[0, 1].set_title('Lung Ratio Distribution')\n",
    "    axes[0, 1].set_xlabel('Lung Pixel Ratio')\n",
    "    axes[0, 1].set_ylabel('Density')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 图像尺寸分布\n",
    "    if datasets['train'].data is not None and 'image_height' in datasets['train'].data.columns:\n",
    "        heights = datasets['train'].data['image_height'].values\n",
    "        widths = datasets['train'].data['image_width'].values\n",
    "        \n",
    "        axes[1, 0].scatter(widths, heights, alpha=0.6, s=20)\n",
    "        axes[1, 0].set_title('Image Size Distribution')\n",
    "        axes[1, 0].set_xlabel('Width')\n",
    "        axes[1, 0].set_ylabel('Height')\n",
    "        \n",
    "        # 添加统计信息\n",
    "        size_stats = f\"Mean: {np.mean(heights):.0f}x{np.mean(widths):.0f}\\n\"\n",
    "        size_stats += f\"Std: {np.std(heights):.0f}x{np.std(widths):.0f}\"\n",
    "        axes[1, 0].text(0.05, 0.95, size_stats, transform=axes[1, 0].transAxes, \n",
    "                       verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 肺部像素比例统计\n",
    "    if all_lung_ratios:\n",
    "        lung_stats = {\n",
    "            'Mean': np.mean(all_lung_ratios),\n",
    "            'Median': np.median(all_lung_ratios),\n",
    "            'Std': np.std(all_lung_ratios),\n",
    "            'Min': np.min(all_lung_ratios),\n",
    "            'Max': np.max(all_lung_ratios)\n",
    "        }\n",
    "        \n",
    "        stats_df = pd.DataFrame(list(lung_stats.items()), columns=['Statistic', 'Value'])\n",
    "        \n",
    "        axes[1, 1].axis('tight')\n",
    "        axes[1, 1].axis('off')\n",
    "        table = axes[1, 1].table(cellText=[[f\"{v:.4f}\"] for v in stats_df['Value']], \n",
    "                                rowLabels=stats_df['Statistic'],\n",
    "                                colLabels=['Lung Ratio'],\n",
    "                                cellLoc='center',\n",
    "                                loc='center')\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        axes[1, 1].set_title('Lung Ratio Statistics')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(experiment_dir / 'data_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# 样本可视化\n",
    "print(\"\\n🖼️ 样本可视化\")\n",
    "\n",
    "# 从训练集中选择几个样本进行可视化\n",
    "train_loader = factory.create_dataloader(datasets['train'], batch_size=4, shuffle=True)\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "for i in range(4):\n",
    "    image = sample_batch['image'][i].squeeze().numpy()\n",
    "    mask = sample_batch['mask'][i].squeeze().numpy()\n",
    "    image_id = sample_batch.get('image_id', [f\"sample_{i}\"] * 4)[i]\n",
    "    \n",
    "    # 原图\n",
    "    axes[0, i].imshow(image, cmap='gray')\n",
    "    axes[0, i].set_title(f'Image: {image_id}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # 掩码叠加\n",
    "    axes[1, i].imshow(image, cmap='gray')\n",
    "    axes[1, i].imshow(mask, cmap='Reds', alpha=0.5)\n",
    "    axes[1, i].set_title(f'Mask Overlay')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(experiment_dir / 'sample_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. 模型创建和分析\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n🏗️ 模型创建和分析\")\n",
    "\n",
    "# 创建模型\n",
    "model = UNet(\n",
    "    in_channels=config['model']['in_channels'],\n",
    "    out_channels=config['model']['out_channels'],\n",
    "    features=config['model']['features'],\n",
    "    dropout_rate=config['model']['dropout_rate'],\n",
    "    bilinear=config['model']['bilinear']\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# 模型摘要\n",
    "model_info = model.get_model_info()\n",
    "print(\"📊 模型信息:\")\n",
    "for key, value in model_info.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# 模型架构可视化\n",
    "summary = ModelSummary(model)\n",
    "summary.print_summary((1, config['data']['image_size'], config['data']['image_size']))\n",
    "\n",
    "# 保存模型信息\n",
    "with open(experiment_dir / 'model_info.json', 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "# ============================================================================\n",
    "# 4. 训练准备\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n⚙️ 训练准备\")\n",
    "\n",
    "# 创建数据加载器\n",
    "dataloaders = factory.create_all_dataloaders(transforms)\n",
    "\n",
    "# 创建损失函数\n",
    "criterion = get_loss_fn(config)\n",
    "print(f\"损失函数: {type(criterion).__name__}\")\n",
    "\n",
    "# 创建优化器\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config['training']['learning_rate'],\n",
    "    weight_decay=config['training']['weight_decay']\n",
    ")\n",
    "\n",
    "# 创建学习率调度器\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    patience=config['training']['scheduler_patience'],\n",
    "    factor=config['training']['scheduler_factor'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 创建指标计算器\n",
    "metrics_calculator = create_metrics_calculator(config)\n",
    "\n",
    "# 创建早停机制\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=config['training']['early_stopping']['patience'],\n",
    "    min_delta=config['training']['early_stopping']['min_delta'],\n",
    "    restore_best_weights=config['training']['early_stopping']['restore_best_weights']\n",
    ")\n",
    "\n",
    "# 创建检查点管理器\n",
    "checkpoint_dir = experiment_dir / 'checkpoints'\n",
    "checkpoint_manager = CheckpointManager(\n",
    "    checkpoint_dir=str(checkpoint_dir),\n",
    "    save_best_only=config['logging']['save_best_only'],\n",
    "    save_every_n_epochs=config['logging']['save_every_n_epochs']\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. 模型训练\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n🚀 开始训练\")\n",
    "\n",
    "# 创建训练引擎\n",
    "trainer = TrainingEngine(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    config=config,\n",
    "    scheduler=scheduler,\n",
    "    metrics_calculator=metrics_calculator,\n",
    "    early_stopping=early_stopping,\n",
    "    checkpoint_manager=checkpoint_manager\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "training_start_time = datetime.now()\n",
    "print(f\"训练开始时间: {training_start_time}\")\n",
    "\n",
    "try:\n",
    "    history = trainer.fit(\n",
    "        train_loader=dataloaders['train'],\n",
    "        val_loader=dataloaders['val'],\n",
    "        num_epochs=config['training']['num_epochs']\n",
    "    )\n",
    "    \n",
    "    training_end_time = datetime.now()\n",
    "    training_duration = training_end_time - training_start_time\n",
    "    print(f\"✅ 训练完成!\")\n",
    "    print(f\"训练用时: {training_duration}\")\n",
    "    \n",
    "    # 保存训练历史\n",
    "    with open(experiment_dir / 'training_history.json', 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ 训练失败: {e}\")\n",
    "    history = None\n",
    "\n",
    "# ============================================================================\n",
    "# 6. 训练结果分析\n",
    "# ============================================================================\n",
    "\n",
    "if history:\n",
    "    print(\"\\n📈 训练结果分析\")\n",
    "    \n",
    "    # 绘制训练曲线\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    epochs = range(len(history['train_loss']))\n",
    "    \n",
    "    # 损失曲线\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], label='Train Loss', color='blue')\n",
    "    axes[0, 0].plot(epochs, history['val_loss'], label='Val Loss', color='red')\n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # 学习率曲线\n",
    "    axes[0, 1].plot(epochs, history['learning_rates'], color='green')\n",
    "    axes[0, 1].set_title('Learning Rate Schedule')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Learning Rate')\n",
    "    axes[0, 1].set_yscale('log')\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Dice分数曲线\n",
    "    if history['train_metrics'] and history['val_metrics']:\n",
    "        train_dice = [m.get('dice', 0) for m in history['train_metrics']]\n",
    "        val_dice = [m.get('dice', 0) for m in history['val_metrics']]\n",
    "        \n",
    "        axes[1, 0].plot(epochs, train_dice, label='Train Dice', color='blue')\n",
    "        axes[1, 0].plot(epochs, val_dice, label='Val Dice', color='red')\n",
    "        axes[1, 0].set_title('Dice Score')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Dice Score')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # IoU分数曲线\n",
    "        train_iou = [m.get('iou', 0) for m in history['train_metrics']]\n",
    "        val_iou = [m.get('iou', 0) for m in history['val_metrics']]\n",
    "        \n",
    "        axes[1, 1].plot(epochs, train_iou, label='Train IoU', color='blue')\n",
    "        axes[1, 1].plot(epochs, val_iou, label='Val IoU', color='red')\n",
    "        axes[1, 1].set_title('IoU Score')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('IoU Score')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(experiment_dir / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 训练统计分析\n",
    "    if history['val_metrics']:\n",
    "        final_metrics = history['val_metrics'][-1]\n",
    "        best_epoch = np.argmin(history['val_loss'])\n",
    "        best_metrics = history['val_metrics'][best_epoch]\n",
    "        \n",
    "        print(f\"📊 训练统计:\")\n",
    "        print(f\"  总轮次: {len(history['train_loss'])}\")\n",
    "        print(f\"  最佳轮次: {best_epoch + 1}\")\n",
    "        print(f\"  最佳验证损失: {history['val_loss'][best_epoch]:.4f}\")\n",
    "        print(f\"  最佳验证Dice: {best_metrics.get('dice', 0):.4f}\")\n",
    "        print(f\"  最佳验证IoU: {best_metrics.get('iou', 0):.4f}\")\n",
    "        print(f\"  最终验证Dice: {final_metrics.get('dice', 0):.4f}\")\n",
    "        print(f\"  最终验证IoU: {final_metrics.get('iou', 0):.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. 模型评估\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n🎯 模型评估\")\n",
    "\n",
    "# 加载最佳模型\n",
    "best_checkpoint_path = checkpoint_manager.get_best_checkpoint_path()\n",
    "if best_checkpoint_path and Path(best_checkpoint_path).exists():\n",
    "    print(f\"加载最佳模型: {best_checkpoint_path}\")\n",
    "    \n",
    "    # 创建推理引擎\n",
    "    inference_engine = InferenceEngine(model, device, config)\n",
    "    inference_engine.load_checkpoint(best_checkpoint_path)\n",
    "    \n",
    "    # 在测试集上评估\n",
    "    test_loader = dataloaders['test']\n",
    "    \n",
    "    print(\"📊 测试集评估...\")\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    metrics_calculator.reset()\n",
    "    \n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        # 推理\n",
    "        results = inference_engine.predict_batch(batch)\n",
    "        \n",
    "        # 收集结果\n",
    "        all_predictions.append(results['logits'])\n",
    "        all_targets.append(batch['mask'])\n",
    "        \n",
    "        # 更新指标\n",
    "        metrics_calculator.update(results['logits'].to(device), batch['mask'].to(device))\n",
    "    \n",
    "    # 计算最终指标\n",
    "    test_metrics = metrics_calculator.compute()\n",
    "    \n",
    "    print(\"🏆 测试集结果:\")\n",
    "    for metric_name, value in test_metrics.items():\n",
    "        print(f\"  {metric_name.upper()}: {value:.4f}\")\n",
    "    \n",
    "    # 保存测试结果\n",
    "    test_results = {\n",
    "        'model': 'unet',\n",
    "        'checkpoint': str(best_checkpoint_path),\n",
    "        'test_metrics': test_metrics,\n",
    "        'num_test_samples': len(datasets['test']),\n",
    "        'config': config\n",
    "    }\n",
    "    \n",
    "    with open(experiment_dir / 'test_results.json', 'w') as f:\n",
    "        json.dump(test_results, f, indent=2)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # 8. 预测可视化\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(\"\\n🖼️ 预测结果可视化\")\n",
    "    \n",
    "    # 选择几个测试样本进行可视化\n",
    "    test_batch = next(iter(test_loader))\n",
    "    test_results = inference_engine.predict_batch(test_batch)\n",
    "    \n",
    "    num_samples = min(6, len(test_batch['image']))\n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
    "    \n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        image = test_batch['image'][i].squeeze().numpy()\n",
    "        mask = test_batch['mask'][i].squeeze().numpy()\n",
    "        pred_prob = test_results['probabilities'][i].squeeze().numpy()\n",
    "        pred_binary = test_results['predictions'][i].squeeze().numpy()\n",
    "        image_id = test_batch.get('image_id', [f\"test_{i}\"] * num_samples)[i]\n",
    "        \n",
    "        # 计算指标\n",
    "        dice = 2 * (pred_binary * mask).sum() / (pred_binary.sum() + mask.sum() + 1e-6)\n",
    "        iou = (pred_binary * mask).sum() / ((pred_binary + mask) > 0).sum()\n",
    "        \n",
    "        # 原图\n",
    "        axes[i, 0].imshow(image, cmap='gray')\n",
    "        axes[i, 0].set_title(f'Original\\n{image_id}')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # 真实掩码\n",
    "        axes[i, 1].imshow(mask, cmap='Reds', alpha=0.8)\n",
    "        axes[i, 1].set_title('Ground Truth')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # 预测概率\n",
    "        im = axes[i, 2].imshow(pred_prob, cmap='Blues', vmin=0, vmax=1)\n",
    "        axes[i, 2].set_title(f'Prediction\\n(Max: {pred_prob.max():.3f})')\n",
    "        axes[i, 2].axis('off')\n",
    "        plt.colorbar(im, ax=axes[i, 2], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # 叠加显示\n",
    "        overlay = np.stack([image, image, image], axis=-1)\n",
    "        overlay[:, :, 1] = np.maximum(overlay[:, :, 1], mask * 0.5)  # 绿色\n",
    "        overlay[:, :, 0] = np.maximum(overlay[:, :, 0], pred_binary * 0.5)  # 红色\n",
    "        \n",
    "        axes[i, 3].imshow(overlay)\n",
    "        axes[i, 3].set_title(f'Overlay\\nDice: {dice:.3f}, IoU: {iou:.3f}')\n",
    "        axes[i, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(experiment_dir / 'test_predictions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 9. 实验总结\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n📋 实验总结\")\n",
    "\n",
    "experiment_summary = {\n",
    "    'experiment_name': 'unet_baseline',\n",
    "    'experiment_date': training_start_time.isoformat() if 'training_start_time' in locals() else datetime.now().isoformat(),\n",
    "    'training_duration': str(training_duration) if 'training_duration' in locals() else \"N/A\",\n",
    "    'model_info': model_info,\n",
    "    'config': config,\n",
    "    'dataset_info': {\n",
    "        'train_samples': len(datasets['train']) if 'train' in datasets else 0,\n",
    "        'val_samples': len(datasets['val']) if 'val' in datasets else 0,\n",
    "        'test_samples': len(datasets['test']) if 'test' in datasets else 0\n",
    "    },\n",
    "    'training_results': {\n",
    "        'completed': history is not None,\n",
    "        'total_epochs': len(history['train_loss']) if history else 0,\n",
    "        'best_val_loss': min(history['val_loss']) if history else None,\n",
    "        'final_val_loss': history['val_loss'][-1] if history else None\n",
    "    },\n",
    "    'test_results': test_metrics if 'test_metrics' in locals() else None,\n",
    "    'files_generated': [\n",
    "        'data_distribution.png',\n",
    "        'sample_visualization.png', \n",
    "        'model_info.json',\n",
    "        'training_history.json',\n",
    "        'training_curves.png',\n",
    "        'test_results.json',\n",
    "        'test_predictions.png',\n",
    "        'experiment_summary.json'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 保存实验总结\n",
    "with open(experiment_dir / 'experiment_summary.json', 'w') as f:\n",
    "    json.dump(experiment_summary, f, indent=2, default=str)\n",
    "\n",
    "print(\"✅ 实验完成!\")\n",
    "print(f\"📁 所有结果保存在: {experiment_dir}\")\n",
    "\n",
    "# 性能评估表格\n",
    "if 'test_metrics' in locals():\n",
    "    print(\"\\n🏆 最终性能总结:\")\n",
    "    metrics_df = pd.DataFrame([\n",
    "        ['Dice Score', f\"{test_metrics['dice']:.4f}\"],\n",
    "        ['IoU Score', f\"{test_metrics['iou']:.4f}\"],\n",
    "        ['Precision', f\"{test_metrics.get('precision', 0):.4f}\"],\n",
    "        ['Recall', f\"{test_metrics.get('recall', 0):.4f}\"],\n",
    "        ['Pixel Accuracy', f\"{test_metrics.get('pixel_accuracy', 0):.4f}\"]\n",
    "    ], columns=['Metric', 'Value'])\n",
    "    \n",
    "    print(metrics_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n🎯 基线模型已完成，可以开始下一阶段的改进实验！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251ae40e-1b24-407a-ac99-6d5f27da8f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4b32ad-6d21-4c05-90c7-541a85976936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f78e1-a76f-43cc-80b3-98bd41592ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7adad23-05b6-44b2-9f62-02982459091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/test_unet.py\n",
    "# U-Net模型的单元测试和集成测试\n",
    "\n",
    "import unittest\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "sys.path.append(str(Path(__file__).parent.parent))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from models.unet import UNet, DoubleConv, Down, Up, OutConv\n",
    "\n",
    "class TestUNetComponents(unittest.TestCase):\n",
    "    \"\"\"测试U-Net组件\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"设置测试环境\"\"\"\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def test_double_conv(self):\n",
    "        \"\"\"测试双卷积层\"\"\"\n",
    "        # 基础测试\n",
    "        double_conv = DoubleConv(3, 64)\n",
    "        input_tensor = torch.randn(2, 3, 256, 256)\n",
    "        output = double_conv(input_tensor)\n",
    "        \n",
    "        self.assertEqual(output.shape, (2, 64, 256, 256))\n",
    "        self.assertFalse(torch.isnan(output).any())\n",
    "        \n",
    "        # 测试中间通道数\n",
    "        double_conv_mid = DoubleConv(3, 64, mid_channels=32)\n",
    "        output_mid = double_conv_mid(input_tensor)\n",
    "        \n",
    "        self.assertEqual(output_mid.shape, (2, 64, 256, 256))\n",
    "        \n",
    "    def test_down_layer(self):\n",
    "        \"\"\"测试下采样层\"\"\"\n",
    "        down = Down(64, 128)\n",
    "        input_tensor = torch.randn(2, 64, 256, 256)\n",
    "        output = down(input_tensor)\n",
    "        \n",
    "        # 下采样应该将空间尺寸减半\n",
    "        self.assertEqual(output.shape, (2, 128, 128, 128))\n",
    "        self.assertFalse(torch.isnan(output).any())\n",
    "        \n",
    "    def test_up_layer_bilinear(self):\n",
    "        \"\"\"测试上采样层（双线性插值）\"\"\"\n",
    "        up = Up(256, 128, bilinear=True)\n",
    "        \n",
    "        # 模拟跳跃连接\n",
    "        x1 = torch.randn(2, 128, 64, 64)  # 低分辨率特征\n",
    "        x2 = torch.randn(2, 128, 128, 128)  # 高分辨率特征（跳跃连接）\n",
    "        \n",
    "        output = up(x1, x2)\n",
    "        \n",
    "        self.assertEqual(output.shape, (2, 128, 128, 128))\n",
    "        self.assertFalse(torch.isnan(output).any())\n",
    "        \n",
    "    def test_up_layer_transpose(self):\n",
    "        \"\"\"测试上采样层（转置卷积）\"\"\"\n",
    "        up = Up(256, 128, bilinear=False)\n",
    "        \n",
    "        x1 = torch.randn(2, 128, 64, 64)\n",
    "        x2 = torch.randn(2, 128, 128, 128)\n",
    "        \n",
    "        output = up(x1, x2)\n",
    "        \n",
    "        self.assertEqual(output.shape, (2, 128, 128, 128))\n",
    "        self.assertFalse(torch.isnan(output).any())\n",
    "        \n",
    "    def test_out_conv(self):\n",
    "        \"\"\"测试输出卷积层\"\"\"\n",
    "        out_conv = OutConv(64, 1)\n",
    "        input_tensor = torch.randn(2, 64, 256, 256)\n",
    "        output = out_conv(input_tensor)\n",
    "        \n",
    "        self.assertEqual(output.shape, (2, 1, 256, 256))\n",
    "        self.assertFalse(torch.isnan(output).any())\n",
    "\n",
    "class TestUNetModel(unittest.TestCase):\n",
    "    \"\"\"测试完整的U-Net模型\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"设置测试环境\"\"\"\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def test_model_creation(self):\n",
    "        \"\"\"测试模型创建\"\"\"\n",
    "        # 默认配置\n",
    "        model = UNet()\n",
    "        self.assertIsInstance(model, nn.Module)\n",
    "        \n",
    "        # 自定义配置\n",
    "        model_custom = UNet(\n",
    "            in_channels=3,\n",
    "            out_channels=2,\n",
    "            features=[32, 64, 128, 256],\n",
    "            bilinear=True,\n",
    "            dropout_rate=0.5\n",
    "        )\n",
    "        self.assertIsInstance(model_custom, nn.Module)\n",
    "        \n",
    "    def test_forward_pass(self):\n",
    "        \"\"\"测试前向传播\"\"\"\n",
    "        model = UNet(in_channels=1, out_channels=1)\n",
    "        model.eval()\n",
    "        \n",
    "        # 测试不同输入尺寸\n",
    "        input_sizes = [\n",
    "            (1, 1, 256, 256),\n",
    "            (2, 1, 512, 512),\n",
    "            (4, 1, 128, 128)\n",
    "        ]\n",
    "        \n",
    "        for batch_size, channels, height, width in input_sizes:\n",
    "            with self.subTest(size=(batch_size, channels, height, width)):\n",
    "                input_tensor = torch.randn(batch_size, channels, height, width)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    output = model(input_tensor)\n",
    "                \n",
    "                # 检查输出形状\n",
    "                expected_shape = (batch_size, 1, height, width)\n",
    "                self.assertEqual(output.shape, expected_shape)\n",
    "                \n",
    "                # 检查输出值\n",
    "                self.assertFalse(torch.isnan(output).any())\n",
    "                self.assertFalse(torch.isinf(output).any())\n",
    "                \n",
    "    def test_backward_pass(self):\n",
    "        \"\"\"测试反向传播\"\"\"\n",
    "        model = UNet(in_channels=1, out_channels=1, dropout_rate=0.3)\n",
    "        model.train()\n",
    "        \n",
    "        input_tensor = torch.randn(2, 1, 256, 256, requires_grad=True)\n",
    "        target = torch.randn(2, 1, 256, 256)\n",
    "        \n",
    "        # 前向传播\n",
    "        output = model(input_tensor)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = nn.MSELoss()(output, target)\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 检查梯度\n",
    "        self.assertIsNotNone(input_tensor.grad)\n",
    "        self.assertFalse(torch.isnan(input_tensor.grad).any())\n",
    "        \n",
    "        # 检查模型参数梯度\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad:\n",
    "                self.assertIsNotNone(param.grad)\n",
    "                self.assertFalse(torch.isnan(param.grad).any())\n",
    "                \n",
    "    def test_different_configurations(self):\n",
    "        \"\"\"测试不同配置\"\"\"\n",
    "        configs = [\n",
    "            {'in_channels': 1, 'out_channels': 1, 'bilinear': False},\n",
    "            {'in_channels': 3, 'out_channels': 2, 'bilinear': True},\n",
    "            {'features': [32, 64, 128], 'dropout_rate': 0.5},\n",
    "            {'features': [64, 128, 256, 512, 1024], 'dropout_rate': 0.2}\n",
    "        ]\n",
    "        \n",
    "        for config in configs:\n",
    "            with self.subTest(config=config):\n",
    "                try:\n",
    "                    model = UNet(**config)\n",
    "                    input_tensor = torch.randn(1, config.get('in_channels', 1), 128, 128)\n",
    "                    \n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        output = model(input_tensor)\n",
    "                    \n",
    "                    expected_out_channels = config.get('out_channels', 1)\n",
    "                    self.assertEqual(output.shape[1], expected_out_channels)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.fail(f\"配置 {config} 测试失败: {e}\")\n",
    "                    \n",
    "    def test_model_info(self):\n",
    "        \"\"\"测试模型信息获取\"\"\"\n",
    "        model = UNet(features=[64, 128, 256, 512])\n",
    "        model_info = model.get_model_info()\n",
    "        \n",
    "        required_keys = [\n",
    "            'model_name', 'in_channels', 'out_channels', 'features',\n",
    "            'bilinear', 'dropout_rate', 'total_parameters', \n",
    "            'trainable_parameters', 'model_size_mb'\n",
    "        ]\n",
    "        \n",
    "        for key in required_keys:\n",
    "            self.assertIn(key, model_info)\n",
    "            \n",
    "        self.assertIsInstance(model_info['total_parameters'], int)\n",
    "        self.assertIsInstance(model_info['trainable_parameters'], int)\n",
    "        self.assertIsInstance(model_info['model_size_mb'], float)\n",
    "        \n",
    "    def test_feature_maps_extraction(self):\n",
    "        \"\"\"测试特征图提取\"\"\"\n",
    "        model = UNet()\n",
    "        model.eval()\n",
    "        \n",
    "        input_tensor = torch.randn(1, 1, 256, 256)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = model.get_feature_maps(input_tensor)\n",
    "            \n",
    "        expected_features = ['enc1', 'enc2', 'enc3', 'enc4', 'bottleneck',\n",
    "                           'dec1', 'dec2', 'dec3', 'dec4', 'output']\n",
    "        \n",
    "        for feat_name in expected_features:\n",
    "            self.assertIn(feat_name, features)\n",
    "            self.assertIsInstance(features[feat_name], torch.Tensor)\n",
    "            \n",
    "        # 检查输出特征图形状\n",
    "        self.assertEqual(features['output'].shape, (1, 1, 256, 256))\n",
    "        \n",
    "    def test_memory_efficiency(self):\n",
    "        \"\"\"测试内存效率\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            self.skipTest(\"需要CUDA进行内存测试\")\n",
    "            \n",
    "        device = torch.device('cuda')\n",
    "        model = UNet().to(device)\n",
    "        \n",
    "        # 清除缓存\n",
    "        torch.cuda.empty_cache()\n",
    "        initial_memory = torch.cuda.memory_allocated()\n",
    "        \n",
    "        # 前向传播\n",
    "        input_tensor = torch.randn(4, 1, 512, 512, device=device)\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            \n",
    "        peak_memory = torch.cuda.max_memory_allocated()\n",
    "        memory_usage = (peak_memory - initial_memory) / 1024**2  # MB\n",
    "        \n",
    "        print(f\"内存使用: {memory_usage:.2f} MB\")\n",
    "        \n",
    "        # 内存使用应该在合理范围内\n",
    "        self.assertLess(memory_usage, 2000)  # 小于2GB\n",
    "        \n",
    "    def test_model_device_compatibility(self):\n",
    "        \"\"\"测试设备兼容性\"\"\"\n",
    "        model = UNet()\n",
    "        \n",
    "        # CPU测试\n",
    "        input_cpu = torch.randn(1, 1, 128, 128)\n",
    "        output_cpu = model(input_cpu)\n",
    "        self.assertEqual(output_cpu.device.type, 'cpu')\n",
    "        \n",
    "        # GPU测试（如果可用）\n",
    "        if torch.cuda.is_available():\n",
    "            model_gpu = model.to('cuda')\n",
    "            input_gpu = torch.randn(1, 1, 128, 128, device='cuda')\n",
    "            output_gpu = model_gpu(input_gpu)\n",
    "            self.assertEqual(output_gpu.device.type, 'cuda')\n",
    "\n",
    "class TestUNetIntegration(unittest.TestCase):\n",
    "    \"\"\"U-Net集成测试\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"设置测试环境\"\"\"\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def test_training_step(self):\n",
    "        \"\"\"测试完整训练步骤\"\"\"\n",
    "        from losses import DiceLoss\n",
    "        \n",
    "        model = UNet(dropout_rate=0.3).to(self.device)\n",
    "        criterion = DiceLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "        \n",
    "        # 模拟训练数据\n",
    "        images = torch.randn(4, 1, 256, 256, device=self.device)\n",
    "        masks = torch.randint(0, 2, (4, 1, 256, 256), device=self.device).float()\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 检查结果\n",
    "        self.assertIsInstance(loss.item(), float)\n",
    "        self.assertFalse(torch.isnan(loss))\n",
    "        \n",
    "    def test_evaluation_step(self):\n",
    "        \"\"\"测试评估步骤\"\"\"\n",
    "        from metrics import DiceScore, IoUScore\n",
    "        \n",
    "        model = UNet().to(self.device)\n",
    "        model.eval()\n",
    "        \n",
    "        dice_metric = DiceScore()\n",
    "        iou_metric = IoUScore()\n",
    "        \n",
    "        # 模拟测试数据\n",
    "        images = torch.randn(4, 1, 256, 256, device=self.device)\n",
    "        masks = torch.randint(0, 2, (4, 1, 256, 256), device=self.device).float()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # 更新指标\n",
    "            dice_metric.update(outputs, masks)\n",
    "            iou_metric.update(outputs, masks)\n",
    "            \n",
    "        # 计算指标\n",
    "        dice_score = dice_metric.compute()\n",
    "        iou_score = iou_metric.compute()\n",
    "        \n",
    "        self.assertIsInstance(dice_score, float)\n",
    "        self.assertIsInstance(iou_score, float)\n",
    "        self.assertTrue(0 <= dice_score <= 1)\n",
    "        self.assertTrue(0 <= iou_score <= 1)\n",
    "        \n",
    "    def test_checkpoint_save_load(self):\n",
    "        \"\"\"测试检查点保存和加载\"\"\"\n",
    "        import tempfile\n",
    "        \n",
    "        # 创建模型\n",
    "        model1 = UNet(features=[32, 64, 128, 256])\n",
    "        \n",
    "        # 保存检查点\n",
    "        with tempfile.NamedTemporaryFile(suffix='.pth', delete=False) as f:\n",
    "            checkpoint_path = f.name\n",
    "            \n",
    "        checkpoint = {\n",
    "            'model_state_dict': model1.state_dict(),\n",
    "            'epoch': 10,\n",
    "            'loss': 0.25\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        \n",
    "        # 创建新模型并加载检查点\n",
    "        model2 = UNet(features=[32, 64, 128, 256])\n",
    "        loaded_checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        model2.load_state_dict(loaded_checkpoint['model_state_dict'])\n",
    "        \n",
    "        # 检查参数是否相同\n",
    "        for p1, p2 in zip(model1.parameters(), model2.parameters()):\n",
    "            self.assertTrue(torch.allclose(p1, p2))\n",
    "            \n",
    "        # 清理\n",
    "        Path(checkpoint_path).unlink()\n",
    "        \n",
    "    def test_inference_consistency(self):\n",
    "        \"\"\"测试推理一致性\"\"\"\n",
    "        model = UNet()\n",
    "        model.eval()\n",
    "        \n",
    "        # 固定随机种子\n",
    "        torch.manual_seed(42)\n",
    "        input_tensor = torch.randn(1, 1, 256, 256)\n",
    "        \n",
    "        # 多次推理应该得到相同结果\n",
    "        with torch.no_grad():\n",
    "            output1 = model(input_tensor)\n",
    "            output2 = model(input_tensor)\n",
    "            \n",
    "        self.assertTrue(torch.allclose(output1, output2, atol=1e-6))\n",
    "        \n",
    "    def test_batch_independence(self):\n",
    "        \"\"\"测试批次独立性\"\"\"\n",
    "        model = UNet()\n",
    "        model.eval()\n",
    "        \n",
    "        # 单个样本推理\n",
    "        single_input = torch.randn(1, 1, 256, 256)\n",
    "        with torch.no_grad():\n",
    "            single_output = model(single_input)\n",
    "        \n",
    "        # 批次推理（包含相同样本）\n",
    "        batch_input = single_input.repeat(3, 1, 1, 1)\n",
    "        with torch.no_grad():\n",
    "            batch_output = model(batch_input)\n",
    "            \n",
    "        # 每个批次输出应该与单个样本输出相同\n",
    "        for i in range(3):\n",
    "            self.assertTrue(torch.allclose(single_output[0], batch_output[i], atol=1e-6))\n",
    "\n",
    "class TestUNetPerformance(unittest.TestCase):\n",
    "    \"\"\"U-Net性能测试\"\"\"\n",
    "    \n",
    "    def test_inference_speed(self):\n",
    "        \"\"\"测试推理速度\"\"\"\n",
    "        import time\n",
    "        \n",
    "        model = UNet()\n",
    "        model.eval()\n",
    "        \n",
    "        input_tensor = torch.randn(1, 1, 512, 512)\n",
    "        \n",
    "        # 预热\n",
    "        with torch.no_grad():\n",
    "            for _ in range(5):\n",
    "                _ = model(input_tensor)\n",
    "                \n",
    "        # 计时\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(10):\n",
    "                output = model(input_tensor)\n",
    "                \n",
    "        end_time = time.time()\n",
    "        \n",
    "        avg_time = (end_time - start_time) / 10\n",
    "        fps = 1.0 / avg_time\n",
    "        \n",
    "        print(f\"平均推理时间: {avg_time:.4f}s\")\n",
    "        print(f\"推理速度: {fps:.2f} FPS\")\n",
    "        \n",
    "        # 推理时间应该在合理范围内\n",
    "        self.assertLess(avg_time, 1.0)  # 少于1秒\n",
    "        \n",
    "    def test_parameter_count(self):\n",
    "        \"\"\"测试参数数量\"\"\"\n",
    "        configs = [\n",
    "            {'features': [32, 64, 128, 256]},      # 小模型\n",
    "            {'features': [64, 128, 256, 512]},     # 标准模型\n",
    "            {'features': [64, 128, 256, 512, 1024]} # 大模型\n",
    "        ]\n",
    "        \n",
    "        expected_ranges = [\n",
    "            (1e6, 10e6),    # 1M-10M参数\n",
    "            (10e6, 50e6),   # 10M-50M参数  \n",
    "            (50e6, 200e6)   # 50M-200M参数\n",
    "        ]\n",
    "        \n",
    "        for config, (min_params, max_params) in zip(configs, expected_ranges):\n",
    "            with self.subTest(config=config):\n",
    "                model = UNet(**config)\n",
    "                total_params = sum(p.numel() for p in model.parameters())\n",
    "                \n",
    "                print(f\"配置 {config}: {total_params:,} 参数\")\n",
    "                \n",
    "                self.assertGreaterEqual(total_params, min_params)\n",
    "                self.assertLessEqual(total_params, max_params)\n",
    "\n",
    "# 运行特定测试的辅助函数\n",
    "def run_specific_tests():\n",
    "    \"\"\"运行特定的测试\"\"\"\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"运行U-Net测试\")\n",
    "    parser.add_argument('--test', type=str, choices=['components', 'model', 'integration', 'performance', 'all'],\n",
    "                       default='all', help='要运行的测试类型')\n",
    "    parser.add_argument('--verbose', action='store_true', help='详细输出')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # 创建测试套件\n",
    "    test_suite = unittest.TestSuite()\n",
    "    \n",
    "    if args.test == 'components' or args.test == 'all':\n",
    "        test_suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestUNetComponents))\n",
    "        \n",
    "    if args.test == 'model' or args.test == 'all':\n",
    "        test_suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestUNetModel))\n",
    "        \n",
    "    if args.test == 'integration' or args.test == 'all':\n",
    "        test_suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestUNetIntegration))\n",
    "        \n",
    "    if args.test == 'performance' or args.test == 'all':\n",
    "        test_suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestUNetPerformance))\n",
    "    \n",
    "    # 运行测试\n",
    "    verbosity = 2 if args.verbose else 1\n",
    "    runner = unittest.TextTestRunner(verbosity=verbosity)\n",
    "    result = runner.run(test_suite)\n",
    "    \n",
    "    return result.wasSuccessful()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 基础测试运行\n",
    "    if len(sys.argv) == 1:\n",
    "        # 运行所有测试\n",
    "        unittest.main(verbosity=2)\n",
    "    else:\n",
    "        # 运行特定测试\n",
    "        success = run_specific_tests()\n",
    "        sys.exit(0 if success else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c0524-d6ac-45da-a7f1-6dd7d41b2823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9200ed4-0adb-49d5-9f32-46f8176bcadf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28369f65-bdcd-423b-8f2b-ec1afbc70c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51260709-04f8-4e0e-88c9-2cd4e75c0f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparam/optuna_search.py\n",
    "# 使用Optuna进行U-Net超参数搜索\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "sys.path.append(str(Path(__file__).parent.parent))\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from configs import ConfigLoader\n",
    "from models.unet import UNet\n",
    "from datasets import ChestXrayDataset, DataLoaderFactory\n",
    "from transforms import get_transforms\n",
    "from losses import get_loss_fn, LossFactory\n",
    "from metrics import create_metrics_calculator\n",
    "from engine import TrainingEngine, EarlyStopping, CheckpointManager\n",
    "from utils import set_seed, setup_logging, get_device\n",
    "\n",
    "class OptunaObjective:\n",
    "    \"\"\"Optuna优化目标函数\"\"\"\n",
    "    \n",
    "    def __init__(self, config, device, datasets, base_output_dir):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.datasets = datasets\n",
    "        self.base_output_dir = Path(base_output_dir)\n",
    "        self.trial_count = 0\n",
    "        \n",
    "        # 创建基础变换（不包含需要优化的增强参数）\n",
    "        self.base_transforms = {\n",
    "            'train': get_transforms('train', config),\n",
    "            'val': get_transforms('val', config),\n",
    "            'test': get_transforms('test', config)\n",
    "        }\n",
    "        \n",
    "    def __call__(self, trial):\n",
    "        \"\"\"优化目标函数\"\"\"\n",
    "        self.trial_count += 1\n",
    "        \n",
    "        # 建议超参数\n",
    "        hyperparams = self._suggest_hyperparameters(trial)\n",
    "        \n",
    "        # 创建试验特定的配置\n",
    "        trial_config = self._create_trial_config(hyperparams)\n",
    "        \n",
    "        # 设置输出目录\n",
    "        trial_dir = self.base_output_dir / f\"trial_{trial.number:04d}\"\n",
    "        trial_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            # 训练模型\n",
    "            best_metric = self._train_model(trial_config, trial_dir, trial)\n",
    "            \n",
    "            # 保存试验结果\n",
    "            self._save_trial_results(trial, hyperparams, best_metric, trial_dir)\n",
    "            \n",
    "            return best_metric\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Trial {trial.number} 失败: {e}\")\n",
    "            # 返回一个差的分数，让optuna知道这个试验失败了\n",
    "            return float('inf')\n",
    "            \n",
    "    def _suggest_hyperparameters(self, trial):\n",
    "        \"\"\"建议超参数\"\"\"\n",
    "        hyperparams = {}\n",
    "        \n",
    "        # 模型架构参数\n",
    "        hyperparams['features_scale'] = trial.suggest_categorical('features_scale', ['small', 'medium', 'large'])\n",
    "        hyperparams['dropout_rate'] = trial.suggest_float('dropout_rate', 0.0, 0.5, step=0.1)\n",
    "        hyperparams['bilinear'] = trial.suggest_categorical('bilinear', [True, False])\n",
    "        \n",
    "        # 训练参数\n",
    "        hyperparams['learning_rate'] = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "        hyperparams['batch_size'] = trial.suggest_categorical('batch_size', [4, 8, 16, 32])\n",
    "        hyperparams['weight_decay'] = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
    "        \n",
    "        # 损失函数权重\n",
    "        hyperparams['bce_weight'] = trial.suggest_float('bce_weight', 0.1, 0.9, step=0.1)\n",
    "        hyperparams['dice_weight'] = 1.0 - hyperparams['bce_weight']  # 确保权重和为1\n",
    "        \n",
    "        # 学习率调度器参数\n",
    "        hyperparams['scheduler_patience'] = trial.suggest_int('scheduler_patience', 5, 20)\n",
    "        hyperparams['scheduler_factor'] = trial.suggest_float('scheduler_factor', 0.3, 0.8, step=0.1)\n",
    "        \n",
    "        # 数据增强参数\n",
    "        hyperparams['aug_prob'] = trial.suggest_float('aug_prob', 0.3, 0.8, step=0.1)\n",
    "        hyperparams['rotation_limit'] = trial.suggest_int('rotation_limit', 5, 25)\n",
    "        hyperparams['scale_limit'] = trial.suggest_float('scale_limit', 0.05, 0.2, step=0.05)\n",
    "        \n",
    "        return hyperparams\n",
    "        \n",
    "    def _create_trial_config(self, hyperparams):\n",
    "        \"\"\"创建试验特定的配置\"\"\"\n",
    "        trial_config = self.config.copy()\n",
    "        \n",
    "        # 模型配置\n",
    "        features_scales = {\n",
    "            'small': [32, 64, 128, 256],\n",
    "            'medium': [64, 128, 256, 512],\n",
    "            'large': [64, 128, 256, 512, 1024]\n",
    "        }\n",
    "        \n",
    "        trial_config['model']['features'] = features_scales[hyperparams['features_scale']]\n",
    "        trial_config['model']['dropout_rate'] = hyperparams['dropout_rate']\n",
    "        trial_config['model']['bilinear'] = hyperparams['bilinear']\n",
    "        \n",
    "        # 训练配置\n",
    "        trial_config['training']['learning_rate'] = hyperparams['learning_rate']\n",
    "        trial_config['training']['batch_size'] = hyperparams['batch_size']\n",
    "        trial_config['training']['weight_decay'] = hyperparams['weight_decay']\n",
    "        trial_config['training']['num_epochs'] = 30  # 减少epoch数以加快搜索\n",
    "        \n",
    "        # 损失权重\n",
    "        trial_config['training']['loss_weights']['bce'] = hyperparams['bce_weight']\n",
    "        trial_config['training']['loss_weights']['dice'] = hyperparams['dice_weight']\n",
    "        \n",
    "        # 调度器配置\n",
    "        trial_config['training']['scheduler_patience'] = hyperparams['scheduler_patience']\n",
    "        trial_config['training']['scheduler_factor'] = hyperparams['scheduler_factor']\n",
    "        \n",
    "        # 早停配置（更激进以加快搜索）\n",
    "        trial_config['training']['early_stopping']['patience'] = 10\n",
    "        \n",
    "        # 数据增强配置\n",
    "        trial_config['augmentation']['shift_scale_rotate']['rotate_limit'] = hyperparams['rotation_limit']\n",
    "        trial_config['augmentation']['shift_scale_rotate']['scale_limit'] = hyperparams['scale_limit']\n",
    "        trial_config['augmentation']['shift_scale_rotate']['p'] = hyperparams['aug_prob']\n",
    "        \n",
    "        return trial_config\n",
    "        \n",
    "    def _train_model(self, trial_config, trial_dir, trial):\n",
    "        \"\"\"训练模型\"\"\"\n",
    "        # 创建模型\n",
    "        model = UNet(\n",
    "            in_channels=trial_config['model']['in_channels'],\n",
    "            out_channels=trial_config['model']['out_channels'],\n",
    "            features=trial_config['model']['features'],\n",
    "            dropout_rate=trial_config['model']['dropout_rate'],\n",
    "            bilinear=trial_config['model']['bilinear']\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # 创建数据加载器\n",
    "        factory = DataLoaderFactory(trial_config)\n",
    "        dataloaders = {}\n",
    "        \n",
    "        for split in ['train', 'val']:\n",
    "            dataset = ChestXrayDataset(\n",
    "                trial_config, \n",
    "                split=split, \n",
    "                transform=self.base_transforms[split]\n",
    "            )\n",
    "            dataloaders[split] = factory.create_dataloader(dataset)\n",
    "            \n",
    "        # 创建损失函数\n",
    "        criterion = get_loss_fn(trial_config)\n",
    "        \n",
    "        # 创建优化器\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=trial_config['training']['learning_rate'],\n",
    "            weight_decay=trial_config['training']['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # 创建调度器\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            patience=trial_config['training']['scheduler_patience'],\n",
    "            factor=trial_config['training']['scheduler_factor']\n",
    "        )\n",
    "        \n",
    "        # 创建早停\n",
    "        early_stopping = EarlyStopping(\n",
    "            patience=trial_config['training']['early_stopping']['patience'],\n",
    "            min_delta=trial_config['training']['early_stopping']['min_delta']\n",
    "        )\n",
    "        \n",
    "        # 创建检查点管理器\n",
    "        checkpoint_manager = CheckpointManager(\n",
    "            checkpoint_dir=str(trial_dir / 'checkpoints'),\n",
    "            save_best_only=True\n",
    "        )\n",
    "        \n",
    "        # 创建指标计算器\n",
    "        metrics_calculator = create_metrics_calculator(trial_config)\n",
    "        \n",
    "        # 创建训练引擎（简化版，专门用于超参数搜索）\n",
    "        trainer = HyperparamTrainer(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            device=self.device,\n",
    "            scheduler=scheduler,\n",
    "            metrics_calculator=metrics_calculator,\n",
    "            early_stopping=early_stopping,\n",
    "            checkpoint_manager=checkpoint_manager,\n",
    "            trial=trial\n",
    "        )\n",
    "        \n",
    "        # 训练\n",
    "        best_metric = trainer.fit(\n",
    "            train_loader=dataloaders['train'],\n",
    "            val_loader=dataloaders['val'],\n",
    "            num_epochs=trial_config['training']['num_epochs']\n",
    "        )\n",
    "        \n",
    "        return best_metric\n",
    "        \n",
    "    def _save_trial_results(self, trial, hyperparams, best_metric, trial_dir):\n",
    "        \"\"\"保存试验结果\"\"\"\n",
    "        results = {\n",
    "            'trial_number': trial.number,\n",
    "            'hyperparameters': hyperparams,\n",
    "            'best_metric': best_metric,\n",
    "            'trial_dir': str(trial_dir),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(trial_dir / 'trial_results.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "class HyperparamTrainer:\n",
    "    \"\"\"简化的训练器，专门用于超参数搜索\"\"\"\n",
    "    \n",
    "    def __init__(self, model, criterion, optimizer, device, scheduler=None,\n",
    "                 metrics_calculator=None, early_stopping=None, \n",
    "                 checkpoint_manager=None, trial=None):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.scheduler = scheduler\n",
    "        self.metrics_calculator = metrics_calculator\n",
    "        self.early_stopping = early_stopping\n",
    "        self.checkpoint_manager = checkpoint_manager\n",
    "        self.trial = trial\n",
    "        \n",
    "        self.best_metric = float('inf')\n",
    "        \n",
    "    def fit(self, train_loader, val_loader, num_epochs):\n",
    "        \"\"\"训练循环\"\"\"\n",
    "        for epoch in range(num_epochs):\n",
    "            # 训练\n",
    "            train_loss = self._train_epoch(train_loader)\n",
    "            \n",
    "            # 验证\n",
    "            val_loss, val_metrics = self._validate_epoch(val_loader)\n",
    "            \n",
    "            # 学习率调度\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step(val_loss)\n",
    "                \n",
    "            # 更新最佳指标\n",
    "            if val_loss < self.best_metric:\n",
    "                self.best_metric = val_loss\n",
    "                \n",
    "                # 保存最佳模型\n",
    "                if self.checkpoint_manager:\n",
    "                    self.checkpoint_manager.save_checkpoint(\n",
    "                        epoch=epoch,\n",
    "                        model=self.model,\n",
    "                        optimizer=self.optimizer,\n",
    "                        scheduler=self.scheduler,\n",
    "                        metrics={'loss': val_loss, **val_metrics},\n",
    "                        is_best=True\n",
    "                    )\n",
    "                    \n",
    "            # 早停检查\n",
    "            if self.early_stopping:\n",
    "                if self.early_stopping(val_loss, self.model):\n",
    "                    break\n",
    "                    \n",
    "            # Optuna剪枝\n",
    "            if self.trial:\n",
    "                self.trial.report(val_loss, epoch)\n",
    "                if self.trial.should_prune():\n",
    "                    raise optuna.TrialPruned()\n",
    "                    \n",
    "        return self.best_metric\n",
    "        \n",
    "    def _train_epoch(self, train_loader):\n",
    "        \"\"\"训练一个epoch\"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            images = batch['image'].to(self.device, non_blocking=True)\n",
    "            masks = batch['mask'].to(self.device, non_blocking=True)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        return running_loss / len(train_loader)\n",
    "        \n",
    "    def _validate_epoch(self, val_loader):\n",
    "        \"\"\"验证一个epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        if self.metrics_calculator:\n",
    "            self.metrics_calculator.reset()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['image'].to(self.device, non_blocking=True)\n",
    "                masks = batch['mask'].to(self.device, non_blocking=True)\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, masks)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                if self.metrics_calculator:\n",
    "                    self.metrics_calculator.update(outputs, masks)\n",
    "                    \n",
    "        val_loss = running_loss / len(val_loader)\n",
    "        val_metrics = self.metrics_calculator.compute() if self.metrics_calculator else {}\n",
    "        \n",
    "        return val_loss, val_metrics\n",
    "\n",
    "def create_study(study_name, storage_url=None, direction='minimize'):\n",
    "    \"\"\"创建Optuna研究\"\"\"\n",
    "    if storage_url:\n",
    "        study = optuna.create_study(\n",
    "            study_name=study_name,\n",
    "            storage=storage_url,\n",
    "            load_if_exists=True,\n",
    "            direction=direction,\n",
    "            pruner=optuna.pruners.MedianPruner(\n",
    "                n_startup_trials=5,\n",
    "                n_warmup_steps=10,\n",
    "                interval_steps=1\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        study = optuna.create_study(\n",
    "            direction=direction,\n",
    "            pruner=optuna.pruners.MedianPruner(\n",
    "                n_startup_trials=5,\n",
    "                n_warmup_steps=10,\n",
    "                interval_steps=1\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return study\n",
    "\n",
    "def run_hyperparameter_search(config_name='unet', \n",
    "                             n_trials=50,\n",
    "                             timeout=None,\n",
    "                             study_name=None,\n",
    "                             storage_url=None,\n",
    "                             output_dir='hyperparam_results'):\n",
    "    \"\"\"运行超参数搜索\"\"\"\n",
    "    \n",
    "    # 设置日志\n",
    "    setup_logging()\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # 设置随机种子\n",
    "    set_seed(42)\n",
    "    \n",
    "    # 获取设备\n",
    "    device = get_device()\n",
    "    \n",
    "    # 加载配置\n",
    "    config_loader = ConfigLoader()\n",
    "    config = config_loader.load_config(config_name)\n",
    "    \n",
    "    # 创建输出目录\n",
    "    output_dir = Path(output_dir) / datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    logger.info(f\"超参数搜索开始\")\n",
    "    logger.info(f\"输出目录: {output_dir}\")\n",
    "    logger.info(f\"试验次数: {n_trials}\")\n",
    "    logger.info(f\"设备: {device}\")\n",
    "    \n",
    "    # 创建数据集（只创建一次，避免重复加载）\n",
    "    transforms = {\n",
    "        'train': get_transforms('train', config),\n",
    "        'val': get_transforms('val', config),\n",
    "        'test': get_transforms('test', config)\n",
    "    }\n",
    "    \n",
    "    datasets = {}\n",
    "    for split in ['train', 'val']:\n",
    "        try:\n",
    "            dataset = ChestXrayDataset(config, split=split, transform=transforms[split])\n",
    "            datasets[split] = dataset\n",
    "            logger.info(f\"{split} 数据集: {len(dataset)} 样本\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"创建 {split} 数据集失败: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # 创建研究\n",
    "    if not study_name:\n",
    "        study_name = f\"unet_hyperparam_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "    study = create_study(study_name, storage_url)\n",
    "    \n",
    "    # 创建目标函数\n",
    "    objective = OptunaObjective(config, device, datasets, output_dir)\n",
    "    \n",
    "    # 保存搜索配置\n",
    "    search_config = {\n",
    "        'study_name': study_name,\n",
    "        'n_trials': n_trials,\n",
    "        'timeout': timeout,\n",
    "        'base_config': config,\n",
    "        'search_space': {\n",
    "            'features_scale': ['small', 'medium', 'large'],\n",
    "            'dropout_rate': [0.0, 0.5],\n",
    "            'bilinear': [True, False],\n",
    "            'learning_rate': [1e-5, 1e-3],\n",
    "            'batch_size': [4, 8, 16, 32],\n",
    "            'weight_decay': [1e-6, 1e-3],\n",
    "            'bce_weight': [0.1, 0.9],\n",
    "            'scheduler_patience': [5, 20],\n",
    "            'scheduler_factor': [0.3, 0.8],\n",
    "            'aug_prob': [0.3, 0.8],\n",
    "            'rotation_limit': [5, 25],\n",
    "            'scale_limit': [0.05, 0.2]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(output_dir / 'search_config.json', 'w') as f:\n",
    "        json.dump(search_config, f, indent=2, default=str)\n",
    "    \n",
    "    # 开始优化\n",
    "    try:\n",
    "        study.optimize(objective, n_trials=n_trials, timeout=timeout)\n",
    "        \n",
    "        logger.info(\"超参数搜索完成!\")\n",
    "        \n",
    "        # 分析结果\n",
    "        analyze_results(study, output_dir)\n",
    "        \n",
    "        return study\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"搜索被用户中断\")\n",
    "        return study\n",
    "    except Exception as e:\n",
    "        logger.error(f\"搜索过程中出错: {e}\")\n",
    "        return study\n",
    "\n",
    "def analyze_results(study, output_dir):\n",
    "    \"\"\"分析搜索结果\"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # 基本统计\n",
    "    logger.info(\"🏆 搜索结果分析:\")\n",
    "    logger.info(f\"  总试验次数: {len(study.trials)}\")\n",
    "    logger.info(f\"  完成的试验: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}\")\n",
    "    logger.info(f\"  剪枝的试验: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}\")\n",
    "    logger.info(f\"  失败的试验: {len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL])}\")\n",
    "    \n",
    "    if study.best_trial:\n",
    "        logger.info(f\"\\n🥇 最佳试验:\")\n",
    "        logger.info(f\"  试验编号: {study.best_trial.number}\")\n",
    "        logger.info(f\"  最佳分数: {study.best_value:.4f}\")\n",
    "        logger.info(f\"  最佳参数:\")\n",
    "        for key, value in study.best_params.items():\n",
    "            logger.info(f\"    {key}: {value}\")\n",
    "    \n",
    "    # 保存详细结果\n",
    "    results_df = study.trials_dataframe()\n",
    "    results_df.to_csv(output_dir / 'optuna_trials.csv', index=False)\n",
    "    \n",
    "    # 保存最佳参数\n",
    "    best_params = {\n",
    "        'best_trial_number': study.best_trial.number if study.best_trial else None,\n",
    "        'best_value': study.best_value if study.best_trial else None,\n",
    "        'best_params': study.best_params if study.best_trial else None,\n",
    "        'search_space': study.best_trial.distributions if study.best_trial else None\n",
    "    }\n",
    "    \n",
    "    with open(output_dir / 'best_params.json', 'w') as f:\n",
    "        json.dump(best_params, f, indent=2, default=str)\n",
    "    \n",
    "    # 生成可视化图表\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        # 参数重要性\n",
    "        if len(study.trials) > 10:\n",
    "            importance = optuna.importance.get_param_importances(study)\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            params = list(importance.keys())\n",
    "            values = list(importance.values())\n",
    "            \n",
    "            ax.barh(params, values)\n",
    "            ax.set_xlabel('Importance')\n",
    "            ax.set_title('Parameter Importance')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(output_dir / 'param_importance.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        \n",
    "        # 优化历史\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "        if completed_trials:\n",
    "            trial_numbers = [t.number for t in completed_trials]\n",
    "            trial_values = [t.value for t in completed_trials]\n",
    "            \n",
    "            ax.plot(trial_numbers, trial_values, 'o-', alpha=0.7)\n",
    "            \n",
    "            # 添加最佳值线\n",
    "            best_values = []\n",
    "            current_best = float('inf')\n",
    "            for value in trial_values:\n",
    "                if value < current_best:\n",
    "                    current_best = value\n",
    "                best_values.append(current_best)\n",
    "                \n",
    "            ax.plot(trial_numbers, best_values, 'r-', linewidth=2, label='Best Value')\n",
    "            \n",
    "            ax.set_xlabel('Trial Number')\n",
    "            ax.set_ylabel('Objective Value')\n",
    "            ax.set_title('Optimization History')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / 'optimization_history.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        logger.info(f\"可视化图表保存至: {output_dir}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        logger.warning(\"matplotlib未安装，跳过可视化\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"生成可视化时出错: {e}\")\n",
    "\n",
    "def create_config_from_best_params(best_params, base_config_name='unet', output_path=None):\n",
    "    \"\"\"根据最佳参数创建新的配置文件\"\"\"\n",
    "    \n",
    "    # 加载基础配置\n",
    "    config_loader = ConfigLoader()\n",
    "    base_config = config_loader.load_config(base_config_name)\n",
    "    \n",
    "    # 应用最佳参数\n",
    "    features_scales = {\n",
    "        'small': [32, 64, 128, 256],\n",
    "        'medium': [64, 128, 256, 512],\n",
    "        'large': [64, 128, 256, 512, 1024]\n",
    "    }\n",
    "    \n",
    "    # 更新配置\n",
    "    optimized_config = base_config.copy()\n",
    "    \n",
    "    if 'features_scale' in best_params:\n",
    "        optimized_config['model']['features'] = features_scales[best_params['features_scale']]\n",
    "    if 'dropout_rate' in best_params:\n",
    "        optimized_config['model']['dropout_rate'] = best_params['dropout_rate']\n",
    "    if 'bilinear' in best_params:\n",
    "        optimized_config['model']['bilinear'] = best_params['bilinear']\n",
    "        \n",
    "    if 'learning_rate' in best_params:\n",
    "        optimized_config['training']['learning_rate'] = best_params['learning_rate']\n",
    "    if 'batch_size' in best_params:\n",
    "        optimized_config['training']['batch_size'] = best_params['batch_size']\n",
    "    if 'weight_decay' in best_params:\n",
    "        optimized_config['training']['weight_decay'] = best_params['weight_decay']\n",
    "        \n",
    "    if 'bce_weight' in best_params:\n",
    "        optimized_config['training']['loss_weights']['bce'] = best_params['bce_weight']\n",
    "        optimized_config['training']['loss_weights']['dice'] = 1.0 - best_params['bce_weight']\n",
    "        \n",
    "    if 'scheduler_patience' in best_params:\n",
    "        optimized_config['training']['scheduler_patience'] = best_params['scheduler_patience']\n",
    "    if 'scheduler_factor' in best_params:\n",
    "        optimized_config['training']['scheduler_factor'] = best_params['scheduler_factor']\n",
    "        \n",
    "    # 数据增强参数\n",
    "    if 'rotation_limit' in best_params:\n",
    "        optimized_config['augmentation']['shift_scale_rotate']['rotate_limit'] = best_params['rotation_limit']\n",
    "    if 'scale_limit' in best_params:\n",
    "        optimized_config['augmentation']['shift_scale_rotate']['scale_limit'] = best_params['scale_limit']\n",
    "    if 'aug_prob' in best_params:\n",
    "        optimized_config['augmentation']['shift_scale_rotate']['p'] = best_params['aug_prob']\n",
    "    \n",
    "    # 更新实验名称\n",
    "    optimized_config['experiment']['name'] = f\"{base_config_name}_optimized\"\n",
    "    optimized_config['experiment']['tags'].append('hyperopt')\n",
    "    \n",
    "    # 保存优化后的配置\n",
    "    if output_path:\n",
    "        import yaml\n",
    "        with open(output_path, 'w') as f:\n",
    "            yaml.dump(optimized_config, f, default_flow_style=False, indent=2)\n",
    "            \n",
    "    return optimized_config\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"U-Net超参数搜索\")\n",
    "    \n",
    "    parser.add_argument('--config', type=str, default='unet',\n",
    "                       help='基础配置文件名称')\n",
    "    parser.add_argument('--n-trials', type=int, default=50,\n",
    "                       help='试验次数')\n",
    "    parser.add_argument('--timeout', type=int, default=None,\n",
    "                       help='超时时间（秒）')\n",
    "    parser.add_argument('--study-name', type=str, default=None,\n",
    "                       help='研究名称')\n",
    "    parser.add_argument('--storage', type=str, default=None,\n",
    "                       help='存储URL（用于分布式搜索）')\n",
    "    parser.add_argument('--output-dir', type=str, default='hyperparam_results',\n",
    "                       help='输出目录')\n",
    "    parser.add_argument('--create-config', action='store_true',\n",
    "                       help='根据最佳参数创建配置文件')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    try:\n",
    "        # 运行超参数搜索\n",
    "        study = run_hyperparameter_search(\n",
    "            config_name=args.config,\n",
    "            n_trials=args.n_trials,\n",
    "            timeout=args.timeout,\n",
    "            study_name=args.study_name,\n",
    "            storage_url=args.storage,\n",
    "            output_dir=args.output_dir\n",
    "        )\n",
    "        \n",
    "        if study and study.best_trial and args.create_config:\n",
    "            # 创建优化后的配置文件\n",
    "            output_config_path = Path(args.output_dir) / 'optimized_config.yaml'\n",
    "            optimized_config = create_config_from_best_params(\n",
    "                study.best_params, \n",
    "                args.config,\n",
    "                output_config_path\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ 优化后的配置文件保存至: {output_config_path}\")\n",
    "            \n",
    "        print(\"🎯 超参数搜索完成!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 搜索失败: {e}\")\n",
    "        return 1\n",
    "        \n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    exit(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0bd4ae-af13-4337-9c02-f3354e1d7c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f0ee59-5ac0-4955-89a3-9abe565038ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653a0783-a937-4187-bf5a-8b01bee876f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a265de08-ba2d-4c7a-8484-c17490415e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练U-Net基线模型:\n",
    "!python scripts/train.py --config unet --model unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310f79bf-ce30-48aa-b848-925649c28b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#评估训练好的模型:\n",
    "!python scripts/evaluate.py --checkpoint checkpoints/unet/best_model.pth --model unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a039333-656e-4c7e-b059-a2360dc46ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#可视化预测结果:\n",
    "!python scripts/visualize_enhanced.py --checkpoint checkpoints/unet/best_model.pth --model unet --mode comprehensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5789be1a-0572-43fa-be36-17e48beb253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#运行完整实验 (这个特殊一些):\n",
    "# 直接在当前notebook中执行实验代码\n",
    "exec(open('notebooks/unet_baseline_experiment.ipynb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899c838a-f12c-4dae-8ee8-1e8f52101c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#执行超参数搜索:\n",
    "!python hyperparam/optuna_search.py --config unet --n-trials 50 --create-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85635c61-f45d-47be-943b-b96a2117a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#运行测试:\n",
    "!python tests/test_unet.py --test all --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96b20cf-d6d3-4fbe-8a05-8f54a7009fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5519a343-d453-4210-a3a9-7a2e45ffacbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d125e0-13c1-4ac4-8673-aad7cfc0dc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8428836-ab72-499d-8ce7-a29d489fa859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
